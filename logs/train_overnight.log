[2021-06-17 03:12:27,345 PID:35818 INFO run_lab.py get_spec_and_run] Running lab spec_file:../../../config_files/RL_project_wizardofwor.json spec_name:DDQN_PrioritizedReplay in mode:train
[2021-06-17 03:12:27,522 PID:35818 INFO logger.py info] Running sessions
[2021-06-17 03:12:30,484 PID:36047 INFO openai.py __init__][2021-06-17 03:12:30,485 PID:36048 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7fdcbf50f780>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7fdcb4727f60>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
 OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7fe82b182b70>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7fe820561f60>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 03:12:30,489 PID:36050 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7f7fc8b5c780>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7f7fbdd70f28>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 03:12:30,517 PID:36049 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7f12d7da1b38>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7f12cd16ff60>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 03:13:02,776 PID:36049 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train
[2021-06-17 03:13:02,776 PID:36048 INFO base.py end_init_nets][2021-06-17 03:13:02,776 PID:36050 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train
[2021-06-17 03:13:02,776 PID:36047 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train
 Initialized algorithm models for lab_mode: train
[2021-06-17 03:13:05,316 PID:36049 INFO base.py __init__][2021-06-17 03:13:05,316 PID:36048 INFO base.py __init__][2021-06-17 03:13:05,316 PID:36050 INFO base.py __init__][2021-06-17 03:13:05,316 PID:36047 INFO base.py __init__] DoubleDQN:
- agent = <slm_lab.agent.Agent object at 0x7fdcba9f3908>
- action_pdtype = Categorical
- action_policy = <function epsilon_greedy at 0x7fdcbc8222f0>
- explore_var_spec = {'end_step': 1000000,
 'end_val': 0.01,
 'name': 'linear_decay',
 'start_step': 10000,
 'start_value': 1.0}
- training_start_step = 10000
- gamma = 0.99
- training_batch_iter = 1
- training_iter = 4
- training_frequency = 32
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7fdcaca37668>
- net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- target_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- net_names = ['net', 'target_net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7fdcac9f9550>
- global_net = None
- global_target_net = None
- online_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- eval_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
) DoubleDQN:
- agent = <slm_lab.agent.Agent object at 0x7f12c547e630>
- action_pdtype = Categorical
- action_policy = <function epsilon_greedy at 0x7f12d526a2f0>
- explore_var_spec = {'end_step': 1000000,
 'end_val': 0.01,
 'name': 'linear_decay',
 'start_step': 10000,
 'start_value': 1.0}
- training_start_step = 10000
- gamma = 0.99
- training_batch_iter = 1
- training_iter = 4
- training_frequency = 32
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f12c547eb38>
- net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- target_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- net_names = ['net', 'target_net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7f12c54434e0>
- global_net = None
- global_target_net = None
- online_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- eval_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
) DoubleDQN:
- agent = <slm_lab.agent.Agent object at 0x7f7fb607e5c0>
- action_pdtype = Categorical
- action_policy = <function epsilon_greedy at 0x7f7fc5e6a2f0>
- explore_var_spec = {'end_step': 1000000,
 'end_val': 0.01,
 'name': 'linear_decay',
 'start_step': 10000,
 'start_value': 1.0}
- training_start_step = 10000
- gamma = 0.99
- training_batch_iter = 1
- training_iter = 4
- training_frequency = 32
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f7fb607e588>
- net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- target_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- net_names = ['net', 'target_net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7f7fb60434e0>
- global_net = None
- global_target_net = None
- online_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- eval_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)


 DoubleDQN:
- agent = <slm_lab.agent.Agent object at 0x7fe818872588>
- action_pdtype = Categorical
- action_policy = <function epsilon_greedy at 0x7fe82865d2f0>
- explore_var_spec = {'end_step': 1000000,
 'end_val': 0.01,
 'name': 'linear_decay',
 'start_step': 10000,
 'start_value': 1.0}
- training_start_step = 10000
- gamma = 0.99
- training_batch_iter = 1
- training_iter = 4
- training_frequency = 32
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7fe818872630>
- net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- target_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- net_names = ['net', 'target_net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7fe818834550>
- global_net = None
- global_target_net = None
- online_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
- eval_net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=512, out_features=10, bias=True)
    (1): Softmax(dim=None)
  )
  (loss_fn): SmoothL1Loss()
)
[2021-06-17 03:13:05,320 PID:36050 INFO __init__.py __init__][2021-06-17 03:13:05,320 PID:36048 INFO __init__.py __init__][2021-06-17 03:13:05,320 PID:36047 INFO __init__.py __init__][2021-06-17 03:13:05,320 PID:36049 INFO __init__.py __init__] Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_031227',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/graph/DDQN_PrioritizedReplay_t0_s3',
 'info_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/info/DDQN_PrioritizedReplay_t0_s3',
 'log_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/log/DDQN_PrioritizedReplay_t0_s3',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/model/DDQN_PrioritizedReplay_t0_s3',
 'prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/DDQN_PrioritizedReplay_t0_s3',
 'random_seed': 1623891749,
 'resume': False,
 'rigorous_eval': 0,
 'session': 3,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'epsilon_greedy',
               'center_return': True,
               'explore_var_spec': {'end_step': 1000000,
                                    'end_val': 0.01,
                                    'name': 'linear_decay',
                                    'start_step': 10000,
                                    'start_value': 1.0},
               'gamma': 0.99,
               'name': 'DoubleDQN',
               'training_batch_iter': 1,
               'training_frequency': 32,
               'training_iter': 4,
               'training_start_step': 10000},
 'memory': {'alpha': 0.6000000000000001,
            'batch_size': 32,
            'epsilon': 10000.0,
            'max_size': 10000,
            'name': 'PrioritizedReplay',
            'use_cer': False},
 'name': 'DDQN',
 'net': {'batch_norm': False,
         'clip_grad_val': 10.0,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': None,
         'loss_spec': {'name': 'SmoothL1Loss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'ConvNet',
         'update_frequency': 1000,
         'update_type': 'replace'}}
- name = DDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7f7fb607e5c0>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7f804ec44048>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.prioritized.PrioritizedReplay object at 0x7f7fb607e828>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.DoubleDQN object at 0x7f7fb66077b8>
 Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_031227',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/graph/DDQN_PrioritizedReplay_t0_s1',
 'info_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/info/DDQN_PrioritizedReplay_t0_s1',
 'log_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/log/DDQN_PrioritizedReplay_t0_s1',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/model/DDQN_PrioritizedReplay_t0_s1',
 'prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/DDQN_PrioritizedReplay_t0_s1',
 'random_seed': 1623889749,
 'resume': False,
 'rigorous_eval': 0,
 'session': 1,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'epsilon_greedy',
               'center_return': True,
               'explore_var_spec': {'end_step': 1000000,
                                    'end_val': 0.01,
                                    'name': 'linear_decay',
                                    'start_step': 10000,
                                    'start_value': 1.0},
               'gamma': 0.99,
               'name': 'DoubleDQN',
               'training_batch_iter': 1,
               'training_frequency': 32,
               'training_iter': 4,
               'training_start_step': 10000},
 'memory': {'alpha': 0.6000000000000001,
            'batch_size': 32,
            'epsilon': 10000.0,
            'max_size': 10000,
            'name': 'PrioritizedReplay',
            'use_cer': False},
 'name': 'DDQN',
 'net': {'batch_norm': False,
         'clip_grad_val': 10.0,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': None,
         'loss_spec': {'name': 'SmoothL1Loss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'ConvNet',
         'update_frequency': 1000,
         'update_type': 'replace'}}
- name = DDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7fdcba9f3908>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7fdd109e8e10>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.prioritized.PrioritizedReplay object at 0x7fdcaca37828>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.DoubleDQN object at 0x7fdcaca37898>
 Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_031227',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/graph/DDQN_PrioritizedReplay_t0_s0',
 'info_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/info/DDQN_PrioritizedReplay_t0_s0',
 'log_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/log/DDQN_PrioritizedReplay_t0_s0',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/model/DDQN_PrioritizedReplay_t0_s0',
 'prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/DDQN_PrioritizedReplay_t0_s0',
 'random_seed': 1623888749,
 'resume': False,
 'rigorous_eval': 0,
 'session': 0,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'epsilon_greedy',
               'center_return': True,
               'explore_var_spec': {'end_step': 1000000,
                                    'end_val': 0.01,
                                    'name': 'linear_decay',
                                    'start_step': 10000,
                                    'start_value': 1.0},
               'gamma': 0.99,
               'name': 'DoubleDQN',
               'training_batch_iter': 1,
               'training_frequency': 32,
               'training_iter': 4,
               'training_start_step': 10000},
 'memory': {'alpha': 0.6000000000000001,
            'batch_size': 32,
            'epsilon': 10000.0,
            'max_size': 10000,
            'name': 'PrioritizedReplay',
            'use_cer': False},
 'name': 'DDQN',
 'net': {'batch_norm': False,
         'clip_grad_val': 10.0,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': None,
         'loss_spec': {'name': 'SmoothL1Loss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'ConvNet',
         'update_frequency': 1000,
         'update_type': 'replace'}}
- name = DDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7fe818872588>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7fe87c823eb8>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.prioritized.PrioritizedReplay object at 0x7fe818872828>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.DoubleDQN object at 0x7fe818872898>
 Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_031227',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/graph/DDQN_PrioritizedReplay_t0_s2',
 'info_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/info/DDQN_PrioritizedReplay_t0_s2',
 'log_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/log/DDQN_PrioritizedReplay_t0_s2',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/model/DDQN_PrioritizedReplay_t0_s2',
 'prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/DDQN_PrioritizedReplay_t0_s2',
 'random_seed': 1623890749,
 'resume': False,
 'rigorous_eval': 0,
 'session': 2,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'epsilon_greedy',
               'center_return': True,
               'explore_var_spec': {'end_step': 1000000,
                                    'end_val': 0.01,
                                    'name': 'linear_decay',
                                    'start_step': 10000,
                                    'start_value': 1.0},
               'gamma': 0.99,
               'name': 'DoubleDQN',
               'training_batch_iter': 1,
               'training_frequency': 32,
               'training_iter': 4,
               'training_start_step': 10000},
 'memory': {'alpha': 0.6000000000000001,
            'batch_size': 32,
            'epsilon': 10000.0,
            'max_size': 10000,
            'name': 'PrioritizedReplay',
            'use_cer': False},
 'name': 'DDQN',
 'net': {'batch_norm': False,
         'clip_grad_val': 10.0,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': None,
         'loss_spec': {'name': 'SmoothL1Loss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'ConvNet',
         'update_frequency': 1000,
         'update_type': 'replace'}}
- name = DDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7f12c547e630>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7f1329444cf8>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.prioritized.PrioritizedReplay object at 0x7f12c547e828>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.DoubleDQN object at 0x7f12c547e898>
[2021-06-17 03:13:05,321 PID:36050 INFO logger.py info][2021-06-17 03:13:05,321 PID:36047 INFO logger.py info][2021-06-17 03:13:05,321 PID:36048 INFO logger.py info] Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_031227',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/graph/DDQN_PrioritizedReplay_t0_s3',
 'info_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/info/DDQN_PrioritizedReplay_t0_s3',
 'log_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/log/DDQN_PrioritizedReplay_t0_s3',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/model/DDQN_PrioritizedReplay_t0_s3',
 'prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/DDQN_PrioritizedReplay_t0_s3',
 'random_seed': 1623891749,
 'resume': False,
 'rigorous_eval': 0,
 'session': 3,
 'trial': 0}
- index = 3
- agent = <slm_lab.agent.Agent object at 0x7f7fb607e5c0>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7f804ec44048>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7f804ec44048>
 Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_031227',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/graph/DDQN_PrioritizedReplay_t0_s0',
 'info_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/info/DDQN_PrioritizedReplay_t0_s0',
 'log_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/log/DDQN_PrioritizedReplay_t0_s0',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/model/DDQN_PrioritizedReplay_t0_s0',
 'prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/DDQN_PrioritizedReplay_t0_s0',
 'random_seed': 1623888749,
 'resume': False,
 'rigorous_eval': 0,
 'session': 0,
 'trial': 0}
- index = 0
- agent = <slm_lab.agent.Agent object at 0x7fe818872588>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7fe87c823eb8>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7fe87c823eb8> Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_031227',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/graph/DDQN_PrioritizedReplay_t0_s1',
 'info_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/info/DDQN_PrioritizedReplay_t0_s1',
 'log_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/log/DDQN_PrioritizedReplay_t0_s1',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/model/DDQN_PrioritizedReplay_t0_s1',
 'prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/DDQN_PrioritizedReplay_t0_s1',
 'random_seed': 1623889749,
 'resume': False,
 'rigorous_eval': 0,
 'session': 1,
 'trial': 0}
- index = 1
- agent = <slm_lab.agent.Agent object at 0x7fdcba9f3908>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7fdd109e8e10>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7fdd109e8e10>

[2021-06-17 03:13:05,321 PID:36049 INFO logger.py info] Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_031227',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/graph/DDQN_PrioritizedReplay_t0_s2',
 'info_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/info/DDQN_PrioritizedReplay_t0_s2',
 'log_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/log/DDQN_PrioritizedReplay_t0_s2',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/model/DDQN_PrioritizedReplay_t0_s2',
 'prepath': 'data/DDQN_PrioritizedReplay_2021_06_17_031227/DDQN_PrioritizedReplay_t0_s2',
 'random_seed': 1623890749,
 'resume': False,
 'rigorous_eval': 0,
 'session': 2,
 'trial': 0}
- index = 2
- agent = <slm_lab.agent.Agent object at 0x7f12c547e630>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7f1329444cf8>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7f1329444cf8>
[2021-06-17 03:13:05,321 PID:36050 INFO logger.py info] Running RL loop for trial 0 session 3[2021-06-17 03:13:05,321 PID:36047 INFO logger.py info]
 Running RL loop for trial 0 session 0
[2021-06-17 03:13:05,321 PID:36048 INFO logger.py info] Running RL loop for trial 0 session 1
[2021-06-17 03:13:05,321 PID:36049 INFO logger.py info] Running RL loop for trial 0 session 2
[2021-06-17 03:13:07,869 PID:36050 INFO __init__.py log_summary][2021-06-17 03:13:07,869 PID:36047 INFO __init__.py log_summary][2021-06-17 03:13:07,869 PID:36048 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan

[2021-06-17 03:13:07,869 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
 Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:15:31,609 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 10000  wall_t: 123  opt_step: 0  frame: 10000  fps: 81.3008  total_reward: 100  total_reward_ma: 100  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:15:31,653 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 10000  wall_t: 118  opt_step: 0  frame: 10000  fps: 84.7458  total_reward: 400  total_reward_ma: 400  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:15:31,599 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 10000  wall_t: 108  opt_step: 0  frame: 10000  fps: 92.5926  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:15:31,633 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 10000  wall_t: 108  opt_step: 0  frame: 10000  fps: 92.5926  total_reward: 400  total_reward_ma: 400  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:17:56,963 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 20000  wall_t: 326  opt_step: 7512  frame: 20000  fps: 61.3497  total_reward: 516.667  total_reward_ma: 458.333  loss: 0.00389097  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:17:56,963 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 20000  wall_t: 327  opt_step: 7512  frame: 20000  fps: 61.1621  total_reward: 500  total_reward_ma: 300  loss: 9.0058e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:17:57,855 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 20000  wall_t: 327  opt_step: 7512  frame: 20000  fps: 61.1621  total_reward: 500  total_reward_ma: 500  loss: 1.76343e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:17:58,134 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 20000  wall_t: 327  opt_step: 7512  frame: 20000  fps: 61.1621  total_reward: 528.571  total_reward_ma: 464.286  loss: 2.72598e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:18:06,865 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 464.286  strength: -178.714  max_strength: -114.429  final_strength: -114.429  sample_efficiency: 8.39928e-05  training_efficiency: 4.26177e-05  stability: 1
[2021-06-17 03:18:06,918 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 500  strength: -143  max_strength: -143  final_strength: -143  sample_efficiency: 5e-05  training_efficiency: 0.00013312  stability: nan
[2021-06-17 03:18:07,006 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 300  strength: -343  max_strength: -143  final_strength: -143  sample_efficiency: 8.95773e-05  training_efficiency: 2.77496e-05  stability: 1
[2021-06-17 03:18:07,163 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 458.333  strength: -184.667  max_strength: -126.333  final_strength: -126.333  sample_efficiency: 8.28971e-05  training_efficiency: 4.55348e-05  stability: 1
[2021-06-17 03:18:53,572 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 30000  wall_t: 383  opt_step: 15024  frame: 30000  fps: 78.329  total_reward: 650  total_reward_ma: 526.19  loss: 2.95377e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:18:53,777 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 30000  wall_t: 383  opt_step: 15024  frame: 30000  fps: 78.329  total_reward: 500  total_reward_ma: 500  loss: 1.45605e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:18:53,855 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 526.19  strength: -116.81  max_strength: 7  final_strength: 7  sample_efficiency: 8.50048e-05  training_efficiency: 4.21394e-05  stability: 1
[2021-06-17 03:18:54,025 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 500  strength: -143  max_strength: -143  final_strength: -143  sample_efficiency: 4.16667e-05  training_efficiency: 9.98403e-05  stability: 1
[2021-06-17 03:18:54,119 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 30000  wall_t: 384  opt_step: 15024  frame: 30000  fps: 78.125  total_reward: 642.857  total_reward_ma: 414.286  loss: 2.04138e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:18:54,391 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 414.286  strength: -228.714  max_strength: -0.142883  final_strength: -0.142883  sample_efficiency: 8.95655e-05  training_efficiency: 2.77577e-05  stability: 1
[2021-06-17 03:18:55,968 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 30000  wall_t: 386  opt_step: 15024  frame: 30000  fps: 77.7202  total_reward: 928.571  total_reward_ma: 615.079  loss: 1.52732e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:18:56,243 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 615.079  strength: -27.9206  max_strength: 285.571  final_strength: 285.571  sample_efficiency: 0.000251876  training_efficiency: -2.61473e-05  stability: 1
[2021-06-17 03:19:39,942 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 40000  wall_t: 430  opt_step: 22536  frame: 40000  fps: 93.0233  total_reward: 662.5  total_reward_ma: 560.268  loss: 9.39257e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:19:40,108 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 40000  wall_t: 430  opt_step: 22536  frame: 40000  fps: 93.0233  total_reward: 957.143  total_reward_ma: 652.381  loss: 1.61893e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:19:40,182 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 40000  wall_t: 430  opt_step: 22536  frame: 40000  fps: 93.0233  total_reward: 728.571  total_reward_ma: 492.857  loss: 0.00399734  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:19:40,392 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 560.268  strength: -82.7321  max_strength: 19.5  final_strength: 19.5  sample_efficiency: 8.85405e-05  training_efficiency: 4.20078e-05  stability: 1
[2021-06-17 03:19:40,664 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 492.857  strength: -150.143  max_strength: 85.5714  final_strength: 85.5714  sample_efficiency: 9.8765e-05  training_efficiency: 2.53902e-05  stability: 1
[2021-06-17 03:19:40,671 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 652.381  strength: 9.38096  max_strength: 314.143  final_strength: 314.143  sample_efficiency: -0.000144374  training_efficiency: -0.000519304  stability: 1
[2021-06-17 03:19:41,764 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 40000  wall_t: 431  opt_step: 22536  frame: 40000  fps: 92.8074  total_reward: 1012.5  total_reward_ma: 714.435  loss: 0.00396535  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:19:42,085 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 714.435  strength: 71.4345  max_strength: 369.5  final_strength: 369.5  sample_efficiency: -4.1507e-05  training_efficiency: 6.5046e-05  stability: 1
[2021-06-17 03:20:26,334 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 50000  wall_t: 476  opt_step: 30048  frame: 50000  fps: 105.042  total_reward: 787.5  total_reward_ma: 605.714  loss: 0.00392543  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:20:26,697 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 605.714  strength: -37.2857  max_strength: 144.5  final_strength: 144.5  sample_efficiency: 0.000141666  training_efficiency: 4.87725e-05  stability: 1
[2021-06-17 03:20:26,848 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 50000  wall_t: 476  opt_step: 30048  frame: 50000  fps: 105.042  total_reward: 912.5  total_reward_ma: 717.411  loss: 3.31327e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:20:26,993 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 50000  wall_t: 477  opt_step: 30048  frame: 50000  fps: 104.822  total_reward: 912.5  total_reward_ma: 576.786  loss: 1.58253e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:20:27,424 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 717.411  strength: 74.4107  max_strength: 314.143  final_strength: 269.5  sample_efficiency: 4.45804e-06  training_efficiency: -1.89681e-05  stability: -0.586294
[2021-06-17 03:20:27,487 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 576.786  strength: -66.2143  max_strength: 269.5  final_strength: 269.5  sample_efficiency: 0.000162882  training_efficiency: 1.89676e-05  stability: 1
[2021-06-17 03:20:28,069 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 50000  wall_t: 478  opt_step: 30048  frame: 50000  fps: 104.603  total_reward: 675  total_reward_ma: 706.548  loss: 0.00392895  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:20:28,255 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 706.548  strength: 63.5476  max_strength: 369.5  final_strength: 32  sample_efficiency: -3.53125e-05  training_efficiency: 6.18468e-05  stability: -0.181152
[2021-06-17 03:21:13,254 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 60000  wall_t: 523  opt_step: 37560  frame: 60000  fps: 114.723  total_reward: 750  total_reward_ma: 629.762  loss: 2.29487e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:21:13,555 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 629.762  strength: -13.2381  max_strength: 144.5  final_strength: 107  sample_efficiency: 0.000310055  training_efficiency: 7.86093e-05  stability: 0.798851
[2021-06-17 03:21:13,610 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 60000  wall_t: 523  opt_step: 37560  frame: 60000  fps: 114.723  total_reward: 650  total_reward_ma: 703.929  loss: 2.04979e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:21:13,719 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 703.929  strength: 60.9286  max_strength: 314.143  final_strength: 7  sample_efficiency: 4.73857e-06  training_efficiency: -1.79205e-05  stability: -0.0319175
[2021-06-17 03:21:13,806 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 60000  wall_t: 523  opt_step: 37560  frame: 60000  fps: 114.723  total_reward: 812.5  total_reward_ma: 616.071  loss: 1.95408e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:21:14,024 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 616.071  strength: -26.9286  max_strength: 269.5  final_strength: 169.5  sample_efficiency: 0.000316272  training_efficiency: 1.09355e-05  stability: 0.69795
[2021-06-17 03:21:14,502 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 60000  wall_t: 524  opt_step: 37560  frame: 60000  fps: 114.504  total_reward: 662.5  total_reward_ma: 699.206  loss: 0.00391262  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:21:14,602 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 699.206  strength: 56.2063  max_strength: 369.5  final_strength: 19.5  sample_efficiency: -3.23069e-05  training_efficiency: 5.98102e-05  stability: -0.101536
[2021-06-17 03:21:59,640 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 70000  wall_t: 569  opt_step: 45072  frame: 70000  fps: 123.023  total_reward: 637.5  total_reward_ma: 630.867  loss: 8.49279e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:21:59,668 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 70000  wall_t: 569  opt_step: 45072  frame: 70000  fps: 123.023  total_reward: 662.5  total_reward_ma: 697.024  loss: 0.00390708  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:21:59,698 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 70000  wall_t: 569  opt_step: 45072  frame: 70000  fps: 123.023  total_reward: 700  total_reward_ma: 628.061  loss: 5.39519e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:21:59,919 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 697.024  strength: 54.0238  max_strength: 314.143  final_strength: 19.5  sample_efficiency: 5.31291e-06  training_efficiency: -1.55077e-05  stability: -0.00820637
[2021-06-17 03:22:00,072 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 630.867  strength: -12.1327  max_strength: 144.5  final_strength: -5.5  sample_efficiency: 0.000290901  training_efficiency: 7.49553e-05  stability: -0.888489
[2021-06-17 03:22:00,077 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 628.061  strength: -14.9388  max_strength: 269.5  final_strength: 57  sample_efficiency: 0.000480879  training_efficiency: 4.8026e-06  stability: -0.315207
[2021-06-17 03:22:00,238 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 70000  wall_t: 570  opt_step: 45072  frame: 70000  fps: 122.807  total_reward: 1075  total_reward_ma: 752.891  loss: 0.00390394  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:22:00,440 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 752.891  strength: 109.891  max_strength: 432  final_strength: 432  sample_efficiency: -6.14074e-06  training_efficiency: 3.8681e-05  stability: -0.0378424
[2021-06-17 03:22:46,312 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 80000  wall_t: 616  opt_step: 52584  frame: 80000  fps: 129.87  total_reward: 300  total_reward_ma: 587.054  loss: 0.00390668  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:22:46,521 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 80000  wall_t: 616  opt_step: 52584  frame: 80000  fps: 129.87  total_reward: 725  total_reward_ma: 642.634  loss: 1.31979e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:22:46,555 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 80000  wall_t: 616  opt_step: 52584  frame: 80000  fps: 129.87  total_reward: 550  total_reward_ma: 676.02  loss: 4.10006e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:22:46,627 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 587.054  strength: -55.9464  max_strength: 269.5  final_strength: -343  sample_efficiency: 0.000121933  training_efficiency: 1.56961e-05  stability: -4.85724
[2021-06-17 03:22:46,837 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 80000  wall_t: 616  opt_step: 52584  frame: 80000  fps: 129.87  total_reward: 1062.5  total_reward_ma: 791.592  loss: 1.45981e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:22:47,103 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 676.02  strength: 33.0204  max_strength: 314.143  final_strength: -93  sample_efficiency: 2.4212e-06  training_efficiency: -2.93988e-05  stability: -0.294623
[2021-06-17 03:22:47,271 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 642.634  strength: -0.366074  max_strength: 144.5  final_strength: 82  sample_efficiency: 0.00808609  training_efficiency: 0.00164121  stability: -0.76619
[2021-06-17 03:22:47,388 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 791.592  strength: 148.592  max_strength: 432  final_strength: 419.5  sample_efficiency: 4.37488e-07  training_efficiency: 3.17418e-05  stability: 0.528755
[2021-06-17 03:23:32,266 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 90000  wall_t: 662  opt_step: 60096  frame: 90000  fps: 135.952  total_reward: 375  total_reward_ma: 563.492  loss: 4.05404e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:23:32,320 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 563.492  strength: -79.5079  max_strength: 269.5  final_strength: -268  sample_efficiency: 8.04273e-05  training_efficiency: 1.60496e-05  stability: -0.368497
[2021-06-17 03:23:33,464 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 90000  wall_t: 663  opt_step: 60096  frame: 90000  fps: 135.747  total_reward: 800  total_reward_ma: 660.119  loss: 1.1232e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:23:33,525 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 90000  wall_t: 663  opt_step: 60096  frame: 90000  fps: 135.747  total_reward: 575  total_reward_ma: 767.526  loss: 6.93913e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:23:33,543 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 90000  wall_t: 663  opt_step: 60096  frame: 90000  fps: 135.747  total_reward: 687.5  total_reward_ma: 677.455  loss: 0.00392117  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:23:33,770 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 677.455  strength: 34.4554  max_strength: 314.143  final_strength: 44.5  sample_efficiency: 3.82411e-06  training_efficiency: -2.19662e-05  stability: -0.815513
[2021-06-17 03:23:33,770 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 660.119  strength: 17.119  max_strength: 157  final_strength: 157  sample_efficiency: -0.000142378  training_efficiency: -1.42398e-05  stability: -50.2192
[2021-06-17 03:23:33,795 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 767.526  strength: 124.526  max_strength: 432  final_strength: -68  sample_efficiency: -2.10126e-07  training_efficiency: 3.2658e-05  stability: 0.284956
[2021-06-17 03:24:17,771 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 100000  wall_t: 707  opt_step: 67608  frame: 100000  fps: 141.443  total_reward: 862.5  total_reward_ma: 593.393  loss: 2.57772e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:24:17,899 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 593.393  strength: -49.6071  max_strength: 269.5  final_strength: 219.5  sample_efficiency: 0.00011159  training_efficiency: 1.66064e-05  stability: 0.144041
[2021-06-17 03:24:20,009 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 100000  wall_t: 710  opt_step: 67608  frame: 100000  fps: 140.845  total_reward: 1500  total_reward_ma: 840.774  loss: 0.0039156  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:24:20,171 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 100000  wall_t: 710  opt_step: 67608  frame: 100000  fps: 140.845  total_reward: 975  total_reward_ma: 691.607  loss: 0.00394515  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:24:20,341 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 100000  wall_t: 710  opt_step: 67608  frame: 100000  fps: 140.845  total_reward: 762.5  total_reward_ma: 686.905  loss: 2.66561e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:24:20,403 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 840.774  strength: 197.774  max_strength: 857  final_strength: 857  sample_efficiency: 4.21416e-06  training_efficiency: 2.49159e-05  stability: 0.241571
[2021-06-17 03:24:20,449 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 686.905  strength: 43.9048  max_strength: 314.143  final_strength: 119.5  sample_efficiency: 5.69184e-06  training_efficiency: -1.085e-05  stability: -0.522415
[2021-06-17 03:24:20,476 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 691.607  strength: 48.6071  max_strength: 332  final_strength: 332  sample_efficiency: -3.82997e-05  training_efficiency: 5.58911e-06  stability: 0.0264255
[2021-06-17 03:25:04,186 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 110000  wall_t: 754  opt_step: 75120  frame: 110000  fps: 145.889  total_reward: 1200  total_reward_ma: 648.539  loss: 4.7293e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:25:04,398 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 648.539  strength: 5.53896  max_strength: 557  final_strength: 557  sample_efficiency: -0.00082544  training_efficiency: -1.35106e-05  stability: -0.234701
[2021-06-17 03:25:06,578 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 110000  wall_t: 756  opt_step: 75120  frame: 110000  fps: 145.503  total_reward: 1437.5  total_reward_ma: 895.022  loss: 0.00390613  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:25:06,730 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 110000  wall_t: 756  opt_step: 75120  frame: 110000  fps: 145.503  total_reward: 587.5  total_reward_ma: 676.964  loss: 0.00390291  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:25:06,770 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 110000  wall_t: 756  opt_step: 75120  frame: 110000  fps: 145.503  total_reward: 875  total_reward_ma: 708.279  loss: 8.44308e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:25:06,846 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 895.022  strength: 252.022  max_strength: 857  final_strength: 794.5  sample_efficiency: 5.61179e-06  training_efficiency: 2.15903e-05  stability: 0.538614
[2021-06-17 03:25:06,856 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 676.964  strength: 33.9643  max_strength: 314.143  final_strength: -55.5  sample_efficiency: 5.1364e-06  training_efficiency: -1.47982e-05  stability: -0.504881
[2021-06-17 03:25:07,040 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 708.279  strength: 65.2792  max_strength: 332  final_strength: 232  sample_efficiency: -2.29884e-05  training_efficiency: 8.08429e-06  stability: 0.485672
[2021-06-17 03:25:50,426 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 120000  wall_t: 800  opt_step: 82632  frame: 120000  fps: 150  total_reward: 637.5  total_reward_ma: 647.619  loss: 3.12462e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:25:50,543 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 647.619  strength: 4.61904  max_strength: 557  final_strength: -5.5  sample_efficiency: -0.000908172  training_efficiency: -1.60521e-05  stability: -18.2849
[2021-06-17 03:25:52,665 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 120000  wall_t: 802  opt_step: 82632  frame: 120000  fps: 149.626  total_reward: 537.5  total_reward_ma: 664.286  loss: 9.45374e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:25:52,775 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 664.286  strength: 21.2857  max_strength: 314.143  final_strength: -105.5  sample_efficiency: 3.69593e-06  training_efficiency: -2.69188e-05  stability: -0.898002
[2021-06-17 03:25:52,952 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 120000  wall_t: 803  opt_step: 82632  frame: 120000  fps: 149.44  total_reward: 837.5  total_reward_ma: 890.228  loss: 4.59409e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:25:53,079 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 890.228  strength: 247.228  max_strength: 857  final_strength: 194.5  sample_efficiency: 5.79022e-06  training_efficiency: 2.09683e-05  stability: 0.454412
[2021-06-17 03:25:53,139 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 120000  wall_t: 803  opt_step: 82632  frame: 120000  fps: 149.44  total_reward: 800  total_reward_ma: 715.923  loss: 3.63296e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:25:53,372 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 715.923  strength: 72.9226  max_strength: 332  final_strength: 157  sample_efficiency: -1.73688e-05  training_efficiency: 8.8051e-06  stability: 0.547399
[2021-06-17 03:26:36,995 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 130000  wall_t: 847  opt_step: 90144  frame: 130000  fps: 153.483  total_reward: 950  total_reward_ma: 670.879  loss: 0.00396072  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:26:37,169 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 670.879  strength: 27.8791  max_strength: 557  final_strength: 307  sample_efficiency: -0.000132377  training_efficiency: 6.94183e-06  stability: -20.1985
[2021-06-17 03:26:38,702 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 130000  wall_t: 848  opt_step: 90144  frame: 130000  fps: 153.302  total_reward: 675  total_reward_ma: 665.179  loss: 7.50287e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:26:38,783 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 665.179  strength: 22.1786  max_strength: 314.143  final_strength: 32  sample_efficiency: 4.17644e-06  training_efficiency: -2.23484e-05  stability: -1.7532
[2021-06-17 03:26:39,730 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 130000  wall_t: 849  opt_step: 90144  frame: 130000  fps: 153.121  total_reward: 937.5  total_reward_ma: 893.865  loss: 3.64945e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:26:39,745 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 130000  wall_t: 849  opt_step: 90144  frame: 130000  fps: 153.121  total_reward: 937.5  total_reward_ma: 732.967  loss: 2.37218e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:26:39,891 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 893.865  strength: 250.864  max_strength: 857  final_strength: 294.5  sample_efficiency: 5.96198e-06  training_efficiency: 2.00765e-05  stability: 0.490181
[2021-06-17 03:26:39,993 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 732.967  strength: 89.967  max_strength: 332  final_strength: 294.5  sample_efficiency: -1.10584e-05  training_efficiency: 9.38129e-06  stability: 0.628602
[2021-06-17 03:27:22,948 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 140000  wall_t: 893  opt_step: 97656  frame: 140000  fps: 156.775  total_reward: 550  total_reward_ma: 662.245  loss: 0.00390359  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:27:23,036 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 662.245  strength: 19.2449  max_strength: 557  final_strength: -93  sample_efficiency: -0.000180535  training_efficiency: 5.80338e-06  stability: -3.34568
[2021-06-17 03:27:24,633 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 140000  wall_t: 894  opt_step: 97656  frame: 140000  fps: 156.6  total_reward: 325  total_reward_ma: 639.011  loss: 0.00391206  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:27:24,688 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 639.011  strength: -3.98901  max_strength: 314.143  final_strength: -318  sample_efficiency: 2.23672e-05  training_efficiency: 0.000177491  stability: -2.73725
[2021-06-17 03:27:25,865 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 140000  wall_t: 895  opt_step: 97656  frame: 140000  fps: 156.425  total_reward: 1062.5  total_reward_ma: 756.505  loss: 3.00276e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:27:26,107 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 756.505  strength: 113.505  max_strength: 419.5  final_strength: 419.5  sample_efficiency: -6.25344e-06  training_efficiency: 9.60799e-06  stability: 0.72212
[2021-06-17 03:27:26,127 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 140000  wall_t: 896  opt_step: 97656  frame: 140000  fps: 156.25  total_reward: 775  total_reward_ma: 885.374  loss: 0.00782037  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:27:26,223 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 885.374  strength: 242.374  max_strength: 857  final_strength: 132  sample_efficiency: 6.00792e-06  training_efficiency: 1.96939e-05  stability: 0.486391
[2021-06-17 03:28:09,071 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 150000  wall_t: 939  opt_step: 105168  frame: 150000  fps: 159.744  total_reward: 525  total_reward_ma: 653.095  loss: 0.00389023  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:28:09,150 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 653.095  strength: 10.0952  max_strength: 557  final_strength: -118  sample_efficiency: -0.000326412  training_efficiency: 2.91611e-06  stability: -4.9385
[2021-06-17 03:28:10,802 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 150000  wall_t: 940  opt_step: 105168  frame: 150000  fps: 159.574  total_reward: 475  total_reward_ma: 627.296  loss: 6.20185e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:28:10,877 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 627.296  strength: -15.7041  max_strength: 314.143  final_strength: -168  sample_efficiency: 1.03699e-05  training_efficiency: 4.91303e-05  stability: -18.1805
[2021-06-17 03:28:12,328 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 150000  wall_t: 942  opt_step: 105168  frame: 150000  fps: 159.236  total_reward: 662.5  total_reward_ma: 870.516  loss: 0.00391761  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:28:12,330 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 150000  wall_t: 942  opt_step: 105168  frame: 150000  fps: 159.236  total_reward: 575  total_reward_ma: 744.405  loss: 0.00391761  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:28:12,462 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 744.405  strength: 101.405  max_strength: 419.5  final_strength: -68  sample_efficiency: -6.83103e-06  training_efficiency: 9.61243e-06  stability: 0.488695
[2021-06-17 03:28:12,463 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 870.516  strength: 227.516  max_strength: 857  final_strength: 19.5  sample_efficiency: 6.01168e-06  training_efficiency: 1.96357e-05  stability: 0.473217
[2021-06-17 03:28:55,213 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 160000  wall_t: 985  opt_step: 112680  frame: 160000  fps: 162.437  total_reward: 487.5  total_reward_ma: 642.746  loss: 2.13537e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:28:55,280 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 642.746  strength: -0.254467  max_strength: 557  final_strength: -155.5  sample_efficiency: 0.0123788  training_efficiency: 0.00023049  stability: -9.81368
[2021-06-17 03:28:57,170 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 160000  wall_t: 987  opt_step: 112680  frame: 160000  fps: 162.107  total_reward: 487.5  total_reward_ma: 617.976  loss: 0.00392603  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:28:57,286 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 617.976  strength: -25.0238  max_strength: 314.143  final_strength: -155.5  sample_efficiency: 8.66315e-06  training_efficiency: 3.24535e-05  stability: -3.52404
[2021-06-17 03:28:58,228 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 160000  wall_t: 988  opt_step: 112680  frame: 160000  fps: 161.943  total_reward: 512.5  total_reward_ma: 848.14  loss: 0.00389463  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:28:58,345 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 848.14  strength: 205.14  max_strength: 857  final_strength: -130.5  sample_efficiency: 6.00221e-06  training_efficiency: 2.00635e-05  stability: 0.432274
[2021-06-17 03:28:58,814 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 160000  wall_t: 988  opt_step: 112680  frame: 160000  fps: 161.943  total_reward: 625  total_reward_ma: 736.942  loss: 9.29562e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:28:58,894 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 736.942  strength: 93.942  max_strength: 419.5  final_strength: -18  sample_efficiency: -6.98768e-06  training_efficiency: 9.62126e-06  stability: 0.465837
[2021-06-17 03:29:41,320 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 170000  wall_t: 1031  opt_step: 120192  frame: 170000  fps: 164.888  total_reward: 400  total_reward_ma: 628.466  loss: 0.00391281  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:29:41,378 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 628.466  strength: -14.5336  max_strength: 557  final_strength: -243  sample_efficiency: 0.000209775  training_efficiency: 1.19811e-05  stability: -422.68
[2021-06-17 03:29:42,888 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 170000  wall_t: 1032  opt_step: 120192  frame: 170000  fps: 164.729  total_reward: 500  total_reward_ma: 610.603  loss: 4.12383e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:29:43,009 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 610.603  strength: -32.3973  max_strength: 314.143  final_strength: -143  sample_efficiency: 7.89601e-06  training_efficiency: 2.57958e-05  stability: -1.64986
[2021-06-17 03:29:44,269 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 170000  wall_t: 1034  opt_step: 120192  frame: 170000  fps: 164.41  total_reward: 725  total_reward_ma: 840.896  loss: 1.71758e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:29:44,356 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 840.896  strength: 197.896  max_strength: 857  final_strength: 82  sample_efficiency: 5.99929e-06  training_efficiency: 1.97773e-05  stability: 0.409702
[2021-06-17 03:29:45,201 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 170000  wall_t: 1035  opt_step: 120192  frame: 170000  fps: 164.251  total_reward: 512.5  total_reward_ma: 723.74  loss: 3.18242e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:29:45,278 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 723.74  strength: 80.7395  max_strength: 419.5  final_strength: -130.5  sample_efficiency: -8.21133e-06  training_efficiency: 9.74498e-06  stability: 0.384593
[2021-06-17 03:30:28,134 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 180000  wall_t: 1078  opt_step: 127704  frame: 180000  fps: 166.976  total_reward: 737.5  total_reward_ma: 634.524  loss: 1.02001e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:30:28,281 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 634.524  strength: -8.47619  max_strength: 557  final_strength: 94.5  sample_efficiency: 0.000336264  training_efficiency: 1.45519e-05  stability: -5.98179
[2021-06-17 03:30:29,780 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 180000  wall_t: 1079  opt_step: 127704  frame: 180000  fps: 166.821  total_reward: 625  total_reward_ma: 611.45  loss: 8.49286e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:30:29,870 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 611.45  strength: -31.5504  max_strength: 314.143  final_strength: -18  sample_efficiency: 7.81746e-06  training_efficiency: 2.51929e-05  stability: -0.918837
[2021-06-17 03:30:31,125 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 180000  wall_t: 1081  opt_step: 127704  frame: 180000  fps: 166.512  total_reward: 762.5  total_reward_ma: 836.541  loss: 1.92871e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:30:31,204 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 836.541  strength: 193.541  max_strength: 857  final_strength: 119.5  sample_efficiency: 5.98407e-06  training_efficiency: 1.93675e-05  stability: 0.424089
[2021-06-17 03:30:32,256 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 180000  wall_t: 1082  opt_step: 127704  frame: 180000  fps: 166.359  total_reward: 737.5  total_reward_ma: 724.504  loss: 0.00391163  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:30:32,359 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 724.504  strength: 81.504  max_strength: 419.5  final_strength: 94.5  sample_efficiency: -7.32455e-06  training_efficiency: 9.62167e-06  stability: 0.326082
[2021-06-17 03:31:14,541 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 190000  wall_t: 1124  opt_step: 135216  frame: 190000  fps: 169.039  total_reward: 750  total_reward_ma: 640.602  loss: 1.07315e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:31:14,631 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 640.602  strength: -2.3985  max_strength: 557  final_strength: 107  sample_efficiency: 0.00111344  training_efficiency: 3.13547e-05  stability: -10.3062
[2021-06-17 03:31:15,562 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 190000  wall_t: 1125  opt_step: 135216  frame: 190000  fps: 168.889  total_reward: 1112.5  total_reward_ma: 639.286  loss: 4.69163e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:31:15,615 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 639.286  strength: -3.71428  max_strength: 469.5  final_strength: 469.5  sample_efficiency: 2.57549e-05  training_efficiency: 0.000150173  stability: -0.854442
[2021-06-17 03:31:17,149 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 190000  wall_t: 1127  opt_step: 135216  frame: 190000  fps: 168.589  total_reward: 575  total_reward_ma: 822.776  loss: 6.13734e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:31:17,225 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 822.776  strength: 179.776  max_strength: 857  final_strength: -68  sample_efficiency: 5.99842e-06  training_efficiency: 1.96058e-05  stability: 0.390023
[2021-06-17 03:31:18,425 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 190000  wall_t: 1128  opt_step: 135216  frame: 190000  fps: 168.44  total_reward: 550  total_reward_ma: 715.32  loss: 4.61812e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:31:18,503 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 715.32  strength: 72.3195  max_strength: 419.5  final_strength: -93  sample_efficiency: -8.17651e-06  training_efficiency: 9.77234e-06  stability: 0.241687
[2021-06-17 03:32:01,271 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 200000  wall_t: 1171  opt_step: 142728  frame: 200000  fps: 170.794  total_reward: 675  total_reward_ma: 642.321  loss: 1.34639e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:32:01,365 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 642.321  strength: -0.678574  max_strength: 557  final_strength: 32  sample_efficiency: 0.00372702  training_efficiency: 8.87654e-05  stability: -38.4984
[2021-06-17 03:32:02,310 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 200000  wall_t: 1172  opt_step: 142728  frame: 200000  fps: 170.648  total_reward: 937.5  total_reward_ma: 654.981  loss: 1.19119e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:32:02,403 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 654.981  strength: 11.9812  max_strength: 469.5  final_strength: 294.5  sample_efficiency: -1.09557e-06  training_efficiency: -3.50408e-05  stability: -16.4947
[2021-06-17 03:32:03,345 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 200000  wall_t: 1173  opt_step: 142728  frame: 200000  fps: 170.503  total_reward: 612.5  total_reward_ma: 812.262  loss: 7.59795e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:32:03,444 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 812.262  strength: 169.262  max_strength: 857  final_strength: -30.5  sample_efficiency: 6.00741e-06  training_efficiency: 1.97194e-05  stability: 0.37788
[2021-06-17 03:32:04,672 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 200000  wall_t: 1174  opt_step: 142728  frame: 200000  fps: 170.358  total_reward: 612.5  total_reward_ma: 710.179  loss: 3.57732e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:32:04,767 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 710.179  strength: 67.1786  max_strength: 419.5  final_strength: -30.5  sample_efficiency: -8.47563e-06  training_efficiency: 9.83513e-06  stability: 0.190362
[2021-06-17 03:32:47,474 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 210000  wall_t: 1217  opt_step: 150240  frame: 210000  fps: 172.555  total_reward: 875  total_reward_ma: 653.401  loss: 0.00391222  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:32:47,575 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 653.401  strength: 10.4014  max_strength: 557  final_strength: 232  sample_efficiency: -0.000226511  training_efficiency: 1.55437e-06  stability: -131.631
[2021-06-17 03:32:48,426 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 210000  wall_t: 1218  opt_step: 150240  frame: 210000  fps: 172.414  total_reward: 487.5  total_reward_ma: 646.607  loss: 0.00392505  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:32:48,516 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 646.607  strength: 3.60714  max_strength: 469.5  final_strength: -155.5  sample_efficiency: -1.3721e-05  training_efficiency: -0.000124916  stability: -6.11484
[2021-06-17 03:32:49,186 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 210000  wall_t: 1219  opt_step: 150240  frame: 210000  fps: 172.272  total_reward: 600  total_reward_ma: 802.154  loss: 0.0077884  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:32:49,268 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 802.154  strength: 159.154  max_strength: 857  final_strength: -43  sample_efficiency: 6.02344e-06  training_efficiency: 1.98874e-05  stability: 0.368582
[2021-06-17 03:32:50,999 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 210000  wall_t: 1221  opt_step: 150240  frame: 210000  fps: 171.99  total_reward: 600  total_reward_ma: 704.932  loss: 1.62791e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:32:51,065 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 704.932  strength: 61.932  max_strength: 419.5  final_strength: -43  sample_efficiency: -8.91329e-06  training_efficiency: 9.94024e-06  stability: 0.162679
[2021-06-17 03:33:33,609 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 220000  wall_t: 1263  opt_step: 157752  frame: 220000  fps: 174.188  total_reward: 925  total_reward_ma: 665.747  loss: 0.0039098  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:33:33,710 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 665.747  strength: 22.7468  max_strength: 557  final_strength: 282  sample_efficiency: -9.63067e-05  training_efficiency: 4.25063e-06  stability: -7.24068
[2021-06-17 03:33:34,643 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 220000  wall_t: 1264  opt_step: 157752  frame: 220000  fps: 174.051  total_reward: 575  total_reward_ma: 643.197  loss: 7.45058e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:33:34,704 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 643.197  strength: 0.19728  max_strength: 469.5  final_strength: -68  sample_efficiency: -0.000313541  training_efficiency: -0.00227929  stability: -21.4505
[2021-06-17 03:33:35,022 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 220000  wall_t: 1265  opt_step: 157752  frame: 220000  fps: 173.913  total_reward: 587.5  total_reward_ma: 792.397  loss: 2.58451e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:33:35,094 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 792.397  strength: 149.397  max_strength: 857  final_strength: -55.5  sample_efficiency: 6.0484e-06  training_efficiency: 2.01162e-05  stability: 0.356718
[2021-06-17 03:33:37,391 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 220000  wall_t: 1267  opt_step: 157752  frame: 220000  fps: 173.639  total_reward: 862.5  total_reward_ma: 712.094  loss: 4.82099e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:33:37,498 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 712.094  strength: 69.0942  max_strength: 419.5  final_strength: 219.5  sample_efficiency: -6.96983e-06  training_efficiency: 9.42023e-06  stability: 0.134996
[2021-06-17 03:34:19,793 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 230000  wall_t: 1309  opt_step: 165264  frame: 230000  fps: 175.707  total_reward: 1350  total_reward_ma: 695.497  loss: 3.80963e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:34:19,958 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 695.497  strength: 52.4969  max_strength: 707  final_strength: 707  sample_efficiency: -3.73693e-05  training_efficiency: 5.30478e-06  stability: -2.59692
[2021-06-17 03:34:20,773 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 230000  wall_t: 1310  opt_step: 165264  frame: 230000  fps: 175.573  total_reward: 587.5  total_reward_ma: 640.666  loss: 4.34388e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:34:20,864 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 640.666  strength: -2.33441  max_strength: 469.5  final_strength: -55.5  sample_efficiency: 2.99914e-05  training_efficiency: 0.000190405  stability: -389.946
[2021-06-17 03:34:21,093 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 230000  wall_t: 1311  opt_step: 165264  frame: 230000  fps: 175.439  total_reward: 487.5  total_reward_ma: 779.141  loss: 3.48604e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:34:21,160 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 779.141  strength: 136.141  max_strength: 857  final_strength: -155.5  sample_efficiency: 6.13285e-06  training_efficiency: 2.08147e-05  stability: 0.315431
[2021-06-17 03:34:23,807 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 230000  wall_t: 1313  opt_step: 165264  frame: 230000  fps: 175.171  total_reward: 550  total_reward_ma: 705.047  loss: 4.02405e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:34:23,880 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 705.047  strength: 62.0466  max_strength: 419.5  final_strength: -93  sample_efficiency: -7.70739e-06  training_efficiency: 9.6398e-06  stability: 0.0543208
[2021-06-17 03:35:05,800 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 240000  wall_t: 1355  opt_step: 172776  frame: 240000  fps: 177.122  total_reward: 1262.5  total_reward_ma: 719.122  loss: 1.16397e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:35:05,990 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 719.122  strength: 76.122  max_strength: 707  final_strength: 619.5  sample_efficiency: -2.32847e-05  training_efficiency: 5.46858e-06  stability: -0.56324
[2021-06-17 03:35:06,953 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 240000  wall_t: 1357  opt_step: 172776  frame: 240000  fps: 176.861  total_reward: 575  total_reward_ma: 637.811  loss: 0.00392399  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:35:07,003 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 637.811  strength: -5.18944  max_strength: 469.5  final_strength: -68  sample_efficiency: 1.52785e-05  training_efficiency: 8.52253e-05  stability: -30.7803
[2021-06-17 03:35:07,224 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 240000  wall_t: 1357  opt_step: 172776  frame: 240000  fps: 176.861  total_reward: 787.5  total_reward_ma: 779.489  loss: 7.02083e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:35:07,274 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 779.489  strength: 136.489  max_strength: 857  final_strength: 144.5  sample_efficiency: 6.04612e-06  training_efficiency: 2.01518e-05  stability: 0.281434
[2021-06-17 03:35:09,657 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 240000  wall_t: 1359  opt_step: 172776  frame: 240000  fps: 176.6  total_reward: 825  total_reward_ma: 710.045  loss: 1.776e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:35:09,744 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 710.045  strength: 67.0446  max_strength: 419.5  final_strength: 182  sample_efficiency: -6.36433e-06  training_efficiency: 9.20411e-06  stability: -0.00730765
[2021-06-17 03:35:52,471 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 250000  wall_t: 1402  opt_step: 180288  frame: 250000  fps: 178.317  total_reward: 687.5  total_reward_ma: 717.857  loss: 1.40928e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:35:52,605 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 717.857  strength: 74.8571  max_strength: 707  final_strength: 44.5  sample_efficiency: -2.26359e-05  training_efficiency: 5.47044e-06  stability: -0.347891
[2021-06-17 03:35:53,366 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 250000  wall_t: 1403  opt_step: 180288  frame: 250000  fps: 178.19  total_reward: 737.5  total_reward_ma: 777.81  loss: 1.52676e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:35:53,434 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 777.81  strength: 134.81  max_strength: 857  final_strength: 94.5  sample_efficiency: 5.98874e-06  training_efficiency: 1.97423e-05  stability: 0.297868
[2021-06-17 03:35:53,722 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 250000  wall_t: 1403  opt_step: 180288  frame: 250000  fps: 178.19  total_reward: 587.5  total_reward_ma: 635.714  loss: 4.68722e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:35:53,803 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 635.714  strength: -7.28571  max_strength: 469.5  final_strength: -55.5  sample_efficiency: 1.16987e-05  training_efficiency: 5.99352e-05  stability: -12.6744
[2021-06-17 03:35:56,444 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 250000  wall_t: 1406  opt_step: 180288  frame: 250000  fps: 177.809  total_reward: 925  total_reward_ma: 718.643  loss: 4.00104e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:35:56,554 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 718.643  strength: 75.6429  max_strength: 419.5  final_strength: 282  sample_efficiency: -4.81878e-06  training_efficiency: 8.6587e-06  stability: 0.106628
[2021-06-17 03:36:39,085 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 260000  wall_t: 1449  opt_step: 187800  frame: 260000  fps: 179.434  total_reward: 637.5  total_reward_ma: 714.767  loss: 1.40952e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:36:39,214 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 714.767  strength: 71.7665  max_strength: 707  final_strength: -5.5  sample_efficiency: -2.2714e-05  training_efficiency: 5.47087e-06  stability: -0.342557
[2021-06-17 03:36:39,440 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 260000  wall_t: 1449  opt_step: 187800  frame: 260000  fps: 179.434  total_reward: 850  total_reward_ma: 780.586  loss: 0.00390115  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:36:39,490 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 780.586  strength: 137.586  max_strength: 857  final_strength: 207  sample_efficiency: 5.86476e-06  training_efficiency: 1.8908e-05  stability: 0.317556
[2021-06-17 03:36:39,815 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 260000  wall_t: 1449  opt_step: 187800  frame: 260000  fps: 179.434  total_reward: 850  total_reward_ma: 644.286  loss: 0.00390328  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:36:39,890 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 644.286  strength: 1.28572  max_strength: 469.5  final_strength: 207  sample_efficiency: -3.88717e-05  training_efficiency: -0.000291755  stability: -8.33415
[2021-06-17 03:36:42,393 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 260000  wall_t: 1452  opt_step: 187800  frame: 260000  fps: 179.063  total_reward: 650  total_reward_ma: 716.003  loss: 1.84113e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:36:42,481 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 716.003  strength: 73.0027  max_strength: 419.5  final_strength: 7  sample_efficiency: -4.78682e-06  training_efficiency: 8.64641e-06  stability: 0.0944287
[2021-06-17 03:37:25,559 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 270000  wall_t: 1495  opt_step: 195312  frame: 270000  fps: 180.602  total_reward: 800  total_reward_ma: 717.923  loss: 0.00393534  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:37:25,601 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 717.923  strength: 74.9233  max_strength: 707  final_strength: 157  sample_efficiency: -2.06637e-05  training_efficiency: 5.44364e-06  stability: -0.346515
[2021-06-17 03:37:25,958 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 270000  wall_t: 1496  opt_step: 195312  frame: 270000  fps: 180.481  total_reward: 812.5  total_reward_ma: 650.755  loss: 7.71335e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:37:26,046 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 650.755  strength: 7.7555  max_strength: 469.5  final_strength: 169.5  sample_efficiency: -3.08302e-06  training_efficiency: -4.22034e-05  stability: -50.9444
[2021-06-17 03:37:26,231 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 270000  wall_t: 1496  opt_step: 195312  frame: 270000  fps: 180.481  total_reward: 787.5  total_reward_ma: 780.842  loss: 4.02781e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:37:26,295 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 780.842  strength: 137.842  max_strength: 857  final_strength: 144.5  sample_efficiency: 5.78086e-06  training_efficiency: 1.83727e-05  stability: 0.339574
[2021-06-17 03:37:29,291 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 270000  wall_t: 1499  opt_step: 195312  frame: 270000  fps: 180.12  total_reward: 575  total_reward_ma: 710.78  loss: 0.00392236  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:37:29,448 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 710.78  strength: 67.7804  max_strength: 419.5  final_strength: -68  sample_efficiency: -5.10231e-06  training_efficiency: 8.77744e-06  stability: 0.0582546
[2021-06-17 03:38:11,830 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 280000  wall_t: 1541  opt_step: 202824  frame: 280000  fps: 181.7  total_reward: 525  total_reward_ma: 711.033  loss: 1.22247e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:38:11,897 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 711.033  strength: 68.0332  max_strength: 707  final_strength: -118  sample_efficiency: -2.21649e-05  training_efficiency: 5.47543e-06  stability: -0.377953
[2021-06-17 03:38:12,048 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 280000  wall_t: 1542  opt_step: 202824  frame: 280000  fps: 181.582  total_reward: 650  total_reward_ma: 776.169  loss: 5.04421e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:38:12,098 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 776.169  strength: 133.169  max_strength: 857  final_strength: 7  sample_efficiency: 5.77671e-06  training_efficiency: 1.83475e-05  stability: 0.328271
[2021-06-17 03:38:12,503 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 280000  wall_t: 1542  opt_step: 202824  frame: 280000  fps: 181.582  total_reward: 787.5  total_reward_ma: 655.82  loss: 2.71535e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:38:12,552 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 655.82  strength: 12.8201  max_strength: 469.5  final_strength: 144.5  sample_efficiency: -3.05072e-07  training_efficiency: -2.2527e-05  stability: -7.40418
[2021-06-17 03:38:15,899 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 280000  wall_t: 1546  opt_step: 202824  frame: 280000  fps: 181.113  total_reward: 462.5  total_reward_ma: 701.913  loss: 1.59239e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:38:16,012 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 701.913  strength: 58.9133  max_strength: 419.5  final_strength: -180.5  sample_efficiency: -6.05141e-06  training_efficiency: 9.19839e-06  stability: -0.0382109
[2021-06-17 03:38:58,224 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 290000  wall_t: 1588  opt_step: 210336  frame: 290000  fps: 182.62  total_reward: 900  total_reward_ma: 717.549  loss: 1.53202e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:38:58,268 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 717.549  strength: 74.5493  max_strength: 707  final_strength: 257  sample_efficiency: -1.91201e-05  training_efficiency: 5.38971e-06  stability: -0.46331
[2021-06-17 03:38:58,329 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 290000  wall_t: 1588  opt_step: 210336  frame: 290000  fps: 182.62  total_reward: 700  total_reward_ma: 773.543  loss: 1.4296e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:38:58,385 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 773.543  strength: 130.543  max_strength: 857  final_strength: 57  sample_efficiency: 5.74165e-06  training_efficiency: 1.81428e-05  stability: 0.329532
[2021-06-17 03:38:59,107 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 290000  wall_t: 1589  opt_step: 210336  frame: 290000  fps: 182.505  total_reward: 812.5  total_reward_ma: 661.416  loss: 1.10426e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:38:59,160 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 661.416  strength: 18.4158  max_strength: 469.5  final_strength: 169.5  sample_efficiency: 9.28713e-07  training_efficiency: -1.35592e-05  stability: -3.89579
[2021-06-17 03:39:02,314 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 290000  wall_t: 1592  opt_step: 210336  frame: 290000  fps: 182.161  total_reward: 962.5  total_reward_ma: 710.899  loss: 0.00394093  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:39:02,428 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 710.899  strength: 67.899  max_strength: 419.5  final_strength: 319.5  sample_efficiency: -4.51e-06  training_efficiency: 8.4773e-06  stability: -0.151814
[2021-06-17 03:39:44,132 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 300000  wall_t: 1634  opt_step: 217848  frame: 300000  fps: 183.599  total_reward: 987.5  total_reward_ma: 726.548  loss: 1.51958e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:39:44,303 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 300000  wall_t: 1634  opt_step: 217848  frame: 300000  fps: 183.599  total_reward: 812.5  total_reward_ma: 774.841  loss: 5.45223e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:39:44,334 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 726.548  strength: 83.5476  max_strength: 707  final_strength: 344.5  sample_efficiency: -1.6034e-05  training_efficiency: 5.27984e-06  stability: -0.289358
[2021-06-17 03:39:44,358 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 774.841  strength: 131.841  max_strength: 857  final_strength: 169.5  sample_efficiency: 5.63844e-06  training_efficiency: 1.7562e-05  stability: 0.339627
[2021-06-17 03:39:45,228 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 300000  wall_t: 1635  opt_step: 217848  frame: 300000  fps: 183.486  total_reward: 675  total_reward_ma: 661.884  loss: 4.47704e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:39:45,313 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 661.884  strength: 18.8842  max_strength: 469.5  final_strength: 32  sample_efficiency: 1.06922e-06  training_efficiency: -1.24987e-05  stability: -2.55312
[2021-06-17 03:39:48,828 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 300000  wall_t: 1638  opt_step: 217848  frame: 300000  fps: 183.15  total_reward: 712.5  total_reward_ma: 710.952  loss: 9.0726e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:39:48,910 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 710.952  strength: 67.9524  max_strength: 419.5  final_strength: 69.5  sample_efficiency: -4.2426e-06  training_efficiency: 8.34478e-06  stability: -0.0918852
[2021-06-17 03:40:30,401 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 310000  wall_t: 1680  opt_step: 225360  frame: 310000  fps: 184.524  total_reward: 887.5  total_reward_ma: 778.475  loss: 0.00392917  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:40:30,516 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 778.475  strength: 135.475  max_strength: 857  final_strength: 244.5  sample_efficiency: 5.49798e-06  training_efficiency: 1.67979e-05  stability: 0.367927
[2021-06-17 03:40:30,634 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 310000  wall_t: 1680  opt_step: 225360  frame: 310000  fps: 184.524  total_reward: 862.5  total_reward_ma: 730.933  loss: 7.30106e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:40:30,775 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 730.933  strength: 87.9332  max_strength: 707  final_strength: 219.5  sample_efficiency: -1.44831e-05  training_efficiency: 5.212e-06  stability: -0.162012
[2021-06-17 03:40:31,891 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 310000  wall_t: 1682  opt_step: 225360  frame: 310000  fps: 184.304  total_reward: 1025  total_reward_ma: 673.988  loss: 4.57928e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:40:31,996 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 673.988  strength: 30.9881  max_strength: 469.5  final_strength: 382  sample_efficiency: 1.95538e-06  training_efficiency: -5.5395e-06  stability: -2.34551
[2021-06-17 03:40:35,592 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 310000  wall_t: 1685  opt_step: 225360  frame: 310000  fps: 183.976  total_reward: 950  total_reward_ma: 718.664  loss: 3.4355e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:40:35,659 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 718.664  strength: 75.6636  max_strength: 419.5  final_strength: 307  sample_efficiency: -3.2651e-06  training_efficiency: 7.83336e-06  stability: -0.0546601
[2021-06-17 03:41:16,430 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 320000  wall_t: 1726  opt_step: 232872  frame: 320000  fps: 185.4  total_reward: 862.5  total_reward_ma: 781.101  loss: 0.0039115  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:41:16,478 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 781.101  strength: 138.101  max_strength: 857  final_strength: 219.5  sample_efficiency: 5.38012e-06  training_efficiency: 1.61769e-05  stability: 0.398772
[2021-06-17 03:41:16,780 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 320000  wall_t: 1726  opt_step: 232872  frame: 320000  fps: 185.4  total_reward: 537.5  total_reward_ma: 724.888  loss: 4.52983e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:41:16,825 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 724.888  strength: 81.8884  max_strength: 707  final_strength: -105.5  sample_efficiency: -1.5192e-05  training_efficiency: 5.24895e-06  stability: -0.187669
[2021-06-17 03:41:18,321 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 320000  wall_t: 1728  opt_step: 232872  frame: 320000  fps: 185.185  total_reward: 575  total_reward_ma: 670.795  loss: 8.63361e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:41:18,388 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 670.795  strength: 27.7949  max_strength: 469.5  final_strength: -68  sample_efficiency: 1.86308e-06  training_efficiency: -6.31557e-06  stability: -1.45486
[2021-06-17 03:41:21,723 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 320000  wall_t: 1731  opt_step: 232872  frame: 320000  fps: 184.864  total_reward: 600  total_reward_ma: 714.955  loss: 0.00391017  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:41:21,789 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 714.955  strength: 71.9554  max_strength: 419.5  final_strength: -43  sample_efficiency: -3.38443e-06  training_efficiency: 7.89945e-06  stability: -0.0658385
[2021-06-17 03:42:02,619 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 330000  wall_t: 1772  opt_step: 240384  frame: 330000  fps: 186.23  total_reward: 1175  total_reward_ma: 793.038  loss: 0.0117115  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:42:02,663 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 793.038  strength: 150.038  max_strength: 857  final_strength: 532  sample_efficiency: 5.12764e-06  training_efficiency: 1.48857e-05  stability: 0.428635
[2021-06-17 03:42:02,849 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 330000  wall_t: 1772  opt_step: 240384  frame: 330000  fps: 186.23  total_reward: 700  total_reward_ma: 724.134  loss: 7.52655e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:42:02,963 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 724.134  strength: 81.1342  max_strength: 707  final_strength: 57  sample_efficiency: -1.48041e-05  training_efficiency: 5.22577e-06  stability: -0.235485
[2021-06-17 03:42:04,330 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 330000  wall_t: 1774  opt_step: 240384  frame: 330000  fps: 186.02  total_reward: 600  total_reward_ma: 668.583  loss: 7.29209e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:42:04,378 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 668.583  strength: 25.5826  max_strength: 469.5  final_strength: -43  sample_efficiency: 1.80177e-06  training_efficiency: -6.86581e-06  stability: -1.64859
[2021-06-17 03:42:07,543 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 330000  wall_t: 1777  opt_step: 240384  frame: 330000  fps: 185.706  total_reward: 575  total_reward_ma: 710.714  loss: 5.73e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:42:07,590 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 710.714  strength: 67.7143  max_strength: 419.5  final_strength: -68  sample_efficiency: -3.57964e-06  training_efficiency: 8.01324e-06  stability: -0.0966002
[2021-06-17 03:42:49,076 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 340000  wall_t: 1819  opt_step: 247896  frame: 340000  fps: 186.916  total_reward: 1137.5  total_reward_ma: 803.169  loss: 9.36095e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:42:49,155 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 803.169  strength: 160.169  max_strength: 857  final_strength: 494.5  sample_efficiency: 4.92909e-06  training_efficiency: 1.39003e-05  stability: 0.482453
[2021-06-17 03:42:49,210 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 340000  wall_t: 1819  opt_step: 247896  frame: 340000  fps: 186.916  total_reward: 1087.5  total_reward_ma: 734.821  loss: 4.43593e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:42:49,329 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 734.821  strength: 91.8214  max_strength: 707  final_strength: 444.5  sample_efficiency: -1.22775e-05  training_efficiency: 5.05608e-06  stability: -0.209183
[2021-06-17 03:42:50,240 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 340000  wall_t: 1820  opt_step: 247896  frame: 340000  fps: 186.813  total_reward: 587.5  total_reward_ma: 666.126  loss: 6.9182e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:42:50,282 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 666.126  strength: 23.1255  max_strength: 469.5  final_strength: -55.5  sample_efficiency: 1.71891e-06  training_efficiency: -7.6585e-06  stability: -1.80298
[2021-06-17 03:42:54,192 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 340000  wall_t: 1824  opt_step: 247896  frame: 340000  fps: 186.404  total_reward: 712.5  total_reward_ma: 710.767  loss: 9.56096e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:42:54,251 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 710.767  strength: 67.7668  max_strength: 419.5  final_strength: 69.5  sample_efficiency: -3.38294e-06  training_efficiency: 7.89321e-06  stability: -0.129971
[2021-06-17 03:43:35,573 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 350000  wall_t: 1865  opt_step: 255408  frame: 350000  fps: 187.668  total_reward: 987.5  total_reward_ma: 808.435  loss: 4.4936e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:43:35,612 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 808.435  strength: 165.435  max_strength: 857  final_strength: 344.5  sample_efficiency: 4.80582e-06  training_efficiency: 1.33062e-05  stability: 0.501904
[2021-06-17 03:43:35,936 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 350000  wall_t: 1866  opt_step: 255408  frame: 350000  fps: 187.567  total_reward: 825  total_reward_ma: 737.398  loss: 0.00393095  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:43:36,075 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 737.398  strength: 94.398  max_strength: 707  final_strength: 182  sample_efficiency: -1.14438e-05  training_efficiency: 4.99324e-06  stability: -0.121102
[2021-06-17 03:43:36,863 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 350000  wall_t: 1866  opt_step: 255408  frame: 350000  fps: 187.567  total_reward: 587.5  total_reward_ma: 663.813  loss: 6.15255e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:43:36,922 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 663.813  strength: 20.813  max_strength: 469.5  final_strength: -55.5  sample_efficiency: 1.62964e-06  training_efficiency: -8.56623e-06  stability: -2.00683
[2021-06-17 03:43:40,414 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 350000  wall_t: 1870  opt_step: 255408  frame: 350000  fps: 187.166  total_reward: 900  total_reward_ma: 716.173  loss: 0.00402902  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:43:40,519 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 716.173  strength: 73.1735  max_strength: 419.5  final_strength: 257  sample_efficiency: -2.75676e-06  training_efficiency: 7.49404e-06  stability: -0.0958862
[2021-06-17 03:44:21,965 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 360000  wall_t: 1912  opt_step: 262920  frame: 360000  fps: 188.285  total_reward: 737.5  total_reward_ma: 806.465  loss: 1.76781e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:44:22,007 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 806.465  strength: 163.465  max_strength: 857  final_strength: 94.5  sample_efficiency: 4.77325e-06  training_efficiency: 1.31536e-05  stability: 0.488363
[2021-06-17 03:44:22,597 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 360000  wall_t: 1912  opt_step: 262920  frame: 360000  fps: 188.285  total_reward: 800  total_reward_ma: 739.137  loss: 3.52341e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:44:22,756 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 739.137  strength: 96.1369  max_strength: 707  final_strength: 157  sample_efficiency: -1.07987e-05  training_efficiency: 4.93926e-06  stability: -0.0669117
[2021-06-17 03:44:23,274 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 360000  wall_t: 1913  opt_step: 262920  frame: 360000  fps: 188.186  total_reward: 612.5  total_reward_ma: 662.347  loss: 7.24051e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:44:23,307 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 662.347  strength: 19.3469  max_strength: 469.5  final_strength: -30.5  sample_efficiency: 1.57792e-06  training_efficiency: -9.12338e-06  stability: -2.24266
[2021-06-17 03:44:26,834 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 360000  wall_t: 1916  opt_step: 262920  frame: 360000  fps: 187.891  total_reward: 912.5  total_reward_ma: 721.627  loss: 6.48751e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:44:26,881 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 721.627  strength: 78.627  max_strength: 419.5  final_strength: 269.5  sample_efficiency: -2.22981e-06  training_efficiency: 7.14265e-06  stability: 0.0140845
[2021-06-17 03:45:07,939 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 370000  wall_t: 1958  opt_step: 270432  frame: 370000  fps: 188.968  total_reward: 987.5  total_reward_ma: 811.358  loss: 2.74948e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:45:07,983 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 811.358  strength: 168.358  max_strength: 857  final_strength: 344.5  sample_efficiency: 4.65874e-06  training_efficiency: 1.26307e-05  stability: 0.496579
[2021-06-17 03:45:09,208 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 370000  wall_t: 1959  opt_step: 270432  frame: 370000  fps: 188.872  total_reward: 500  total_reward_ma: 732.674  loss: 1.23332e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:45:09,258 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 732.674  strength: 89.6737  max_strength: 707  final_strength: -143  sample_efficiency: -1.13806e-05  training_efficiency: 4.99277e-06  stability: -0.105195
[2021-06-17 03:45:09,899 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 370000  wall_t: 1960  opt_step: 270432  frame: 370000  fps: 188.776  total_reward: 712.5  total_reward_ma: 663.74  loss: 3.81757e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:45:09,950 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 663.74  strength: 20.7401  max_strength: 469.5  final_strength: 69.5  sample_efficiency: 1.68262e-06  training_efficiency: -7.92995e-06  stability: -2.38871
[2021-06-17 03:45:13,110 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 370000  wall_t: 1963  opt_step: 270432  frame: 370000  fps: 188.487  total_reward: 612.5  total_reward_ma: 718.678  loss: 7.85384e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:45:13,224 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 718.678  strength: 75.6776  max_strength: 419.5  final_strength: -30.5  sample_efficiency: -2.28354e-06  training_efficiency: 7.18018e-06  stability: 0.00196826
[2021-06-17 03:45:54,297 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 380000  wall_t: 2004  opt_step: 277944  frame: 380000  fps: 189.621  total_reward: 825  total_reward_ma: 811.717  loss: 5.10163e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:45:54,356 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 811.717  strength: 168.717  max_strength: 857  final_strength: 182  sample_efficiency: 4.6012e-06  training_efficiency: 1.23743e-05  stability: 0.498334
[2021-06-17 03:45:55,077 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 380000  wall_t: 2005  opt_step: 277944  frame: 380000  fps: 189.526  total_reward: 725  total_reward_ma: 732.472  loss: 4.50335e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:45:55,126 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 732.472  strength: 89.4718  max_strength: 707  final_strength: 82  sample_efficiency: -1.10426e-05  training_efficiency: 4.95913e-06  stability: -0.152828
[2021-06-17 03:45:56,096 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 380000  wall_t: 2006  opt_step: 277944  frame: 380000  fps: 189.432  total_reward: 625  total_reward_ma: 662.693  loss: 2.73727e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:45:56,160 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 662.693  strength: 19.6931  max_strength: 469.5  final_strength: -18  sample_efficiency: 1.65918e-06  training_efficiency: -8.21472e-06  stability: -2.19047
[2021-06-17 03:45:59,818 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 380000  wall_t: 2009  opt_step: 277944  frame: 380000  fps: 189.149  total_reward: 800  total_reward_ma: 720.818  loss: 0.00775325  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:45:59,906 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 720.818  strength: 77.8177  max_strength: 419.5  final_strength: 157  sample_efficiency: -2.02258e-06  training_efficiency: 6.98998e-06  stability: -0.00890291
[2021-06-17 03:46:40,646 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 390000  wall_t: 2050  opt_step: 285456  frame: 390000  fps: 190.244  total_reward: 662.5  total_reward_ma: 807.891  loss: 5.237e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:46:40,689 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 807.891  strength: 164.891  max_strength: 857  final_strength: 19.5  sample_efficiency: 4.59502e-06  training_efficiency: 1.23474e-05  stability: 0.487229
[2021-06-17 03:46:40,830 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 390000  wall_t: 2050  opt_step: 285456  frame: 390000  fps: 190.244  total_reward: 412.5  total_reward_ma: 724.267  loss: 1.26371e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:46:40,887 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 724.267  strength: 81.2674  max_strength: 707  final_strength: -230.5  sample_efficiency: -1.20322e-05  training_efficiency: 5.06501e-06  stability: -0.216937
[2021-06-17 03:46:42,289 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 390000  wall_t: 2052  opt_step: 285456  frame: 390000  fps: 190.058  total_reward: 875  total_reward_ma: 668.28  loss: 0.000377439  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:46:42,347 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 668.28  strength: 25.2801  max_strength: 469.5  final_strength: 232  sample_efficiency: 1.87772e-06  training_efficiency: -5.38479e-06  stability: -2.26929
[2021-06-17 03:46:45,796 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 390000  wall_t: 2055  opt_step: 285456  frame: 390000  fps: 189.781  total_reward: 1712.5  total_reward_ma: 746.245  loss: 1.18764e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:46:45,894 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 746.245  strength: 103.245  max_strength: 1069.5  final_strength: 1069.5  sample_efficiency: -8.04309e-07  training_efficiency: 6.06384e-06  stability: 0.0446629
[2021-06-17 03:47:27,110 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 400000  wall_t: 2097  opt_step: 292968  frame: 400000  fps: 190.749  total_reward: 712.5  total_reward_ma: 805.506  loss: 5.24844e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:47:27,167 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 805.506  strength: 162.506  max_strength: 857  final_strength: 69.5  sample_efficiency: 4.57262e-06  training_efficiency: 1.22518e-05  stability: 0.488783
[2021-06-17 03:47:27,602 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 400000  wall_t: 2097  opt_step: 292968  frame: 400000  fps: 190.749  total_reward: 662.5  total_reward_ma: 722.723  loss: 1.57863e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:47:27,653 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 722.723  strength: 79.7232  max_strength: 707  final_strength: 19.5  sample_efficiency: -1.19433e-05  training_efficiency: 5.05491e-06  stability: -0.30544
[2021-06-17 03:47:28,867 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 400000  wall_t: 2098  opt_step: 292968  frame: 400000  fps: 190.658  total_reward: 712.5  total_reward_ma: 669.414  loss: 0.00399409  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:47:28,924 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 669.414  strength: 26.4139  max_strength: 469.5  final_strength: 69.5  sample_efficiency: 1.9197e-06  training_efficiency: -4.79122e-06  stability: -1.6489
[2021-06-17 03:47:32,354 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 400000  wall_t: 2102  opt_step: 292968  frame: 400000  fps: 190.295  total_reward: 1650  total_reward_ma: 768.839  loss: 0.00391126  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:47:32,441 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 768.839  strength: 125.839  max_strength: 1069.5  final_strength: 1007  sample_efficiency: -1.4326e-07  training_efficiency: 5.53359e-06  stability: 0.282889
[2021-06-17 03:48:13,413 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 410000  wall_t: 2143  opt_step: 300480  frame: 410000  fps: 191.321  total_reward: 837.5  total_reward_ma: 806.286  loss: 1.82791e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:48:13,457 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 806.286  strength: 163.286  max_strength: 857  final_strength: 194.5  sample_efficiency: 4.51063e-06  training_efficiency: 1.19926e-05  stability: 0.494249
[2021-06-17 03:48:14,257 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 410000  wall_t: 2144  opt_step: 300480  frame: 410000  fps: 191.231  total_reward: 687.5  total_reward_ma: 721.864  loss: 0.00393811  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:48:14,299 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 721.864  strength: 78.8641  max_strength: 707  final_strength: 44.5  sample_efficiency: -1.17454e-05  training_efficiency: 5.03115e-06  stability: -0.297458
[2021-06-17 03:48:15,039 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 410000  wall_t: 2145  opt_step: 300480  frame: 410000  fps: 191.142  total_reward: 475  total_reward_ma: 664.554  loss: 5.14561e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:48:15,086 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 664.554  strength: 21.5536  max_strength: 469.5  final_strength: -168  sample_efficiency: 1.81851e-06  training_efficiency: -6.37336e-06  stability: -1.70074
[2021-06-17 03:48:18,566 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 410000  wall_t: 2148  opt_step: 300480  frame: 410000  fps: 190.875  total_reward: 862.5  total_reward_ma: 771.124  loss: 9.69022e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:48:18,712 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 771.124  strength: 128.124  max_strength: 1069.5  final_strength: 219.5  sample_efficiency: -3.53588e-08  training_efficiency: 5.44143e-06  stability: 0.269902
[2021-06-17 03:48:59,627 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 420000  wall_t: 2189  opt_step: 307992  frame: 420000  fps: 191.868  total_reward: 962.5  total_reward_ma: 810.006  loss: 1.35795e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:48:59,684 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 810.006  strength: 167.006  max_strength: 857  final_strength: 319.5  sample_efficiency: 4.41363e-06  training_efficiency: 1.15942e-05  stability: 0.508943
[2021-06-17 03:49:00,603 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 420000  wall_t: 2190  opt_step: 307992  frame: 420000  fps: 191.781  total_reward: 687.5  total_reward_ma: 721.046  loss: 4.8858e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:49:00,715 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 721.046  strength: 78.0459  max_strength: 707  final_strength: 44.5  sample_efficiency: -1.15536e-05  training_efficiency: 5.00692e-06  stability: -0.279602
[2021-06-17 03:49:01,515 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 420000  wall_t: 2191  opt_step: 307992  frame: 420000  fps: 191.693  total_reward: 687.5  total_reward_ma: 665.113  loss: 7.52345e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:49:01,582 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 665.113  strength: 22.1132  max_strength: 469.5  final_strength: 44.5  sample_efficiency: 1.84611e-06  training_efficiency: -5.90118e-06  stability: -2.22701
[2021-06-17 03:49:05,422 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 420000  wall_t: 2195  opt_step: 307992  frame: 420000  fps: 191.344  total_reward: 550  total_reward_ma: 765.859  loss: 0.00394498  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:49:05,471 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 765.859  strength: 122.859  max_strength: 1069.5  final_strength: -93  sample_efficiency: -7.8908e-08  training_efficiency: 5.48099e-06  stability: 0.24092
[2021-06-17 03:49:46,324 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 430000  wall_t: 2236  opt_step: 315504  frame: 430000  fps: 192.308  total_reward: 1237.5  total_reward_ma: 819.947  loss: 4.71658e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:49:46,380 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 819.947  strength: 176.947  max_strength: 857  final_strength: 594.5  sample_efficiency: 4.25048e-06  training_efficiency: 1.0936e-05  stability: 0.53131
[2021-06-17 03:49:46,879 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 430000  wall_t: 2236  opt_step: 315504  frame: 430000  fps: 192.308  total_reward: 375  total_reward_ma: 712.998  loss: 3.0174e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:49:46,940 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 712.998  strength: 69.9983  max_strength: 707  final_strength: -268  sample_efficiency: -1.27894e-05  training_efficiency: 5.17052e-06  stability: -0.357565
[2021-06-17 03:49:47,596 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 430000  wall_t: 2237  opt_step: 315504  frame: 430000  fps: 192.222  total_reward: 912.5  total_reward_ma: 671.003  loss: 8.18864e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:49:47,649 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 671.003  strength: 28.0034  max_strength: 469.5  final_strength: 269.5  sample_efficiency: 1.95598e-06  training_efficiency: -3.82272e-06  stability: -2.06862
[2021-06-17 03:49:52,239 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 430000  wall_t: 2242  opt_step: 315504  frame: 430000  fps: 191.793  total_reward: 462.5  total_reward_ma: 758.804  loss: 0.000109405  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:49:52,326 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 758.804  strength: 115.804  max_strength: 1069.5  final_strength: -180.5  sample_efficiency: -1.66066e-07  training_efficiency: 5.56477e-06  stability: 0.210282
[2021-06-17 03:50:32,590 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 440000  wall_t: 2282  opt_step: 323016  frame: 440000  fps: 192.813  total_reward: 987.5  total_reward_ma: 823.755  loss: 0.00781332  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:50:32,649 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 823.755  strength: 180.755  max_strength: 857  final_strength: 344.5  sample_efficiency: 4.16481e-06  training_efficiency: 1.05963e-05  stability: 0.535074
[2021-06-17 03:50:33,251 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 440000  wall_t: 2283  opt_step: 323016  frame: 440000  fps: 192.729  total_reward: 450  total_reward_ma: 707.021  loss: 2.82952e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:50:33,299 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 707.021  strength: 64.0211  max_strength: 707  final_strength: -193  sample_efficiency: -1.38214e-05  training_efficiency: 5.31267e-06  stability: -0.47844
[2021-06-17 03:50:34,044 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 440000  wall_t: 2284  opt_step: 323016  frame: 440000  fps: 192.644  total_reward: 587.5  total_reward_ma: 669.061  loss: 0.000582343  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:50:34,167 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 669.061  strength: 26.0615  max_strength: 469.5  final_strength: -55.5  sample_efficiency: 1.94029e-06  training_efficiency: -4.16537e-06  stability: -1.64181
[2021-06-17 03:50:38,637 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 440000  wall_t: 2288  opt_step: 323016  frame: 440000  fps: 192.308  total_reward: 350  total_reward_ma: 749.513  loss: 1.61506e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:50:38,697 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 749.513  strength: 106.513  max_strength: 1069.5  final_strength: -293  sample_efficiency: -3.18537e-07  training_efficiency: 5.71913e-06  stability: 0.159064
[2021-06-17 03:51:18,866 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 450000  wall_t: 2328  opt_step: 330528  frame: 450000  fps: 193.299  total_reward: 825  total_reward_ma: 823.783  loss: 7.86646e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:51:18,910 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 823.783  strength: 180.783  max_strength: 857  final_strength: 182  sample_efficiency: 4.12135e-06  training_efficiency: 1.0427e-05  stability: 0.534781
[2021-06-17 03:51:19,432 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 450000  wall_t: 2329  opt_step: 330528  frame: 450000  fps: 193.216  total_reward: 412.5  total_reward_ma: 700.476  loss: 5.48846e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:51:19,484 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 700.476  strength: 57.4762  max_strength: 707  final_strength: -230.5  sample_efficiency: -1.52512e-05  training_efficiency: 5.5165e-06  stability: -0.593047
[2021-06-17 03:51:20,861 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 450000  wall_t: 2330  opt_step: 330528  frame: 450000  fps: 193.133  total_reward: 487.5  total_reward_ma: 664.935  loss: 0.000283698  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:51:20,919 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 664.935  strength: 21.9351  max_strength: 469.5  final_strength: -155.5  sample_efficiency: 1.89487e-06  training_efficiency: -5.32392e-06  stability: -1.86188
[2021-06-17 03:51:24,680 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 450000  wall_t: 2334  opt_step: 330528  frame: 450000  fps: 192.802  total_reward: 437.5  total_reward_ma: 742.579  loss: 2.31115e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:51:24,739 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 742.579  strength: 99.5794  max_strength: 1069.5  final_strength: -205.5  sample_efficiency: -4.35055e-07  training_efficiency: 5.84266e-06  stability: 0.10649
[2021-06-17 03:52:04,892 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 460000  wall_t: 2375  opt_step: 338040  frame: 460000  fps: 193.684  total_reward: 487.5  total_reward_ma: 816.473  loss: 0.0039435  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:52:04,945 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 816.473  strength: 173.473  max_strength: 857  final_strength: -155.5  sample_efficiency: 4.1593e-06  training_efficiency: 1.05725e-05  stability: 0.503702
[2021-06-17 03:52:05,444 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 460000  wall_t: 2375  opt_step: 338040  frame: 460000  fps: 193.684  total_reward: 662.5  total_reward_ma: 699.651  loss: 0.00388074  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:52:05,500 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 699.651  strength: 56.6506  max_strength: 707  final_strength: 19.5  sample_efficiency: -1.51208e-05  training_efficiency: 5.49736e-06  stability: -0.735018
[2021-06-17 03:52:06,779 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 460000  wall_t: 2376  opt_step: 338040  frame: 460000  fps: 193.603  total_reward: 400  total_reward_ma: 659.048  loss: 6.09479e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:52:06,835 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 659.048  strength: 16.0476  max_strength: 469.5  final_strength: -243  sample_efficiency: 1.80097e-06  training_efficiency: -8.11086e-06  stability: -2.41363
[2021-06-17 03:52:10,453 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 460000  wall_t: 2380  opt_step: 338040  frame: 460000  fps: 193.277  total_reward: 600  total_reward_ma: 739.48  loss: 2.33928e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:52:10,522 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 739.48  strength: 96.4798  max_strength: 1069.5  final_strength: -43  sample_efficiency: -4.60333e-07  training_efficiency: 5.87061e-06  stability: 0.0655136
[2021-06-17 03:52:51,146 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 470000  wall_t: 2421  opt_step: 345552  frame: 470000  fps: 194.135  total_reward: 425  total_reward_ma: 808.143  loss: 5.27411e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:52:51,204 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 808.143  strength: 165.143  max_strength: 857  final_strength: -218  sample_efficiency: 4.21636e-06  training_efficiency: 1.07882e-05  stability: 0.486199
[2021-06-17 03:52:51,495 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 470000  wall_t: 2421  opt_step: 345552  frame: 470000  fps: 194.135  total_reward: 862.5  total_reward_ma: 703.116  loss: 1.09587e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:52:51,557 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 703.116  strength: 60.1155  max_strength: 707  final_strength: 219.5  sample_efficiency: -1.37808e-05  training_efficiency: 5.2951e-06  stability: -0.722035
[2021-06-17 03:52:52,933 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 470000  wall_t: 2423  opt_step: 345552  frame: 470000  fps: 193.974  total_reward: 362.5  total_reward_ma: 652.601  loss: 0.00388455  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:52:52,992 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 652.601  strength: 9.60093  max_strength: 469.5  final_strength: -280.5  sample_efficiency: 1.59348e-06  training_efficiency: -1.51003e-05  stability: -3.61424
[2021-06-17 03:52:56,588 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 470000  wall_t: 2426  opt_step: 345552  frame: 470000  fps: 193.735  total_reward: 725  total_reward_ma: 739.172  loss: 0.00403199  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:52:56,668 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 739.172  strength: 96.1717  max_strength: 1069.5  final_strength: 82  sample_efficiency: -4.13384e-07  training_efficiency: 5.81661e-06  stability: 0.0564595
[2021-06-17 03:53:37,646 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 480000  wall_t: 2467  opt_step: 353064  frame: 480000  fps: 194.568  total_reward: 862.5  total_reward_ma: 706.436  loss: 8.95205e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:53:37,681 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 480000  wall_t: 2467  opt_step: 353064  frame: 480000  fps: 194.568  total_reward: 675  total_reward_ma: 805.37  loss: 6.97852e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:53:37,685 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 706.436  strength: 63.436  max_strength: 707  final_strength: 219.5  sample_efficiency: -1.26372e-05  training_efficiency: 5.11757e-06  stability: -0.588255
[2021-06-17 03:53:37,741 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 805.37  strength: 162.37  max_strength: 857  final_strength: 32  sample_efficiency: 4.20761e-06  training_efficiency: 1.07555e-05  stability: 0.471768
[2021-06-17 03:53:39,547 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 480000  wall_t: 2469  opt_step: 353064  frame: 480000  fps: 194.411  total_reward: 687.5  total_reward_ma: 653.343  loss: 0.000107982  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:53:39,593 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 653.343  strength: 10.3435  max_strength: 469.5  final_strength: 44.5  sample_efficiency: 1.63832e-06  training_efficiency: -1.34588e-05  stability: -6.54488
[2021-06-17 03:53:42,682 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 480000  wall_t: 2472  opt_step: 353064  frame: 480000  fps: 194.175  total_reward: 650  total_reward_ma: 737.314  loss: 3.98563e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:53:42,782 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 737.314  strength: 94.314  max_strength: 1069.5  final_strength: 7  sample_efficiency: -4.09524e-07  training_efficiency: 5.81199e-06  stability: 0.0569839
[2021-06-17 03:54:24,227 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 490000  wall_t: 2514  opt_step: 360576  frame: 490000  fps: 194.909  total_reward: 537.5  total_reward_ma: 799.903  loss: 0.00389791  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:54:24,293 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 799.903  strength: 156.903  max_strength: 857  final_strength: -105.5  sample_efficiency: 4.23734e-06  training_efficiency: 1.08651e-05  stability: 0.456294
[2021-06-17 03:54:24,357 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 490000  wall_t: 2514  opt_step: 360576  frame: 490000  fps: 194.909  total_reward: 637.5  total_reward_ma: 705.029  loss: 1.02135e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:54:24,407 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 705.029  strength: 62.0292  max_strength: 707  final_strength: -5.5  sample_efficiency: -1.26638e-05  training_efficiency: 5.12181e-06  stability: -0.547655
[2021-06-17 03:54:26,218 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 490000  wall_t: 2516  opt_step: 360576  frame: 490000  fps: 194.754  total_reward: 687.5  total_reward_ma: 654.055  loss: 0.00392265  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:54:26,275 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 654.055  strength: 11.0551  max_strength: 469.5  final_strength: 44.5  sample_efficiency: 1.67207e-06  training_efficiency: -1.20976e-05  stability: -5.85425
[2021-06-17 03:54:29,192 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 490000  wall_t: 2519  opt_step: 360576  frame: 490000  fps: 194.522  total_reward: 712.5  total_reward_ma: 736.808  loss: 3.93271e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:54:29,240 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 736.808  strength: 93.8076  max_strength: 1069.5  final_strength: 69.5  sample_efficiency: -3.72475e-07  training_efficiency: 5.76605e-06  stability: 0.0584421
[2021-06-17 03:55:10,743 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 500000  wall_t: 2560  opt_step: 368088  frame: 500000  fps: 195.312  total_reward: 812.5  total_reward_ma: 800.155  loss: 0.00395164  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:55:10,795 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 800.155  strength: 157.155  max_strength: 857  final_strength: 169.5  sample_efficiency: 4.18908e-06  training_efficiency: 1.06893e-05  stability: 0.448833
[2021-06-17 03:55:10,797 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 500000  wall_t: 2560  opt_step: 368088  frame: 500000  fps: 195.312  total_reward: 650  total_reward_ma: 703.929  loss: 4.76504e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:55:10,857 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 703.929  strength: 60.9286  max_strength: 707  final_strength: 7  sample_efficiency: -1.26301e-05  training_efficiency: 5.11629e-06  stability: -0.550456
[2021-06-17 03:55:12,774 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 500000  wall_t: 2562  opt_step: 368088  frame: 500000  fps: 195.16  total_reward: 1000  total_reward_ma: 661.115  loss: 0.0046199  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:55:12,831 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 661.115  strength: 18.1152  max_strength: 469.5  final_strength: 357  sample_efficiency: 1.80396e-06  training_efficiency: -6.13942e-06  stability: -5.27945
[2021-06-17 03:55:15,317 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 500000  wall_t: 2565  opt_step: 368088  frame: 500000  fps: 194.932  total_reward: 637.5  total_reward_ma: 734.821  loss: 4.98377e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:55:15,382 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 734.821  strength: 91.8214  max_strength: 1069.5  final_strength: -5.5  sample_efficiency: -3.75317e-07  training_efficiency: 5.7697e-06  stability: 0.0563619
[2021-06-17 03:55:56,665 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 510000  wall_t: 2606  opt_step: 375600  frame: 510000  fps: 195.702  total_reward: 875  total_reward_ma: 801.622  loss: 1.63638e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:55:56,725 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 801.622  strength: 158.622  max_strength: 857  final_strength: 232  sample_efficiency: 4.12517e-06  training_efficiency: 1.04591e-05  stability: 0.460723
[2021-06-17 03:55:57,352 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 510000  wall_t: 2607  opt_step: 375600  frame: 510000  fps: 195.627  total_reward: 600  total_reward_ma: 701.891  loss: 6.45452e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:55:57,399 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 701.891  strength: 58.8908  max_strength: 707  final_strength: -43  sample_efficiency: -1.2839e-05  training_efficiency: 5.15142e-06  stability: -0.563306
[2021-06-17 03:55:59,204 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 510000  wall_t: 2609  opt_step: 375600  frame: 510000  fps: 195.477  total_reward: 825  total_reward_ma: 664.393  loss: 0.000185807  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:55:59,274 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 664.393  strength: 21.3929  max_strength: 469.5  final_strength: 182  sample_efficiency: 1.83064e-06  training_efficiency: -4.64179e-06  stability: -2.95107
[2021-06-17 03:56:01,620 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 510000  wall_t: 2611  opt_step: 375600  frame: 510000  fps: 195.327  total_reward: 887.5  total_reward_ma: 737.815  loss: 5.2865e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:56:01,683 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 737.815  strength: 94.8151  max_strength: 1069.5  final_strength: 244.5  sample_efficiency: -2.57197e-07  training_efficiency: 5.61259e-06  stability: 0.0552314
[2021-06-17 03:56:42,479 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 520000  wall_t: 2652  opt_step: 383112  frame: 520000  fps: 196.078  total_reward: 925  total_reward_ma: 803.995  loss: 1.54294e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:56:42,540 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 803.995  strength: 160.995  max_strength: 857  final_strength: 282  sample_efficiency: 4.051e-06  training_efficiency: 1.01947e-05  stability: 0.476188
[2021-06-17 03:56:43,511 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 520000  wall_t: 2653  opt_step: 383112  frame: 520000  fps: 196.005  total_reward: 662.5  total_reward_ma: 701.133  loss: 8.17408e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:56:43,571 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 701.133  strength: 58.1332  max_strength: 707  final_strength: 19.5  sample_efficiency: -1.27437e-05  training_efficiency: 5.13503e-06  stability: -0.585688
[2021-06-17 03:56:45,082 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 520000  wall_t: 2655  opt_step: 383112  frame: 520000  fps: 195.857  total_reward: 837.5  total_reward_ma: 667.787  loss: 3.83388e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:56:45,136 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 667.787  strength: 24.7871  max_strength: 469.5  final_strength: 194.5  sample_efficiency: 1.84487e-06  training_efficiency: -3.526e-06  stability: -2.2788
[2021-06-17 03:56:48,017 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 520000  wall_t: 2658  opt_step: 383112  frame: 520000  fps: 195.636  total_reward: 662.5  total_reward_ma: 736.367  loss: 0.000155839  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:56:48,109 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 736.367  strength: 93.3668  max_strength: 1069.5  final_strength: 19.5  sample_efficiency: -2.4844e-07  training_efficiency: 5.60053e-06  stability: 0.0564713
[2021-06-17 03:57:28,770 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 530000  wall_t: 2698  opt_step: 390624  frame: 530000  fps: 196.442  total_reward: 850  total_reward_ma: 804.863  loss: 0.0039033  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:57:28,814 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 804.863  strength: 161.863  max_strength: 857  final_strength: 207  sample_efficiency: 3.99878e-06  training_efficiency: 1.00105e-05  stability: 0.484874
[2021-06-17 03:57:29,666 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 530000  wall_t: 2699  opt_step: 390624  frame: 530000  fps: 196.369  total_reward: 1100  total_reward_ma: 708.659  loss: 0.00393385  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:57:29,709 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 708.659  strength: 65.659  max_strength: 707  final_strength: 457  sample_efficiency: -1.08224e-05  training_efficiency: 4.79686e-06  stability: -0.575459
[2021-06-17 03:57:31,514 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 530000  wall_t: 2701  opt_step: 390624  frame: 530000  fps: 196.224  total_reward: 712.5  total_reward_ma: 668.647  loss: 6.03332e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:57:31,555 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 668.647  strength: 25.647  max_strength: 469.5  final_strength: 69.5  sample_efficiency: 1.84705e-06  training_efficiency: -3.20884e-06  stability: -1.87321
[2021-06-17 03:57:34,345 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 530000  wall_t: 2704  opt_step: 390624  frame: 530000  fps: 196.006  total_reward: 375  total_reward_ma: 729.549  loss: 0.000142928  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:57:34,392 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 729.549  strength: 86.5485  max_strength: 1069.5  final_strength: -268  sample_efficiency: -3.73191e-07  training_efficiency: 5.77817e-06  stability: 0.00104451
[2021-06-17 03:58:14,936 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 540000  wall_t: 2745  opt_step: 398136  frame: 540000  fps: 196.721  total_reward: 712.5  total_reward_ma: 803.153  loss: 0.00395461  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:58:14,992 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 803.153  strength: 160.153  max_strength: 857  final_strength: 69.5  sample_efficiency: 3.98152e-06  training_efficiency: 9.95021e-06  stability: 0.481276
[2021-06-17 03:58:15,729 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 540000  wall_t: 2745  opt_step: 398136  frame: 540000  fps: 196.721  total_reward: 1225  total_reward_ma: 718.221  loss: 2.73803e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:58:15,782 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 718.221  strength: 75.2209  max_strength: 707  final_strength: 582  sample_efficiency: -9.0064e-06  training_efficiency: 4.46944e-06  stability: -0.368563
[2021-06-17 03:58:17,774 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 540000  wall_t: 2747  opt_step: 398136  frame: 540000  fps: 196.578  total_reward: 500  total_reward_ma: 665.465  loss: 0.0068433  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:58:17,829 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 665.465  strength: 22.465  max_strength: 469.5  final_strength: -143  sample_efficiency: 1.84647e-06  training_efficiency: -3.8959e-06  stability: -1.88281
[2021-06-17 03:58:20,509 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 540000  wall_t: 2750  opt_step: 398136  frame: 540000  fps: 196.364  total_reward: 475  total_reward_ma: 724.835  loss: 0.000168086  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:58:20,567 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 724.835  strength: 81.8347  max_strength: 1069.5  final_strength: -168  sample_efficiency: -4.57781e-07  training_efficiency: 5.90235e-06  stability: -0.0573195
[2021-06-17 03:59:00,915 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 550000  wall_t: 2791  opt_step: 405648  frame: 550000  fps: 197.062  total_reward: 625  total_reward_ma: 799.913  loss: 0.00389045  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:59:01,019 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 799.913  strength: 156.913  max_strength: 857  final_strength: -18  sample_efficiency: 3.98603e-06  training_efficiency: 9.96583e-06  stability: 0.475327
[2021-06-17 03:59:01,683 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 550000  wall_t: 2791  opt_step: 405648  frame: 550000  fps: 197.062  total_reward: 1100  total_reward_ma: 725.162  loss: 3.27612e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:59:01,740 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 725.162  strength: 82.1623  max_strength: 707  final_strength: 457  sample_efficiency: -7.91171e-06  training_efficiency: 4.26675e-06  stability: -0.203246
[2021-06-17 03:59:03,788 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 550000  wall_t: 2793  opt_step: 405648  frame: 550000  fps: 196.921  total_reward: 712.5  total_reward_ma: 666.336  loss: 0.000208918  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:59:03,833 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 666.336  strength: 23.336  max_strength: 469.5  final_strength: 69.5  sample_efficiency: 1.84491e-06  training_efficiency: -3.54507e-06  stability: -2.22905
[2021-06-17 03:59:06,821 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 550000  wall_t: 2796  opt_step: 405648  frame: 550000  fps: 196.71  total_reward: 812.5  total_reward_ma: 726.429  loss: 0.00403732  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:59:06,878 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 726.429  strength: 83.4286  max_strength: 1069.5  final_strength: 169.5  sample_efficiency: -3.73708e-07  training_efficiency: 5.77538e-06  stability: -0.0975157
[2021-06-17 03:59:47,011 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 560000  wall_t: 2837  opt_step: 413160  frame: 560000  fps: 197.392  total_reward: 487.5  total_reward_ma: 794.335  loss: 0.00392865  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:59:47,057 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 794.335  strength: 151.335  max_strength: 857  final_strength: -155.5  sample_efficiency: 4.02641e-06  training_efficiency: 1.01043e-05  stability: 0.4583
[2021-06-17 03:59:47,644 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 560000  wall_t: 2837  opt_step: 413160  frame: 560000  fps: 197.392  total_reward: 1012.5  total_reward_ma: 730.293  loss: 3.37262e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:59:47,691 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 730.293  strength: 87.2934  max_strength: 707  final_strength: 369.5  sample_efficiency: -7.17871e-06  training_efficiency: 4.12719e-06  stability: -0.100925
[2021-06-17 03:59:50,218 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 560000  wall_t: 2840  opt_step: 413160  frame: 560000  fps: 197.183  total_reward: 762.5  total_reward_ma: 668.084  loss: 0.000127662  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:59:50,316 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 668.084  strength: 25.0844  max_strength: 469.5  final_strength: 119.5  sample_efficiency: 1.83979e-06  training_efficiency: -3.02836e-06  stability: -2.05096
[2021-06-17 03:59:53,050 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 560000  wall_t: 2843  opt_step: 413160  frame: 560000  fps: 196.975  total_reward: 562.5  total_reward_ma: 723.501  loss: 0.00515169  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 03:59:53,096 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 723.501  strength: 80.5013  max_strength: 1069.5  final_strength: -80.5  sample_efficiency: -4.12268e-07  training_efficiency: 5.83529e-06  stability: -0.111457
[2021-06-17 04:00:33,217 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 570000  wall_t: 2883  opt_step: 420672  frame: 570000  fps: 197.711  total_reward: 712.5  total_reward_ma: 792.899  loss: 0.00391938  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:00:33,304 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 792.899  strength: 149.899  max_strength: 857  final_strength: 69.5  sample_efficiency: 4.00793e-06  training_efficiency: 1.00414e-05  stability: 0.448361
[2021-06-17 04:00:34,217 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 570000  wall_t: 2884  opt_step: 420672  frame: 570000  fps: 197.642  total_reward: 887.5  total_reward_ma: 733.051  loss: 2.75666e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:00:34,283 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 733.051  strength: 90.0514  max_strength: 707  final_strength: 244.5  sample_efficiency: -6.75319e-06  training_efficiency: 4.04383e-06  stability: -0.04328
[2021-06-17 04:00:36,616 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 570000  wall_t: 2886  opt_step: 420672  frame: 570000  fps: 197.505  total_reward: 587.5  total_reward_ma: 666.645  loss: 0.000128255  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:00:36,671 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 666.645  strength: 23.6454  max_strength: 469.5  final_strength: -55.5  sample_efficiency: 1.84337e-06  training_efficiency: -3.25493e-06  stability: -1.91354
[2021-06-17 04:00:39,764 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 570000  wall_t: 2889  opt_step: 420672  frame: 570000  fps: 197.3  total_reward: 462.5  total_reward_ma: 718.922  loss: 0.000239177  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:00:39,815 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 718.922  strength: 75.9223  max_strength: 1069.5  final_strength: -180.5  sample_efficiency: -5.02638e-07  training_efficiency: 5.97953e-06  stability: -0.153487
[2021-06-17 04:01:19,707 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 580000  wall_t: 2929  opt_step: 428184  frame: 580000  fps: 198.02  total_reward: 987.5  total_reward_ma: 796.254  loss: 0.00399003  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:01:19,752 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 796.254  strength: 153.254  max_strength: 857  final_strength: 344.5  sample_efficiency: 3.91941e-06  training_efficiency: 9.74276e-06  stability: 0.452848
[2021-06-17 04:01:20,811 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 580000  wall_t: 2930  opt_step: 428184  frame: 580000  fps: 197.952  total_reward: 1050  total_reward_ma: 738.516  loss: 0.000106952  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:01:20,861 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 738.516  strength: 95.516  max_strength: 707  final_strength: 407  sample_efficiency: -6.13039e-06  training_efficiency: 3.91832e-06  stability: 0.00641519
[2021-06-17 04:01:22,685 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 580000  wall_t: 2932  opt_step: 428184  frame: 580000  fps: 197.817  total_reward: 425  total_reward_ma: 662.406  loss: 0.00419391  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:01:22,724 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 662.406  strength: 19.406  max_strength: 469.5  final_strength: -218  sample_efficiency: 1.86686e-06  training_efficiency: -4.35668e-06  stability: -2.15838
[2021-06-17 04:01:25,924 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 580000  wall_t: 2936  opt_step: 428184  frame: 580000  fps: 197.548  total_reward: 1650  total_reward_ma: 734.975  loss: 0.000250233  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:01:25,978 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 734.975  strength: 91.9754  max_strength: 1069.5  final_strength: 1007  sample_efficiency: -8.22923e-08  training_efficiency: 5.29164e-06  stability: -0.201598
[2021-06-17 04:02:05,936 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 590000  wall_t: 2976  opt_step: 435696  frame: 590000  fps: 198.253  total_reward: 675  total_reward_ma: 794.199  loss: 3.41001e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:02:05,979 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 794.199  strength: 151.199  max_strength: 857  final_strength: 32  sample_efficiency: 3.91143e-06  training_efficiency: 9.71605e-06  stability: 0.438897
[2021-06-17 04:02:07,062 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 590000  wall_t: 2977  opt_step: 435696  frame: 590000  fps: 198.186  total_reward: 712.5  total_reward_ma: 738.075  loss: 0.0121255  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:02:07,120 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 738.075  strength: 95.0751  max_strength: 707  final_strength: 69.5  sample_efficiency: -6.03344e-06  training_efficiency: 3.89821e-06  stability: 0.0184892
[2021-06-17 04:02:09,165 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 590000  wall_t: 2979  opt_step: 435696  frame: 590000  fps: 198.053  total_reward: 537.5  total_reward_ma: 660.252  loss: 0.000230608  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:02:09,219 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 660.252  strength: 17.2525  max_strength: 469.5  final_strength: -105.5  sample_efficiency: 1.88499e-06  training_efficiency: -5.058e-06  stability: -2.78083
[2021-06-17 04:02:12,562 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 590000  wall_t: 2982  opt_step: 435696  frame: 590000  fps: 197.854  total_reward: 687.5  total_reward_ma: 734.171  loss: 0.000138207  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:02:12,622 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 734.171  strength: 91.1707  max_strength: 1069.5  final_strength: 44.5  sample_efficiency: -6.75898e-08  training_efficiency: 5.26685e-06  stability: -0.155201
[2021-06-17 04:02:52,317 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 600000  wall_t: 3022  opt_step: 443208  frame: 600000  fps: 198.544  total_reward: 800  total_reward_ma: 794.296  loss: 4.29125e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:02:52,375 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 794.296  strength: 151.296  max_strength: 857  final_strength: 157  sample_efficiency: 3.87261e-06  training_efficiency: 9.58703e-06  stability: 0.44091
[2021-06-17 04:02:53,431 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 600000  wall_t: 3023  opt_step: 443208  frame: 600000  fps: 198.478  total_reward: 625  total_reward_ma: 736.19  loss: 6.32589e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:02:53,492 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 736.19  strength: 93.1905  max_strength: 707  final_strength: -18  sample_efficiency: -6.05823e-06  training_efficiency: 3.90349e-06  stability: 0.0150512
[2021-06-17 04:02:55,577 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 600000  wall_t: 3025  opt_step: 443208  frame: 600000  fps: 198.347  total_reward: 837.5  total_reward_ma: 663.257  loss: 0.00412013  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:02:55,634 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 663.257  strength: 20.2567  max_strength: 469.5  final_strength: 194.5  sample_efficiency: 1.84946e-06  training_efficiency: -3.86766e-06  stability: -3.17946
[2021-06-17 04:02:58,907 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 600000  wall_t: 3029  opt_step: 443208  frame: 600000  fps: 198.085  total_reward: 462.5  total_reward_ma: 729.643  loss: 8.09725e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:02:58,962 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 729.643  strength: 86.6429  max_strength: 1069.5  final_strength: -180.5  sample_efficiency: -1.27805e-07  training_efficiency: 5.37138e-06  stability: -0.187473
[2021-06-17 04:03:38,610 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 610000  wall_t: 3068  opt_step: 450720  frame: 610000  fps: 198.827  total_reward: 975  total_reward_ma: 797.258  loss: 2.4589e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:03:38,674 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 797.258  strength: 154.258  max_strength: 857  final_strength: 332  sample_efficiency: 3.79381e-06  training_efficiency: 9.32705e-06  stability: 0.450579
[2021-06-17 04:03:39,198 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 610000  wall_t: 3069  opt_step: 450720  frame: 610000  fps: 198.762  total_reward: 537.5  total_reward_ma: 732.933  loss: 0.00392334  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:03:39,268 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 732.933  strength: 89.9333  max_strength: 707  final_strength: -105.5  sample_efficiency: -6.20626e-06  training_efficiency: 3.9359e-06  stability: -0.00376844
[2021-06-17 04:03:42,133 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 610000  wall_t: 3072  opt_step: 450720  frame: 610000  fps: 198.568  total_reward: 1125  total_reward_ma: 670.952  loss: 0.000181575  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:03:42,192 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 670.952  strength: 27.9524  max_strength: 482  final_strength: 482  sample_efficiency: 1.78907e-06  training_efficiency: -2.11849e-06  stability: -2.49928
[2021-06-17 04:03:44,991 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 610000  wall_t: 3075  opt_step: 450720  frame: 610000  fps: 198.374  total_reward: 487.5  total_reward_ma: 725.673  loss: 0.000202464  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:03:45,044 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 725.673  strength: 82.6733  max_strength: 1069.5  final_strength: -155.5  sample_efficiency: -1.82294e-07  training_efficiency: 5.46859e-06  stability: -0.228703
[2021-06-17 04:04:24,835 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 620000  wall_t: 3114  opt_step: 458232  frame: 620000  fps: 199.101  total_reward: 825  total_reward_ma: 797.705  loss: 4.5546e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:04:24,893 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 797.705  strength: 154.705  max_strength: 857  final_strength: 182  sample_efficiency: 3.75243e-06  training_efficiency: 9.19148e-06  stability: 0.454023
[2021-06-17 04:04:25,536 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 620000  wall_t: 3115  opt_step: 458232  frame: 620000  fps: 199.037  total_reward: 675  total_reward_ma: 731.999  loss: 0.00395701  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:04:25,600 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 731.999  strength: 88.9988  max_strength: 707  final_strength: 32  sample_efficiency: -6.16091e-06  training_efficiency: 3.92573e-06  stability: -0.023072
[2021-06-17 04:04:28,289 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 620000  wall_t: 3118  opt_step: 458232  frame: 620000  fps: 198.845  total_reward: 1575  total_reward_ma: 685.773  loss: 0.000344405  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:04:28,351 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 685.773  strength: 42.7728  max_strength: 932  final_strength: 932  sample_efficiency: 1.72615e-06  training_efficiency: -5.82224e-07  stability: -1.49361
[2021-06-17 04:04:31,024 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 620000  wall_t: 3121  opt_step: 458232  frame: 620000  fps: 198.654  total_reward: 787.5  total_reward_ma: 726.67  loss: 0.000236313  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:04:31,120 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 726.67  strength: 83.6705  max_strength: 1069.5  final_strength: 144.5  sample_efficiency: -1.32289e-07  training_efficiency: 5.37705e-06  stability: -0.266589
[2021-06-17 04:05:10,705 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 630000  wall_t: 3160  opt_step: 465744  frame: 630000  fps: 199.367  total_reward: 512.5  total_reward_ma: 793.178  loss: 3.32226e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:05:10,764 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 793.178  strength: 150.178  max_strength: 857  final_strength: -130.5  sample_efficiency: 3.7823e-06  training_efficiency: 9.28865e-06  stability: 0.431803
[2021-06-17 04:05:11,361 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 630000  wall_t: 3161  opt_step: 465744  frame: 630000  fps: 199.304  total_reward: 1125  total_reward_ma: 738.237  loss: 6.88882e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:05:11,414 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 738.237  strength: 95.237  max_strength: 707  final_strength: 482  sample_efficiency: -5.53847e-06  training_efficiency: 3.78284e-06  stability: -0.017139
[2021-06-17 04:05:14,857 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 630000  wall_t: 3164  opt_step: 465744  frame: 630000  fps: 199.115  total_reward: 1212.5  total_reward_ma: 694.268  loss: 0.00374617  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:05:14,928 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 694.268  strength: 51.2684  max_strength: 932  final_strength: 569.5  sample_efficiency: 1.70127e-06  training_efficiency: -9.32257e-08  stability: -0.741814
[2021-06-17 04:05:16,903 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 630000  wall_t: 3167  opt_step: 465744  frame: 630000  fps: 198.926  total_reward: 625  total_reward_ma: 725.057  loss: 0.000121119  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:05:16,949 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 725.057  strength: 82.0567  max_strength: 1069.5  final_strength: -18  sample_efficiency: -1.38276e-07  training_efficiency: 5.3883e-06  stability: -0.262633
[2021-06-17 04:05:57,197 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 640000  wall_t: 3207  opt_step: 473256  frame: 640000  fps: 199.563  total_reward: 475  total_reward_ma: 788.207  loss: 6.09231e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:05:57,248 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 788.207  strength: 145.207  max_strength: 857  final_strength: -168  sample_efficiency: 3.82243e-06  training_efficiency: 9.41837e-06  stability: 0.420002
[2021-06-17 04:05:57,562 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 640000  wall_t: 3207  opt_step: 473256  frame: 640000  fps: 199.563  total_reward: 1300  total_reward_ma: 747.015  loss: 8.14887e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:05:57,667 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 747.015  strength: 104.015  max_strength: 707  final_strength: 657  sample_efficiency: -4.83764e-06  training_efficiency: 3.61804e-06  stability: 0.0645722
[2021-06-17 04:06:01,505 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 640000  wall_t: 3211  opt_step: 473256  frame: 640000  fps: 199.315  total_reward: 925  total_reward_ma: 697.931  loss: 8.62635e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:06:01,619 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 697.931  strength: 54.9308  max_strength: 932  final_strength: 282  sample_efficiency: 1.68996e-06  training_efficiency: 8.65564e-08  stability: -0.52019
[2021-06-17 04:06:03,339 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 640000  wall_t: 3213  opt_step: 473256  frame: 640000  fps: 199.191  total_reward: 587.5  total_reward_ma: 722.907  loss: 0.000125379  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:06:03,382 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 722.907  strength: 79.9074  max_strength: 1069.5  final_strength: -55.5  sample_efficiency: -1.56734e-07  training_efficiency: 5.42385e-06  stability: -0.274284
[2021-06-17 04:06:43,471 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 650000  wall_t: 3253  opt_step: 480768  frame: 650000  fps: 199.816  total_reward: 562.5  total_reward_ma: 784.734  loss: 0.000132882  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:06:43,518 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 784.734  strength: 141.734  max_strength: 857  final_strength: -80.5  sample_efficiency: 3.84238e-06  training_efficiency: 9.48249e-06  stability: 0.409517
[2021-06-17 04:06:43,840 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 650000  wall_t: 3253  opt_step: 480768  frame: 650000  fps: 199.816  total_reward: 925  total_reward_ma: 749.753  loss: 0.00402035  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:06:43,935 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 749.753  strength: 106.753  max_strength: 707  final_strength: 282  sample_efficiency: -4.57852e-06  training_efficiency: 3.55553e-06  stability: 0.100561
[2021-06-17 04:06:47,724 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 650000  wall_t: 3257  opt_step: 480768  frame: 650000  fps: 199.57  total_reward: 612.5  total_reward_ma: 696.596  loss: 0.000104116  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:06:47,770 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 696.596  strength: 53.596  max_strength: 932  final_strength: -30.5  sample_efficiency: 1.69131e-06  training_efficiency: 6.88314e-08  stability: -0.486615
[2021-06-17 04:06:50,071 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 650000  wall_t: 3260  opt_step: 480768  frame: 650000  fps: 199.387  total_reward: 637.5  total_reward_ma: 721.593  loss: 0.000134691  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:06:50,127 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 721.593  strength: 78.5934  max_strength: 1069.5  final_strength: -5.5  sample_efficiency: -1.58559e-07  training_efficiency: 5.42745e-06  stability: -0.288113
[2021-06-17 04:07:29,877 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 660000  wall_t: 3299  opt_step: 488280  frame: 660000  fps: 200.061  total_reward: 1125  total_reward_ma: 789.89  loss: 7.25925e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:07:29,953 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 789.89  strength: 146.89  max_strength: 857  final_strength: 482  sample_efficiency: 3.72668e-06  training_efficiency: 9.11286e-06  stability: 0.404357
[2021-06-17 04:07:30,076 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 660000  wall_t: 3300  opt_step: 488280  frame: 660000  fps: 200  total_reward: 900  total_reward_ma: 752.029  loss: 0.00398311  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:07:30,194 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 752.029  strength: 109.029  max_strength: 707  final_strength: 257  sample_efficiency: -4.36088e-06  training_efficiency: 3.50169e-06  stability: 0.133512
[2021-06-17 04:07:34,195 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 660000  wall_t: 3304  opt_step: 488280  frame: 660000  fps: 199.758  total_reward: 437.5  total_reward_ma: 692.61  loss: 4.88561e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:07:34,272 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 692.61  strength: 49.6099  max_strength: 932  final_strength: -205.5  sample_efficiency: 1.70253e-06  training_efficiency: -5.72974e-08  stability: -0.550852
[2021-06-17 04:07:36,931 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 660000  wall_t: 3307  opt_step: 488280  frame: 660000  fps: 199.577  total_reward: 350  total_reward_ma: 715.963  loss: 0.000209224  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:07:37,026 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 715.963  strength: 72.9632  max_strength: 1069.5  final_strength: -293  sample_efficiency: -2.60394e-07  training_efficiency: 5.63307e-06  stability: -0.345777
[2021-06-17 04:08:15,626 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 670000  wall_t: 3345  opt_step: 495792  frame: 670000  fps: 200.299  total_reward: 1175  total_reward_ma: 795.638  loss: 9.65801e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:08:15,670 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 795.638  strength: 152.638  max_strength: 857  final_strength: 532  sample_efficiency: 3.61046e-06  training_efficiency: 8.74373e-06  stability: 0.433971
[2021-06-17 04:08:16,302 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 670000  wall_t: 3346  opt_step: 495792  frame: 670000  fps: 200.239  total_reward: 862.5  total_reward_ma: 753.678  loss: 0.00473335  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:08:16,376 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 753.678  strength: 110.678  max_strength: 707  final_strength: 219.5  sample_efficiency: -4.18762e-06  training_efficiency: 3.45774e-06  stability: 0.159247
[2021-06-17 04:08:20,033 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 670000  wall_t: 3350  opt_step: 495792  frame: 670000  fps: 200  total_reward: 450  total_reward_ma: 688.934  loss: 0.000131824  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:08:20,099 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 688.934  strength: 45.934  max_strength: 932  final_strength: -193  sample_efficiency: 1.7159e-06  training_efficiency: -1.89349e-07  stability: -0.649684
[2021-06-17 04:08:23,257 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 670000  wall_t: 3353  opt_step: 495792  frame: 670000  fps: 199.821  total_reward: 700  total_reward_ma: 715.725  loss: 2.54362e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:08:23,302 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 715.725  strength: 72.7249  max_strength: 1069.5  final_strength: 57  sample_efficiency: -2.39888e-07  training_efficiency: 5.59076e-06  stability: -0.42766
[2021-06-17 04:09:01,860 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 680000  wall_t: 3391  opt_step: 503304  frame: 680000  fps: 200.531  total_reward: 362.5  total_reward_ma: 789.268  loss: 0.000140836  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:09:01,961 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 789.268  strength: 146.268  max_strength: 857  final_strength: -280.5  sample_efficiency: 3.6708e-06  training_efficiency: 8.93429e-06  stability: 0.383968
[2021-06-17 04:09:02,507 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 680000  wall_t: 3392  opt_step: 503304  frame: 680000  fps: 200.472  total_reward: 662.5  total_reward_ma: 752.337  loss: 4.90038e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:09:02,552 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 752.337  strength: 109.337  max_strength: 707  final_strength: 19.5  sample_efficiency: -4.17278e-06  training_efficiency: 3.45389e-06  stability: 0.157163
[2021-06-17 04:09:06,812 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 680000  wall_t: 3396  opt_step: 503304  frame: 680000  fps: 200.236  total_reward: 775  total_reward_ma: 690.219  loss: 0.00313315  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:09:06,993 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 690.219  strength: 47.2185  max_strength: 932  final_strength: 132  sample_efficiency: 1.70567e-06  training_efficiency: -9.85487e-08  stability: -0.754706
[2021-06-17 04:09:09,641 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 680000  wall_t: 3399  opt_step: 503304  frame: 680000  fps: 200.059  total_reward: 550  total_reward_ma: 713.288  loss: 0.000153351  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:09:09,687 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 713.288  strength: 70.2878  max_strength: 1069.5  final_strength: -93  sample_efficiency: -2.7317e-07  training_efficiency: 5.66089e-06  stability: -0.441744
[2021-06-17 04:09:48,015 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 690000  wall_t: 3438  opt_step: 510816  frame: 690000  fps: 200.698  total_reward: 462.5  total_reward_ma: 784.532  loss: 0.000110142  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:09:48,080 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 784.532  strength: 141.532  max_strength: 857  final_strength: -180.5  sample_efficiency: 3.71186e-06  training_efficiency: 9.06323e-06  stability: 0.366595
[2021-06-17 04:09:48,715 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 690000  wall_t: 3438  opt_step: 510816  frame: 690000  fps: 200.698  total_reward: 512.5  total_reward_ma: 748.861  loss: 0.00424805  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:09:48,772 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 748.861  strength: 105.861  max_strength: 707  final_strength: -130.5  sample_efficiency: -4.27322e-06  training_efficiency: 3.48062e-06  stability: 0.139198
[2021-06-17 04:09:53,065 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 690000  wall_t: 3443  opt_step: 510816  frame: 690000  fps: 200.407  total_reward: 700  total_reward_ma: 690.362  loss: 0.00439041  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:09:53,165 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 690.362  strength: 47.3624  max_strength: 932  final_strength: 57  sample_efficiency: 1.70113e-06  training_efficiency: -6.21574e-08  stability: -0.7052
[2021-06-17 04:09:55,584 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 690000  wall_t: 3445  opt_step: 510816  frame: 690000  fps: 200.29  total_reward: 600  total_reward_ma: 711.646  loss: 0.000348224  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:09:55,652 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 711.646  strength: 68.646  max_strength: 1069.5  final_strength: -43  sample_efficiency: -2.88807e-07  training_efficiency: 5.69451e-06  stability: -0.469797
[2021-06-17 04:10:34,506 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 700000  wall_t: 3484  opt_step: 518328  frame: 700000  fps: 200.918  total_reward: 450  total_reward_ma: 779.753  loss: 0.00399153  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:10:34,552 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 779.753  strength: 136.753  max_strength: 857  final_strength: -193  sample_efficiency: 3.7579e-06  training_efficiency: 9.20706e-06  stability: 0.353607
[2021-06-17 04:10:35,435 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 700000  wall_t: 3485  opt_step: 518328  frame: 700000  fps: 200.861  total_reward: 725  total_reward_ma: 748.52  loss: 5.82134e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:10:35,512 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 748.52  strength: 105.52  max_strength: 707  final_strength: 82  sample_efficiency: -4.20992e-06  training_efficiency: 3.4634e-06  stability: 0.123819
[2021-06-17 04:10:40,199 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 700000  wall_t: 3490  opt_step: 518328  frame: 700000  fps: 200.573  total_reward: 812.5  total_reward_ma: 692.132  loss: 0.00012738  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:10:40,264 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 692.132  strength: 49.1325  max_strength: 932  final_strength: 169.5  sample_efficiency: 1.6875e-06  training_efficiency: 3.74103e-08  stability: -0.67502
[2021-06-17 04:10:42,190 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 700000  wall_t: 3492  opt_step: 518328  frame: 700000  fps: 200.458  total_reward: 1350  total_reward_ma: 720.765  loss: 0.00408063  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:10:42,256 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 720.765  strength: 77.7653  max_strength: 1069.5  final_strength: 707  sample_efficiency: -6.57576e-08  training_efficiency: 5.20549e-06  stability: -0.48314
[2021-06-17 04:11:21,436 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 710000  wall_t: 3531  opt_step: 525840  frame: 710000  fps: 201.076  total_reward: 637.5  total_reward_ma: 777.75  loss: 3.04575e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:11:21,494 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 777.75  strength: 134.75  max_strength: 857  final_strength: -5.5  sample_efficiency: 3.75925e-06  training_efficiency: 9.21126e-06  stability: 0.340575
[2021-06-17 04:11:22,393 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 710000  wall_t: 3532  opt_step: 525840  frame: 710000  fps: 201.019  total_reward: 612.5  total_reward_ma: 746.605  loss: 0.000248267  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:11:22,478 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 746.605  strength: 103.605  max_strength: 707  final_strength: -30.5  sample_efficiency: -4.23322e-06  training_efficiency: 3.46987e-06  stability: 0.118315
[2021-06-17 04:11:27,258 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 710000  wall_t: 3537  opt_step: 525840  frame: 710000  fps: 200.735  total_reward: 787.5  total_reward_ma: 693.495  loss: 0.00043808  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:11:27,368 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 693.495  strength: 50.4949  max_strength: 932  final_strength: 144.5  sample_efficiency: 1.67609e-06  training_efficiency: 1.13625e-07  stability: -0.598647
[2021-06-17 04:11:29,585 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 710000  wall_t: 3539  opt_step: 525840  frame: 710000  fps: 200.622  total_reward: 862.5  total_reward_ma: 722.762  loss: 0.000163143  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:11:29,636 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 722.762  strength: 79.7616  max_strength: 1069.5  final_strength: 219.5  sample_efficiency: -8.61747e-09  training_efficiency: 5.07743e-06  stability: -0.380068
[2021-06-17 04:12:07,746 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 720000  wall_t: 3577  opt_step: 533352  frame: 720000  fps: 201.286  total_reward: 1100  total_reward_ma: 782.226  loss: 0.000293904  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:12:07,786 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 782.226  strength: 139.226  max_strength: 857  final_strength: 457  sample_efficiency: 3.65119e-06  training_efficiency: 8.8768e-06  stability: 0.340196
[2021-06-17 04:12:08,588 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 720000  wall_t: 3578  opt_step: 533352  frame: 720000  fps: 201.23  total_reward: 487.5  total_reward_ma: 743.006  loss: 0.000224312  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:12:08,645 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 743.006  strength: 100.006  max_strength: 707  final_strength: -155.5  sample_efficiency: -4.35463e-06  training_efficiency: 3.50432e-06  stability: 0.0976666
[2021-06-17 04:12:13,248 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 720000  wall_t: 3583  opt_step: 533352  frame: 720000  fps: 200.949  total_reward: 975  total_reward_ma: 697.46  loss: 0.000126559  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:12:13,311 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 697.46  strength: 54.4598  max_strength: 932  final_strength: 332  sample_efficiency: 1.65143e-06  training_efficiency: 2.64856e-07  stability: -0.533293
[2021-06-17 04:12:15,619 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 720000  wall_t: 3585  opt_step: 533352  frame: 720000  fps: 200.837  total_reward: 812.5  total_reward_ma: 724.008  loss: 0.00392634  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:12:15,674 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 724.008  strength: 81.0079  max_strength: 1069.5  final_strength: 169.5  sample_efficiency: 3.19953e-08  training_efficiency: 4.98437e-06  stability: -0.335406
[2021-06-17 04:12:53,950 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 730000  wall_t: 3624  opt_step: 540864  frame: 730000  fps: 201.435  total_reward: 1225  total_reward_ma: 788.291  loss: 0.00384259  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:12:54,010 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 788.291  strength: 145.291  max_strength: 857  final_strength: 582  sample_efficiency: 3.526e-06  training_efficiency: 8.49116e-06  stability: 0.370276
[2021-06-17 04:12:54,950 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 730000  wall_t: 3625  opt_step: 540864  frame: 730000  fps: 201.379  total_reward: 875  total_reward_ma: 744.814  loss: 0.000162681  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:12:55,005 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 744.814  strength: 101.814  max_strength: 707  final_strength: 232  sample_efficiency: -4.17595e-06  training_efficiency: 3.45264e-06  stability: 0.0781799
[2021-06-17 04:12:59,809 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 730000  wall_t: 3629  opt_step: 540864  frame: 730000  fps: 201.157  total_reward: 587.5  total_reward_ma: 695.932  loss: 0.000141656  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:12:59,857 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 695.932  strength: 52.9325  max_strength: 932  final_strength: -55.5  sample_efficiency: 1.65553e-06  training_efficiency: 2.41787e-07  stability: -0.501856
[2021-06-17 04:13:01,971 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 730000  wall_t: 3632  opt_step: 540864  frame: 730000  fps: 200.991  total_reward: 812.5  total_reward_ma: 725.22  loss: 0.00427308  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:13:02,018 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 725.22  strength: 82.2202  max_strength: 1069.5  final_strength: 169.5  sample_efficiency: 6.9777e-08  training_efficiency: 4.89582e-06  stability: -0.296598
[2021-06-17 04:13:40,294 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 740000  wall_t: 3670  opt_step: 548376  frame: 740000  fps: 201.635  total_reward: 975  total_reward_ma: 790.814  loss: 0.00390765  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:13:40,339 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 790.814  strength: 147.814  max_strength: 857  final_strength: 332  sample_efficiency: 3.46e-06  training_efficiency: 8.28878e-06  stability: 0.38126
[2021-06-17 04:13:41,098 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 740000  wall_t: 3671  opt_step: 548376  frame: 740000  fps: 201.58  total_reward: 700  total_reward_ma: 744.208  loss: 0.000382027  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:13:41,145 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 744.208  strength: 101.208  max_strength: 707  final_strength: 57  sample_efficiency: -4.13388e-06  training_efficiency: 3.44024e-06  stability: 0.0834087
[2021-06-17 04:13:45,874 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 740000  wall_t: 3675  opt_step: 548376  frame: 740000  fps: 201.361  total_reward: 525  total_reward_ma: 693.591  loss: 0.000221932  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:13:45,963 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 693.591  strength: 50.591  max_strength: 932  final_strength: -118  sample_efficiency: 1.66525e-06  training_efficiency: 1.91248e-07  stability: -0.540127
[2021-06-17 04:13:47,916 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 740000  wall_t: 3678  opt_step: 548376  frame: 740000  fps: 201.196  total_reward: 775  total_reward_ma: 725.893  loss: 0.000113094  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:13:47,980 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 725.893  strength: 82.8929  max_strength: 1069.5  final_strength: 132  sample_efficiency: 9.73554e-08  training_efficiency: 4.82971e-06  stability: -0.26623
[2021-06-17 04:14:26,468 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 750000  wall_t: 3716  opt_step: 555888  frame: 750000  fps: 201.83  total_reward: 762.5  total_reward_ma: 790.437  loss: 0.000257622  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:14:26,524 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 790.437  strength: 147.437  max_strength: 857  final_strength: 119.5  sample_efficiency: 3.43702e-06  training_efficiency: 8.21865e-06  stability: 0.380613
[2021-06-17 04:14:27,159 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 750000  wall_t: 3717  opt_step: 555888  frame: 750000  fps: 201.776  total_reward: 1025  total_reward_ma: 747.952  loss: 0.00013691  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:14:27,205 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 747.952  strength: 104.952  max_strength: 707  final_strength: 382  sample_efficiency: -3.86856e-06  training_efficiency: 3.36059e-06  stability: 0.0903845
[2021-06-17 04:14:32,344 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 750000  wall_t: 3722  opt_step: 555888  frame: 750000  fps: 201.505  total_reward: 612.5  total_reward_ma: 692.495  loss: 3.59404e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:14:32,397 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 692.495  strength: 49.4952  max_strength: 932  final_strength: -30.5  sample_efficiency: 1.66802e-06  training_efficiency: 1.7786e-07  stability: -0.589335
[2021-06-17 04:14:34,336 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 750000  wall_t: 3724  opt_step: 555888  frame: 750000  fps: 201.396  total_reward: 487.5  total_reward_ma: 722.714  loss: 6.45331e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:14:34,378 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 722.714  strength: 79.7143  max_strength: 1069.5  final_strength: -155.5  sample_efficiency: 6.52081e-08  training_efficiency: 4.90854e-06  stability: -0.285851
[2021-06-17 04:15:13,195 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 760000  wall_t: 3763  opt_step: 563400  frame: 760000  fps: 201.967  total_reward: 1187.5  total_reward_ma: 753.736  loss: 0.000169625  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:15:13,262 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 760000  wall_t: 3763  opt_step: 563400  frame: 760000  fps: 201.967  total_reward: 637.5  total_reward_ma: 788.424  loss: 5.72697e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:15:13,344 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 753.736  strength: 110.736  max_strength: 707  final_strength: 544.5  sample_efficiency: -3.53314e-06  training_efficiency: 3.258e-06  stability: 0.134528
[2021-06-17 04:15:13,348 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 788.424  strength: 145.424  max_strength: 857  final_strength: -5.5  sample_efficiency: 3.43807e-06  training_efficiency: 8.22185e-06  stability: 0.376003
[2021-06-17 04:15:18,683 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 760000  wall_t: 3768  opt_step: 563400  frame: 760000  fps: 201.699  total_reward: 662.5  total_reward_ma: 692.095  loss: 0.0001235  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:15:18,746 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 692.095  strength: 49.0952  max_strength: 932  final_strength: 19.5  sample_efficiency: 1.66615e-06  training_efficiency: 1.86318e-07  stability: -0.60257
[2021-06-17 04:15:20,784 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 760000  wall_t: 3770  opt_step: 563400  frame: 760000  fps: 201.592  total_reward: 562.5  total_reward_ma: 720.606  loss: 0.000119438  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:15:20,832 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 720.606  strength: 77.6062  max_strength: 1069.5  final_strength: -80.5  sample_efficiency: 4.81395e-08  training_efficiency: 4.95131e-06  stability: -0.319295
[2021-06-17 04:15:59,209 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 770000  wall_t: 3809  opt_step: 570912  frame: 770000  fps: 202.153  total_reward: 1400  total_reward_ma: 762.129  loss: 7.50464e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:15:59,301 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 770000  wall_t: 3809  opt_step: 570912  frame: 770000  fps: 202.153  total_reward: 687.5  total_reward_ma: 787.113  loss: 0.00410664  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:15:59,341 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 762.129  strength: 119.129  max_strength: 757  final_strength: 757  sample_efficiency: -3.13439e-06  training_efficiency: 3.13368e-06  stability: 0.190523
[2021-06-17 04:15:59,361 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 787.113  strength: 144.113  max_strength: 857  final_strength: 44.5  sample_efficiency: 3.42949e-06  training_efficiency: 8.19591e-06  stability: 0.375692
[2021-06-17 04:16:04,446 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 770000  wall_t: 3814  opt_step: 570912  frame: 770000  fps: 201.888  total_reward: 862.5  total_reward_ma: 694.337  loss: 0.0001705  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:16:04,541 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 694.337  strength: 51.3374  max_strength: 932  final_strength: 219.5  sample_efficiency: 1.64548e-06  training_efficiency: 2.74377e-07  stability: -0.594083
[2021-06-17 04:16:06,632 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 770000  wall_t: 3816  opt_step: 570912  frame: 770000  fps: 201.782  total_reward: 662.5  total_reward_ma: 719.852  loss: 0.00404096  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:16:06,677 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 719.852  strength: 76.8516  max_strength: 1069.5  final_strength: 19.5  sample_efficiency: 5.22605e-08  training_efficiency: 4.94076e-06  stability: -0.337302
[2021-06-17 04:16:45,076 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 780000  wall_t: 3855  opt_step: 578424  frame: 780000  fps: 202.335  total_reward: 1075  total_reward_ma: 766.14  loss: 0.00778373  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:16:45,158 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 766.14  strength: 123.14  max_strength: 757  final_strength: 432  sample_efficiency: -2.93575e-06  training_efficiency: 3.0705e-06  stability: 0.221895
[2021-06-17 04:16:45,354 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 780000  wall_t: 3855  opt_step: 578424  frame: 780000  fps: 202.335  total_reward: 725  total_reward_ma: 786.317  loss: 9.47741e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:16:45,403 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 786.317  strength: 143.317  max_strength: 857  final_strength: 82  sample_efficiency: 3.41374e-06  training_efficiency: 8.14847e-06  stability: 0.378196
[2021-06-17 04:16:50,525 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 780000  wall_t: 3860  opt_step: 578424  frame: 780000  fps: 202.073  total_reward: 1062.5  total_reward_ma: 699.119  loss: 0.000169538  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:16:50,577 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 699.119  strength: 56.1187  max_strength: 932  final_strength: 419.5  sample_efficiency: 1.6102e-06  training_efficiency: 4.15577e-07  stability: -0.504403
[2021-06-17 04:16:53,156 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 780000  wall_t: 3863  opt_step: 578424  frame: 780000  fps: 201.916  total_reward: 825  total_reward_ma: 721.2  loss: 0.000107863  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:16:53,232 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 721.2  strength: 78.1996  max_strength: 1069.5  final_strength: 182  sample_efficiency: 8.89552e-08  training_efficiency: 4.84492e-06  stability: -0.332895
[2021-06-17 04:17:31,294 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 790000  wall_t: 3901  opt_step: 585936  frame: 790000  fps: 202.512  total_reward: 1312.5  total_reward_ma: 773.056  loss: 0.000980185  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:17:31,348 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 790000  wall_t: 3901  opt_step: 585936  frame: 790000  fps: 202.512  total_reward: 450  total_reward_ma: 782.06  loss: 3.66874e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:17:31,409 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 773.056  strength: 130.056  max_strength: 757  final_strength: 669.5  sample_efficiency: -2.66197e-06  training_efficiency: 2.98163e-06  stability: 0.256892
[2021-06-17 04:17:31,415 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 782.06  strength: 139.06  max_strength: 857  final_strength: -193  sample_efficiency: 3.45147e-06  training_efficiency: 8.26164e-06  stability: 0.358157
[2021-06-17 04:17:36,863 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 790000  wall_t: 3906  opt_step: 585936  frame: 790000  fps: 202.253  total_reward: 1100  total_reward_ma: 704.258  loss: 0.000136048  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:17:36,961 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 704.258  strength: 61.2582  max_strength: 932  final_strength: 457  sample_efficiency: 1.57726e-06  training_efficiency: 5.39062e-07  stability: -0.358354
[2021-06-17 04:17:39,814 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 790000  wall_t: 3909  opt_step: 585936  frame: 790000  fps: 202.098  total_reward: 750  total_reward_ma: 721.564  loss: 3.02011e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:17:39,872 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 721.564  strength: 78.5642  max_strength: 1069.5  final_strength: 107  sample_efficiency: 1.09244e-07  training_efficiency: 4.79082e-06  stability: -0.30542
[2021-06-17 04:18:17,428 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 800000  wall_t: 3947  opt_step: 593448  frame: 800000  fps: 202.686  total_reward: 700  total_reward_ma: 781.034  loss: 0.00418631  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:18:17,485 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 781.034  strength: 138.034  max_strength: 857  final_strength: 57  sample_efficiency: 3.44011e-06  training_efficiency: 8.22769e-06  stability: 0.34688
[2021-06-17 04:18:17,597 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 800000  wall_t: 3947  opt_step: 593448  frame: 800000  fps: 202.686  total_reward: 1062.5  total_reward_ma: 776.674  loss: 0.000135998  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:18:17,712 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 776.674  strength: 133.674  max_strength: 757  final_strength: 419.5  sample_efficiency: -2.50851e-06  training_efficiency: 2.93077e-06  stability: 0.280982
[2021-06-17 04:18:23,408 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 800000  wall_t: 3953  opt_step: 593448  frame: 800000  fps: 202.378  total_reward: 712.5  total_reward_ma: 704.363  loss: 0.00391192  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:18:23,453 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 704.363  strength: 61.3626  max_strength: 932  final_strength: 69.5  sample_efficiency: 1.57257e-06  training_efficiency: 5.55493e-07  stability: -0.309535
[2021-06-17 04:18:25,996 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 800000  wall_t: 3956  opt_step: 593448  frame: 800000  fps: 202.224  total_reward: 662.5  total_reward_ma: 720.826  loss: 6.79675e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:18:26,055 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 720.826  strength: 77.8259  max_strength: 1069.5  final_strength: 19.5  sample_efficiency: 1.12817e-07  training_efficiency: 4.78109e-06  stability: -0.297012
[2021-06-17 04:19:03,241 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 810000  wall_t: 3993  opt_step: 600960  frame: 810000  fps: 202.855  total_reward: 900  total_reward_ma: 778.197  loss: 0.000393473  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:19:03,331 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 778.197  strength: 135.197  max_strength: 757  final_strength: 257  sample_efficiency: -2.42066e-06  training_efficiency: 2.90104e-06  stability: 0.293992
[2021-06-17 04:19:03,554 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 810000  wall_t: 3993  opt_step: 600960  frame: 810000  fps: 202.855  total_reward: 862.5  total_reward_ma: 782.04  loss: 5.49669e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:19:03,617 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 782.04  strength: 139.04  max_strength: 857  final_strength: 219.5  sample_efficiency: 3.39712e-06  training_efficiency: 8.09977e-06  stability: 0.350252
[2021-06-17 04:19:09,804 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 810000  wall_t: 3999  opt_step: 600960  frame: 810000  fps: 202.551  total_reward: 537.5  total_reward_ma: 702.277  loss: 4.8795e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:19:09,846 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 702.277  strength: 59.2768  max_strength: 932  final_strength: -105.5  sample_efficiency: 1.58009e-06  training_efficiency: 5.30832e-07  stability: -0.32686
[2021-06-17 04:19:12,311 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 810000  wall_t: 4002  opt_step: 600960  frame: 810000  fps: 202.399  total_reward: 650  total_reward_ma: 719.951  loss: 0.000151099  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:19:12,369 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 719.951  strength: 76.9515  max_strength: 1069.5  final_strength: 7  sample_efficiency: 1.14077e-07  training_efficiency: 4.77759e-06  stability: -0.294958
[2021-06-17 04:19:49,512 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 820000  wall_t: 4039  opt_step: 608472  frame: 820000  fps: 203.021  total_reward: 1075  total_reward_ma: 785.613  loss: 0.000969381  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:19:49,619 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 785.613  strength: 142.613  max_strength: 857  final_strength: 432  sample_efficiency: 3.31668e-06  training_efficiency: 7.86126e-06  stability: 0.362915
[2021-06-17 04:19:49,926 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 820000  wall_t: 4040  opt_step: 608472  frame: 820000  fps: 202.97  total_reward: 787.5  total_reward_ma: 778.31  loss: 0.000216679  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:19:50,007 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 778.31  strength: 135.31  max_strength: 757  final_strength: 144.5  sample_efficiency: -2.37326e-06  training_efficiency: 2.88466e-06  stability: 0.300288
[2021-06-17 04:19:56,071 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 820000  wall_t: 4046  opt_step: 608472  frame: 820000  fps: 202.669  total_reward: 612.5  total_reward_ma: 701.168  loss: 7.76278e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:19:56,115 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 701.168  strength: 58.1684  max_strength: 932  final_strength: -30.5  sample_efficiency: 1.58242e-06  training_efficiency: 5.23629e-07  stability: -0.356379
[2021-06-17 04:19:58,594 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 820000  wall_t: 4048  opt_step: 608472  frame: 820000  fps: 202.569  total_reward: 812.5  total_reward_ma: 721.08  loss: 0.000299187  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:19:58,650 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 721.08  strength: 78.0801  max_strength: 1069.5  final_strength: 169.5  sample_efficiency: 1.43342e-07  training_efficiency: 4.69462e-06  stability: -0.293504
[2021-06-17 04:20:36,181 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 830000  wall_t: 4086  opt_step: 615984  frame: 830000  fps: 203.133  total_reward: 875  total_reward_ma: 779.475  loss: 0.00444769  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:20:36,346 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 779.475  strength: 136.475  max_strength: 757  final_strength: 232  sample_efficiency: -2.29997e-06  training_efficiency: 2.85883e-06  stability: 0.3094
[2021-06-17 04:20:36,385 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 830000  wall_t: 4086  opt_step: 615984  frame: 830000  fps: 203.133  total_reward: 1137.5  total_reward_ma: 789.852  loss: 0.00023363  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:20:36,456 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 789.852  strength: 146.852  max_strength: 857  final_strength: 494.5  sample_efficiency: 3.231e-06  training_efficiency: 7.60819e-06  stability: 0.38645
[2021-06-17 04:20:42,779 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 830000  wall_t: 4092  opt_step: 615984  frame: 830000  fps: 202.835  total_reward: 600  total_reward_ma: 699.935  loss: 0.000309186  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:20:42,830 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 699.935  strength: 56.9347  max_strength: 932  final_strength: -43  sample_efficiency: 1.5859e-06  training_efficiency: 5.135e-07  stability: -0.367812
[2021-06-17 04:20:44,827 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 830000  wall_t: 4094  opt_step: 615984  frame: 830000  fps: 202.736  total_reward: 725  total_reward_ma: 721.127  loss: 0.000210872  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:20:44,884 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 721.127  strength: 78.1274  max_strength: 1069.5  final_strength: 82  sample_efficiency: 1.56764e-07  training_efficiency: 4.65578e-06  stability: -0.272926
[2021-06-17 04:21:22,009 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 840000  wall_t: 4132  opt_step: 623496  frame: 840000  fps: 203.291  total_reward: 650  total_reward_ma: 777.934  loss: 0.000174877  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:21:22,078 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 777.934  strength: 134.934  max_strength: 757  final_strength: 7  sample_efficiency: -2.29782e-06  training_efficiency: 2.85805e-06  stability: 0.303681
[2021-06-17 04:21:22,582 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 840000  wall_t: 4132  opt_step: 623496  frame: 840000  fps: 203.291  total_reward: 837.5  total_reward_ma: 790.419  loss: 0.000322081  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:21:22,633 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 790.419  strength: 147.42  max_strength: 857  final_strength: 194.5  sample_efficiency: 3.19895e-06  training_efficiency: 7.51388e-06  stability: 0.386729
[2021-06-17 04:21:28,910 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 840000  wall_t: 4139  opt_step: 623496  frame: 840000  fps: 202.948  total_reward: 450  total_reward_ma: 696.923  loss: 0.00774248  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:21:28,964 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 696.923  strength: 53.9234  max_strength: 932  final_strength: -193  sample_efficiency: 1.60295e-06  training_efficiency: 4.66481e-07  stability: -0.41254
[2021-06-17 04:21:31,163 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 840000  wall_t: 4141  opt_step: 623496  frame: 840000  fps: 202.85  total_reward: 437.5  total_reward_ma: 717.751  loss: 0.00399673  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:21:31,204 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 717.751  strength: 74.7508  max_strength: 1069.5  final_strength: -205.5  sample_efficiency: 1.22933e-07  training_efficiency: 4.75567e-06  stability: -0.301165
[2021-06-17 04:22:08,134 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 850000  wall_t: 4178  opt_step: 631008  frame: 850000  fps: 203.447  total_reward: 650  total_reward_ma: 776.429  loss: 7.76003e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:22:08,178 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 776.429  strength: 133.429  max_strength: 757  final_strength: 7  sample_efficiency: -2.29567e-06  training_efficiency: 2.85727e-06  stability: 0.304111
[2021-06-17 04:22:08,802 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 850000  wall_t: 4178  opt_step: 631008  frame: 850000  fps: 203.447  total_reward: 537.5  total_reward_ma: 787.444  loss: 9.06681e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:22:08,845 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 787.444  strength: 144.444  max_strength: 857  final_strength: -105.5  sample_efficiency: 3.21633e-06  training_efficiency: 7.56483e-06  stability: 0.372135
[2021-06-17 04:22:15,366 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 850000  wall_t: 4185  opt_step: 631008  frame: 850000  fps: 203.106  total_reward: 600  total_reward_ma: 695.77  loss: 9.54145e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:22:15,407 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 695.77  strength: 52.7696  max_strength: 932  final_strength: -43  sample_efficiency: 1.60709e-06  training_efficiency: 4.55633e-07  stability: -0.473452
[2021-06-17 04:22:17,442 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 850000  wall_t: 4187  opt_step: 631008  frame: 850000  fps: 203.009  total_reward: 712.5  total_reward_ma: 717.689  loss: 0.000109036  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:22:17,526 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 717.689  strength: 74.6891  max_strength: 1069.5  final_strength: 69.5  sample_efficiency: 1.34467e-07  training_efficiency: 4.72095e-06  stability: -0.34375
[2021-06-17 04:22:54,358 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 860000  wall_t: 4224  opt_step: 638520  frame: 860000  fps: 203.598  total_reward: 637.5  total_reward_ma: 774.813  loss: 7.0573e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:22:54,402 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 774.813  strength: 131.813  max_strength: 757  final_strength: -5.5  sample_efficiency: -2.29735e-06  training_efficiency: 2.85789e-06  stability: 0.303439
[2021-06-17 04:22:54,548 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 860000  wall_t: 4224  opt_step: 638520  frame: 860000  fps: 203.598  total_reward: 675  total_reward_ma: 786.136  loss: 0.000159182  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:22:54,600 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 786.136  strength: 143.136  max_strength: 857  final_strength: 32  sample_efficiency: 3.21099e-06  training_efficiency: 7.54924e-06  stability: 0.36674
[2021-06-17 04:23:01,506 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 860000  wall_t: 4231  opt_step: 638520  frame: 860000  fps: 203.262  total_reward: 450  total_reward_ma: 692.878  loss: 0.000157033  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:23:01,546 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 692.878  strength: 49.8781  max_strength: 932  final_strength: -193  sample_efficiency: 1.62732e-06  training_efficiency: 4.0508e-07  stability: -0.521585
[2021-06-17 04:23:03,343 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 860000  wall_t: 4233  opt_step: 638520  frame: 860000  fps: 203.166  total_reward: 775  total_reward_ma: 718.355  loss: 4.7012e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:23:03,391 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 718.355  strength: 75.3555  max_strength: 1069.5  final_strength: 132  sample_efficiency: 1.55412e-07  training_efficiency: 4.65669e-06  stability: -0.329039
[2021-06-17 04:23:40,619 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 870000  wall_t: 4270  opt_step: 646032  frame: 870000  fps: 203.747  total_reward: 550  total_reward_ma: 783.422  loss: 8.31477e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:23:40,669 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 783.422  strength: 140.422  max_strength: 857  final_strength: -93  sample_efficiency: 3.22669e-06  training_efficiency: 7.59492e-06  stability: 0.358232
[2021-06-17 04:23:40,702 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 870000  wall_t: 4270  opt_step: 646032  frame: 870000  fps: 203.747  total_reward: 600  total_reward_ma: 772.804  loss: 0.00412003  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:23:40,743 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 772.804  strength: 129.804  max_strength: 757  final_strength: -43  sample_efficiency: -2.31048e-06  training_efficiency: 2.86288e-06  stability: 0.299793
[2021-06-17 04:23:47,661 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 870000  wall_t: 4277  opt_step: 646032  frame: 870000  fps: 203.414  total_reward: 625  total_reward_ma: 692.089  loss: 7.53962e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:23:47,702 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 692.089  strength: 49.0889  max_strength: 932  final_strength: -18  sample_efficiency: 1.62935e-06  training_efficiency: 4.00207e-07  stability: -0.590852
[2021-06-17 04:23:49,219 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 870000  wall_t: 4279  opt_step: 646032  frame: 870000  fps: 203.319  total_reward: 800  total_reward_ma: 719.294  loss: 5.9539e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:23:49,297 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 719.294  strength: 76.2939  max_strength: 1069.5  final_strength: 157  sample_efficiency: 1.78924e-07  training_efficiency: 4.58316e-06  stability: -0.301969
[2021-06-17 04:24:26,093 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 880000  wall_t: 4316  opt_step: 653544  frame: 880000  fps: 203.892  total_reward: 850  total_reward_ma: 784.179  loss: 0.000102141  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:24:26,181 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 880000  wall_t: 4316  opt_step: 653544  frame: 880000  fps: 203.892  total_reward: 587.5  total_reward_ma: 770.698  loss: 0.00404633  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:24:26,222 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 784.179  strength: 141.179  max_strength: 857  final_strength: 207  sample_efficiency: 3.19186e-06  training_efficiency: 7.49387e-06  stability: 0.353346
[2021-06-17 04:24:26,266 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 770.698  strength: 127.698  max_strength: 757  final_strength: -55.5  sample_efficiency: -2.3275e-06  training_efficiency: 2.86946e-06  stability: 0.29602
[2021-06-17 04:24:33,566 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 880000  wall_t: 4323  opt_step: 653544  frame: 880000  fps: 203.562  total_reward: 887.5  total_reward_ma: 694.335  loss: 0.00406943  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:24:33,695 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 694.335  strength: 51.335  max_strength: 932  final_strength: 244.5  sample_efficiency: 1.60236e-06  training_efficiency: 4.62065e-07  stability: -0.597635
[2021-06-17 04:24:35,051 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 880000  wall_t: 4325  opt_step: 653544  frame: 880000  fps: 203.468  total_reward: 550  total_reward_ma: 717.37  loss: 0.0039582  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:24:35,155 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 717.37  strength: 74.3701  max_strength: 1069.5  final_strength: -93  sample_efficiency: 1.65319e-07  training_efficiency: 4.62655e-06  stability: -0.308837
[2021-06-17 04:25:12,540 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 890000  wall_t: 4362  opt_step: 661056  frame: 890000  fps: 204.035  total_reward: 725  total_reward_ma: 783.514  loss: 0.00405272  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:25:12,613 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 783.514  strength: 140.514  max_strength: 857  final_strength: 82  sample_efficiency: 3.1783e-06  training_efficiency: 7.45466e-06  stability: 0.354059
[2021-06-17 04:25:12,843 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 890000  wall_t: 4362  opt_step: 661056  frame: 890000  fps: 204.035  total_reward: 500  total_reward_ma: 767.656  loss: 0.00437969  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:25:12,890 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 767.656  strength: 124.657  max_strength: 757  final_strength: -143  sample_efficiency: -2.37198e-06  training_efficiency: 2.88695e-06  stability: 0.284756
[2021-06-17 04:25:20,054 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 890000  wall_t: 4370  opt_step: 661056  frame: 890000  fps: 203.661  total_reward: 962.5  total_reward_ma: 697.382  loss: 0.000101053  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:25:20,100 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 697.382  strength: 54.3823  max_strength: 932  final_strength: 319.5  sample_efficiency: 1.5704e-06  training_efficiency: 5.3221e-07  stability: -0.510172
[2021-06-17 04:25:21,457 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 890000  wall_t: 4371  opt_step: 661056  frame: 890000  fps: 203.615  total_reward: 825  total_reward_ma: 718.579  loss: 0.00430506  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:25:21,507 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 718.579  strength: 75.5795  max_strength: 1069.5  final_strength: 182  sample_efficiency: 1.91247e-07  training_efficiency: 4.5423e-06  stability: -0.327436
[2021-06-17 04:25:58,526 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 900000  wall_t: 4408  opt_step: 668568  frame: 900000  fps: 204.174  total_reward: 837.5  total_reward_ma: 784.114  loss: 6.25071e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:25:58,590 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 784.114  strength: 141.114  max_strength: 857  final_strength: 194.5  sample_efficiency: 3.14664e-06  training_efficiency: 7.3634e-06  stability: 0.358295
[2021-06-17 04:25:59,052 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 900000  wall_t: 4409  opt_step: 668568  frame: 900000  fps: 204.128  total_reward: 500  total_reward_ma: 764.683  loss: 0.000108447  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:25:59,184 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 764.683  strength: 121.683  max_strength: 757  final_strength: -143  sample_efficiency: -2.41746e-06  training_efficiency: 2.90512e-06  stability: 0.275537
[2021-06-17 04:26:06,168 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 900000  wall_t: 4416  opt_step: 668568  frame: 900000  fps: 203.804  total_reward: 787.5  total_reward_ma: 698.395  loss: 0.00434867  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:26:06,220 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 698.395  strength: 55.3949  max_strength: 932  final_strength: 144.5  sample_efficiency: 1.55694e-06  training_efficiency: 5.6045e-07  stability: -0.445917
[2021-06-17 04:26:07,893 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 900000  wall_t: 4418  opt_step: 668568  frame: 900000  fps: 203.712  total_reward: 662.5  total_reward_ma: 717.956  loss: 3.90246e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:26:07,938 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 717.956  strength: 74.9563  max_strength: 1069.5  final_strength: 19.5  sample_efficiency: 1.93906e-07  training_efficiency: 4.53349e-06  stability: -0.315678
[2021-06-17 04:26:45,015 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 910000  wall_t: 4455  opt_step: 676080  frame: 910000  fps: 204.265  total_reward: 625  total_reward_ma: 782.365  loss: 0.000104609  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:26:45,057 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 782.365  strength: 139.365  max_strength: 857  final_strength: -18  sample_efficiency: 3.14954e-06  training_efficiency: 7.37175e-06  stability: 0.35139
[2021-06-17 04:26:45,074 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 910000  wall_t: 4455  opt_step: 676080  frame: 910000  fps: 204.265  total_reward: 937.5  total_reward_ma: 766.582  loss: 0.000206428  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:26:45,118 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 766.582  strength: 123.582  max_strength: 757  final_strength: 294.5  sample_efficiency: -2.32538e-06  training_efficiency: 2.86777e-06  stability: 0.266078
[2021-06-17 04:26:52,010 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 910000  wall_t: 4462  opt_step: 676080  frame: 910000  fps: 203.944  total_reward: 625  total_reward_ma: 697.579  loss: 7.95242e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:26:52,064 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 697.579  strength: 54.5794  max_strength: 932  final_strength: -18  sample_efficiency: 1.55862e-06  training_efficiency: 5.57084e-07  stability: -0.436499
[2021-06-17 04:26:54,019 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 910000  wall_t: 4464  opt_step: 676080  frame: 910000  fps: 203.853  total_reward: 537.5  total_reward_ma: 715.973  loss: 7.77785e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:26:54,123 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 715.973  strength: 72.9733  max_strength: 1069.5  final_strength: -105.5  sample_efficiency: 1.79528e-07  training_efficiency: 4.58201e-06  stability: -0.330404
[2021-06-17 04:27:31,150 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 920000  wall_t: 4501  opt_step: 683592  frame: 920000  fps: 204.399  total_reward: 787.5  total_reward_ma: 782.421  loss: 5.74366e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:27:31,261 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 782.421  strength: 139.421  max_strength: 857  final_strength: 144.5  sample_efficiency: 3.12631e-06  training_efficiency: 7.30518e-06  stability: 0.35047
[2021-06-17 04:27:31,405 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 920000  wall_t: 4501  opt_step: 683592  frame: 920000  fps: 204.399  total_reward: 750  total_reward_ma: 766.401  loss: 0.00367244  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:27:31,457 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 766.401  strength: 123.401  max_strength: 757  final_strength: 107  sample_efficiency: -2.29322e-06  training_efficiency: 2.85453e-06  stability: 0.268624
[2021-06-17 04:27:38,514 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 920000  wall_t: 4508  opt_step: 683592  frame: 920000  fps: 204.082  total_reward: 825  total_reward_ma: 698.98  loss: 0.000135038  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:27:38,557 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 698.98  strength: 55.9796  max_strength: 932  final_strength: 182  sample_efficiency: 1.54177e-06  training_efficiency: 5.89445e-07  stability: -0.441762
[2021-06-17 04:27:40,632 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 920000  wall_t: 4510  opt_step: 683592  frame: 920000  fps: 203.991  total_reward: 525  total_reward_ma: 713.898  loss: 2.35827e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:27:40,686 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 713.898  strength: 70.8975  max_strength: 1069.5  final_strength: -118  sample_efficiency: 1.63112e-07  training_efficiency: 4.63844e-06  stability: -0.353423
[2021-06-17 04:28:17,536 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 930000  wall_t: 4547  opt_step: 691104  frame: 930000  fps: 204.53  total_reward: 675  total_reward_ma: 765.419  loss: 0.000266741  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:28:17,594 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 765.419  strength: 122.419  max_strength: 757  final_strength: 32  sample_efficiency: -2.28375e-06  training_efficiency: 2.85058e-06  stability: 0.268911
[2021-06-17 04:28:17,969 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 930000  wall_t: 4548  opt_step: 691104  frame: 930000  fps: 204.485  total_reward: 1000  total_reward_ma: 784.761  loss: 0.000309714  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:28:18,011 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 784.761  strength: 141.761  max_strength: 857  final_strength: 357  sample_efficiency: 3.07077e-06  training_efficiency: 7.14655e-06  stability: 0.357787
[2021-06-17 04:28:24,978 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 930000  wall_t: 4555  opt_step: 691104  frame: 930000  fps: 204.171  total_reward: 887.5  total_reward_ma: 701.029  loss: 0.00015378  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:28:25,071 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 701.029  strength: 58.0287  max_strength: 932  final_strength: 244.5  sample_efficiency: 1.5204e-06  training_efficiency: 6.28717e-07  stability: -0.390252
[2021-06-17 04:28:26,838 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 930000  wall_t: 4556  opt_step: 691104  frame: 930000  fps: 204.126  total_reward: 537.5  total_reward_ma: 712.001  loss: 0.000125987  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:28:26,895 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 712.001  strength: 69.0008  max_strength: 1069.5  final_strength: -105.5  sample_efficiency: 1.48115e-07  training_efficiency: 4.69091e-06  stability: -0.377908
[2021-06-17 04:29:04,076 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 940000  wall_t: 4594  opt_step: 698616  frame: 940000  fps: 204.615  total_reward: 500  total_reward_ma: 762.595  loss: 9.26018e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:29:04,167 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 762.595  strength: 119.595  max_strength: 757  final_strength: -143  sample_efficiency: -2.32633e-06  training_efficiency: 2.86863e-06  stability: 0.255595
[2021-06-17 04:29:04,330 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 940000  wall_t: 4594  opt_step: 698616  frame: 940000  fps: 204.615  total_reward: 1162.5  total_reward_ma: 788.779  loss: 0.0037699  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:29:04,457 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 788.779  strength: 145.779  max_strength: 857  final_strength: 519.5  sample_efficiency: 2.99468e-06  training_efficiency: 6.92988e-06  stability: 0.375177
[2021-06-17 04:29:11,580 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 940000  wall_t: 4601  opt_step: 698616  frame: 940000  fps: 204.303  total_reward: 1300  total_reward_ma: 707.469  loss: 0.000192038  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:29:11,650 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 707.469  strength: 64.4693  max_strength: 932  final_strength: 657  sample_efficiency: 1.47037e-06  training_efficiency: 7.16675e-07  stability: -0.326581
[2021-06-17 04:29:13,341 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 940000  wall_t: 4603  opt_step: 698616  frame: 940000  fps: 204.215  total_reward: 825  total_reward_ma: 713.203  loss: 0.00406358  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:29:13,385 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 713.203  strength: 70.2029  max_strength: 1069.5  final_strength: 182  sample_efficiency: 1.7337e-07  training_efficiency: 4.60102e-06  stability: -0.400561
[2021-06-17 04:29:50,296 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 950000  wall_t: 4640  opt_step: 706128  frame: 950000  fps: 204.741  total_reward: 287.5  total_reward_ma: 757.594  loss: 0.00414425  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:29:50,343 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 757.594  strength: 114.594  max_strength: 757  final_strength: -355.5  sample_efficiency: -2.43667e-06  training_efficiency: 2.91606e-06  stability: 0.227223
[2021-06-17 04:29:50,396 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 950000  wall_t: 4640  opt_step: 706128  frame: 950000  fps: 204.741  total_reward: 887.5  total_reward_ma: 789.818  loss: 0.000193356  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:29:50,498 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 789.818  strength: 146.818  max_strength: 857  final_strength: 244.5  sample_efficiency: 2.96064e-06  training_efficiency: 6.83323e-06  stability: 0.378796
[2021-06-17 04:29:58,084 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 950000  wall_t: 4648  opt_step: 706128  frame: 950000  fps: 204.389  total_reward: 1250  total_reward_ma: 713.241  loss: 0.00382619  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:29:58,187 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 713.241  strength: 70.2409  max_strength: 932  final_strength: 607  sample_efficiency: 1.43197e-06  training_efficiency: 7.80982e-07  stability: -0.189554
[2021-06-17 04:29:59,246 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 950000  wall_t: 4649  opt_step: 706128  frame: 950000  fps: 204.345  total_reward: 700  total_reward_ma: 713.064  loss: 0.00812372  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:29:59,292 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 713.064  strength: 70.0639  max_strength: 1069.5  final_strength: 57  sample_efficiency: 1.809e-07  training_efficiency: 4.57374e-06  stability: -0.380876
[2021-06-17 04:30:36,721 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 960000  wall_t: 4686  opt_step: 713640  frame: 960000  fps: 204.866  total_reward: 512.5  total_reward_ma: 755.041  loss: 0.000149285  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:30:36,771 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 960000  wall_t: 4686  opt_step: 713640  frame: 960000  fps: 204.866  total_reward: 737.5  total_reward_ma: 789.273  loss: 0.000150894  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:30:36,917 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 789.273  strength: 146.273  max_strength: 857  final_strength: 94.5  sample_efficiency: 2.94773e-06  training_efficiency: 6.79667e-06  stability: 0.378932
[2021-06-17 04:30:36,932 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 755.041  strength: 112.041  max_strength: 757  final_strength: -130.5  sample_efficiency: -2.47888e-06  training_efficiency: 2.93444e-06  stability: 0.201988
[2021-06-17 04:30:44,183 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 960000  wall_t: 4694  opt_step: 713640  frame: 960000  fps: 204.516  total_reward: 762.5  total_reward_ma: 713.759  loss: 0.000129064  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:30:44,304 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 713.759  strength: 70.7594  max_strength: 932  final_strength: 119.5  sample_efficiency: 1.42503e-06  training_efficiency: 7.92009e-07  stability: -0.154029
[2021-06-17 04:30:45,246 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 960000  wall_t: 4695  opt_step: 713640  frame: 960000  fps: 204.473  total_reward: 775  total_reward_ma: 713.709  loss: 7.17018e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:30:45,373 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 713.709  strength: 70.7091  max_strength: 1069.5  final_strength: 132  sample_efficiency: 1.97638e-07  training_efficiency: 4.51205e-06  stability: -0.369051
[2021-06-17 04:31:22,738 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 970000  wall_t: 4732  opt_step: 721152  frame: 970000  fps: 204.987  total_reward: 875  total_reward_ma: 790.157  loss: 0.00398021  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:31:22,883 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 790.157  strength: 147.157  max_strength: 857  final_strength: 232  sample_efficiency: 2.91657e-06  training_efficiency: 6.70874e-06  stability: 0.383111
[2021-06-17 04:31:23,439 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 970000  wall_t: 4733  opt_step: 721152  frame: 970000  fps: 204.944  total_reward: 762.5  total_reward_ma: 755.118  loss: 0.0116954  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:31:23,485 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 755.118  strength: 112.118  max_strength: 757  final_strength: 119.5  sample_efficiency: -2.44031e-06  training_efficiency: 2.91743e-06  stability: 0.192306
[2021-06-17 04:31:30,340 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 970000  wall_t: 4740  opt_step: 721152  frame: 970000  fps: 204.641  total_reward: 525  total_reward_ma: 711.793  loss: 0.000113763  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:31:30,383 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 711.793  strength: 68.7932  max_strength: 932  final_strength: -118  sample_efficiency: 1.43207e-06  training_efficiency: 7.81384e-07  stability: -0.168845
[2021-06-17 04:31:31,269 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 970000  wall_t: 4741  opt_step: 721152  frame: 970000  fps: 204.598  total_reward: 712.5  total_reward_ma: 713.697  loss: 7.54946e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:31:31,334 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 713.697  strength: 70.6966  max_strength: 1069.5  final_strength: 69.5  sample_efficiency: 2.06084e-07  training_efficiency: 4.48038e-06  stability: -0.351636
[2021-06-17 04:32:08,953 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 980000  wall_t: 4779  opt_step: 728664  frame: 980000  fps: 205.064  total_reward: 687.5  total_reward_ma: 789.11  loss: 0.000136239  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:32:08,999 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 789.11  strength: 146.11  max_strength: 857  final_strength: 44.5  sample_efficiency: 2.91068e-06  training_efficiency: 6.69216e-06  stability: 0.380002
[2021-06-17 04:32:09,726 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 980000  wall_t: 4779  opt_step: 728664  frame: 980000  fps: 205.064  total_reward: 837.5  total_reward_ma: 755.958  loss: 0.000143596  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:32:09,772 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 755.958  strength: 112.958  max_strength: 757  final_strength: 194.5  sample_efficiency: -2.3795e-06  training_efficiency: 2.89029e-06  stability: 0.201181
[2021-06-17 04:32:16,688 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 980000  wall_t: 4786  opt_step: 728664  frame: 980000  fps: 204.764  total_reward: 662.5  total_reward_ma: 711.285  loss: 6.30679e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:32:16,765 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 711.285  strength: 68.285  max_strength: 932  final_strength: 19.5  sample_efficiency: 1.43086e-06  training_efficiency: 7.83123e-07  stability: -0.189729
[2021-06-17 04:32:17,858 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 980000  wall_t: 4787  opt_step: 728664  frame: 980000  fps: 204.721  total_reward: 550  total_reward_ma: 712.026  loss: 3.93966e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:32:17,904 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 712.026  strength: 69.0262  max_strength: 1069.5  final_strength: -93  sample_efficiency: 1.94888e-07  training_efficiency: 4.52311e-06  stability: -0.361634
[2021-06-17 04:32:55,084 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 990000  wall_t: 4825  opt_step: 736176  frame: 990000  fps: 205.181  total_reward: 550  total_reward_ma: 786.694  loss: 9.30514e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:32:55,130 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 786.694  strength: 143.694  max_strength: 857  final_strength: -93  sample_efficiency: 2.9231e-06  training_efficiency: 6.72703e-06  stability: 0.372326
[2021-06-17 04:32:56,235 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 990000  wall_t: 4826  opt_step: 736176  frame: 990000  fps: 205.139  total_reward: 900  total_reward_ma: 757.413  loss: 0.000259312  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:32:56,283 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 757.413  strength: 114.413  max_strength: 757  final_strength: 257  sample_efficiency: -2.3026e-06  training_efficiency: 2.85553e-06  stability: 0.215216
[2021-06-17 04:33:02,790 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 990000  wall_t: 4832  opt_step: 736176  frame: 990000  fps: 204.884  total_reward: 612.5  total_reward_ma: 710.277  loss: 2.5735e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:33:02,832 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 710.277  strength: 67.277  max_strength: 932  final_strength: -30.5  sample_efficiency: 1.4328e-06  training_efficiency: 7.80462e-07  stability: -0.193776
[2021-06-17 04:33:04,072 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 990000  wall_t: 4834  opt_step: 736176  frame: 990000  fps: 204.799  total_reward: 825  total_reward_ma: 713.167  loss: 6.47468e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:33:04,115 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 713.167  strength: 70.1674  max_strength: 1069.5  final_strength: 182  sample_efficiency: 2.16247e-07  training_efficiency: 4.44019e-06  stability: -0.380354
[2021-06-17 04:33:40,803 PID:36048 INFO __init__.py log_summary] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 1e+06  wall_t: 4870  opt_step: 743688  frame: 1e+06  fps: 205.339  total_reward: 600  total_reward_ma: 784.827  loss: 0.00396125  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:33:40,844 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 784.827  strength: 141.827  max_strength: 857  final_strength: -43  sample_efficiency: 2.92894e-06  training_efficiency: 6.74335e-06  stability: 0.368223
[2021-06-17 04:33:41,818 PID:36049 INFO __init__.py log_summary] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 1e+06  wall_t: 4871  opt_step: 743688  frame: 1e+06  fps: 205.297  total_reward: 1162.5  total_reward_ma: 761.464  loss: 0.000457398  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:33:41,880 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 761.464  strength: 118.464  max_strength: 757  final_strength: 519.5  sample_efficiency: -2.15777e-06  training_efficiency: 2.78927e-06  stability: 0.233022
[2021-06-17 04:33:46,739 PID:36050 INFO __init__.py log_summary] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 1e+06  wall_t: 4876  opt_step: 743688  frame: 1e+06  fps: 205.086  total_reward: 550  total_reward_ma: 708.658  loss: 9.3397e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:33:46,795 PID:36050 INFO __init__.py log_metrics] Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 708.658  strength: 65.658  max_strength: 932  final_strength: -93  sample_efficiency: 1.439e-06  training_efficiency: 7.7239e-07  stability: -0.208778
[2021-06-17 04:33:47,326 PID:36047 INFO __init__.py log_summary] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 1e+06  wall_t: 4877  opt_step: 743688  frame: 1e+06  fps: 205.044  total_reward: 887.5  total_reward_ma: 714.911  loss: 2.03821e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:33:47,411 PID:36047 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 714.911  strength: 71.9107  max_strength: 1069.5  final_strength: 244.5  sample_efficiency: 2.42895e-07  training_efficiency: 4.33494e-06  stability: -0.344188
[2021-06-17 04:35:11,713 PID:36050 WARNING viz.py <lambda>][2021-06-17 04:35:11,713 PID:36049 WARNING viz.py <lambda>][2021-06-17 04:35:11,713 PID:36047 WARNING viz.py <lambda>] Failed to generate graph. Run retro-analysis to generate graphs later. 
For some reason plotly.py was unable to communicate with the
local orca server process, even though the server process seems to be running.

Please review the process and connection information below:

orca status
-----------
    state: running
    executable: /home/cvladu/anaconda3/envs/lab/bin/orca
    version: 1.3.0
    port: 45529
    pid: 37910
    command: ['/home/cvladu/anaconda3/envs/lab/bin/orca', 'serve', '-p', '45529', '--plotly', '/home/cvladu/anaconda3/envs/lab/lib/python3.7/site-packages/plotly/package_data/plotly.min.js', '--graph-only', '--mathjax', 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js']



If running on a headless server, prepend your Python command with `xvfb-run -a `, for example `xvfb-run -a python run_lab.py`
 Failed to generate graph. Run retro-analysis to generate graphs later. 
For some reason plotly.py was unable to communicate with the
local orca server process, even though the server process seems to be running.

Please review the process and connection information below:

orca status
-----------
    state: running
    executable: /home/cvladu/anaconda3/envs/lab/bin/orca
    version: 1.3.0
    port: 45355
    pid: 37907
    command: ['/home/cvladu/anaconda3/envs/lab/bin/orca', 'serve', '-p', '45355', '--plotly', '/home/cvladu/anaconda3/envs/lab/lib/python3.7/site-packages/plotly/package_data/plotly.min.js', '--graph-only', '--mathjax', 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js']



If running on a headless server, prepend your Python command with `xvfb-run -a `, for example `xvfb-run -a python run_lab.py` Failed to generate graph. Run retro-analysis to generate graphs later. 
For some reason plotly.py was unable to communicate with the
local orca server process, even though the server process seems to be running.

Please review the process and connection information below:

orca status
-----------
    state: running
    executable: /home/cvladu/anaconda3/envs/lab/bin/orca
    version: 1.3.0
    port: 38875
    pid: 37908
    command: ['/home/cvladu/anaconda3/envs/lab/bin/orca', 'serve', '-p', '38875', '--plotly', '/home/cvladu/anaconda3/envs/lab/lib/python3.7/site-packages/plotly/package_data/plotly.min.js', '--graph-only', '--mathjax', 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js']



If running on a headless server, prepend your Python command with `xvfb-run -a `, for example `xvfb-run -a python run_lab.py`

[2021-06-17 04:35:11,713 PID:36048 WARNING viz.py <lambda>] Failed to generate graph. Run retro-analysis to generate graphs later. 
For some reason plotly.py was unable to communicate with the
local orca server process, even though the server process seems to be running.

Please review the process and connection information below:

orca status
-----------
    state: running
    executable: /home/cvladu/anaconda3/envs/lab/bin/orca
    version: 1.3.0
    port: 45765
    pid: 37909
    command: ['/home/cvladu/anaconda3/envs/lab/bin/orca', 'serve', '-p', '45765', '--plotly', '/home/cvladu/anaconda3/envs/lab/lib/python3.7/site-packages/plotly/package_data/plotly.min.js', '--graph-only', '--mathjax', 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js']



If running on a headless server, prepend your Python command with `xvfb-run -a `, for example `xvfb-run -a python run_lab.py`
[2021-06-17 04:36:07,338 PID:36048 INFO __init__.py log_metrics] Trial 0 session 1 DDQN_PrioritizedReplay_t0_s1 [eval_df metrics] final_return_ma: 784.827  strength: 141.827  max_strength: 857  final_strength: -43  sample_efficiency: 2.92894e-06  training_efficiency: 6.74335e-06  stability: 0.368223
[2021-06-17 04:36:07,358 PID:36047 INFO __init__.py log_metrics][2021-06-17 04:36:07,358 PID:36050 INFO __init__.py log_metrics] Trial 0 session 0 DDQN_PrioritizedReplay_t0_s0 [eval_df metrics] final_return_ma: 714.911  strength: 71.9107  max_strength: 1069.5  final_strength: 244.5  sample_efficiency: 2.42895e-07  training_efficiency: 4.33494e-06  stability: -0.344188 Trial 0 session 3 DDQN_PrioritizedReplay_t0_s3 [eval_df metrics] final_return_ma: 708.658  strength: 65.658  max_strength: 932  final_strength: -93  sample_efficiency: 1.439e-06  training_efficiency: 7.7239e-07  stability: -0.208778

[2021-06-17 04:36:07,358 PID:36049 INFO __init__.py log_metrics] Trial 0 session 2 DDQN_PrioritizedReplay_t0_s2 [eval_df metrics] final_return_ma: 761.464  strength: 118.464  max_strength: 757  final_strength: 519.5  sample_efficiency: -2.15777e-06  training_efficiency: 2.78927e-06  stability: 0.233022
[2021-06-17 04:36:24,415 PID:36048 INFO logger.py info] Session 1 done
[2021-06-17 04:36:24,561 PID:36050 INFO logger.py info] Session 3 done
[2021-06-17 04:36:24,605 PID:36047 INFO logger.py info] Session 0 done
[2021-06-17 04:36:24,616 PID:36049 INFO logger.py info] Session 2 done
[2021-06-17 04:38:44,226 PID:35818 INFO analysis.py analyze_trial] All trial data zipped to data/DDQN_PrioritizedReplay_2021_06_17_031227.zip
[2021-06-17 04:38:44,226 PID:35818 INFO logger.py info] Trial 0 done
[2021-06-17 04:39:03,713 PID:38460 INFO run_lab.py get_spec_and_run] Running lab spec_file:../../../config_files/RL_project_wizardofwor.json spec_name:Dueling_DDQN_PrioritizedReplay in mode:train
[2021-06-17 04:39:03,857 PID:38460 INFO logger.py info] Running sessions
[2021-06-17 04:39:06,836 PID:38690 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7fc113395be0>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7fc108770ef0>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 04:39:06,841 PID:38688 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7ff4c7d67be0>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7ff4bd130ef0>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 04:39:06,864 PID:38687 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7f366af0fbe0>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7f36602f0e80>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 04:39:06,873 PID:38689 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7f9256a57400>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7f91c5b70e80>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 04:39:42,153 PID:38689 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train
[2021-06-17 04:39:42,153 PID:38688 INFO base.py end_init_nets][2021-06-17 04:39:42,153 PID:38687 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train Initialized algorithm models for lab_mode: train
[2021-06-17 04:39:42,153 PID:38690 INFO base.py end_init_nets]
 Initialized algorithm models for lab_mode: train
[2021-06-17 04:39:46,619 PID:38690 INFO base.py __init__][2021-06-17 04:39:46,619 PID:38688 INFO base.py __init__][2021-06-17 04:39:46,619 PID:38689 INFO base.py __init__][2021-06-17 04:39:46,619 PID:38687 INFO base.py __init__] DoubleDQN:
- agent = <slm_lab.agent.Agent object at 0x7ff4b545c5f8>
- action_pdtype = Categorical
- action_policy = <function epsilon_greedy at 0x7ff4c52292f0>
- explore_var_spec = {'end_step': 1000000,
 'end_val': 0.01,
 'name': 'linear_decay',
 'start_step': 10000,
 'start_value': 1.0}
- training_start_step = 10000
- gamma = 0.99
- training_batch_iter = 1
- training_iter = 4
- training_frequency = 32
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7ff4b545cac8>
- net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- target_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- net_names = ['net', 'target_net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7ff4b5423438>
- global_net = None
- global_target_net = None
- online_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- eval_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
 DoubleDQN:
- agent = <slm_lab.agent.Agent object at 0x7f91bde9c588>
- action_pdtype = Categorical
- action_policy = <function epsilon_greedy at 0x7f91cdc6a2f0>
- explore_var_spec = {'end_step': 1000000,
 'end_val': 0.01,
 'name': 'linear_decay',
 'start_step': 10000,
 'start_value': 1.0}
- training_start_step = 10000
- gamma = 0.99
- training_batch_iter = 1
- training_iter = 4
- training_frequency = 32
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f91bde9c4e0>
- net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- target_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- net_names = ['net', 'target_net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7f91bde61470>
- global_net = None
- global_target_net = None
- online_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- eval_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
) DoubleDQN:
- agent = <slm_lab.agent.Agent object at 0x7f3658600588>
- action_pdtype = Categorical
- action_policy = <function epsilon_greedy at 0x7f36683e92f0>
- explore_var_spec = {'end_step': 1000000,
 'end_val': 0.01,
 'name': 'linear_decay',
 'start_step': 10000,
 'start_value': 1.0}
- training_start_step = 10000
- gamma = 0.99
- training_batch_iter = 1
- training_iter = 4
- training_frequency = 32
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f3658600470>
- net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- target_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- net_names = ['net', 'target_net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7f36585c7438>
- global_net = None
- global_target_net = None
- online_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- eval_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)

 DoubleDQN:
- agent = <slm_lab.agent.Agent object at 0x7fc100a805c0>
- action_pdtype = Categorical
- action_policy = <function epsilon_greedy at 0x7fc1108682f0>
- explore_var_spec = {'end_step': 1000000,
 'end_val': 0.01,
 'name': 'linear_decay',
 'start_step': 10000,
 'start_value': 1.0}
- training_start_step = 10000
- gamma = 0.99
- training_batch_iter = 1
- training_iter = 4
- training_frequency = 32
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7fc100a80588>
- net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- target_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- net_names = ['net', 'target_net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7fc100a47438>
- global_net = None
- global_target_net = None
- online_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
- eval_net = DuelingConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (v): Linear(in_features=512, out_features=1, bias=True)
  (adv): Linear(in_features=512, out_features=10, bias=True)
  (model_tails): ModuleList(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Linear(in_features=512, out_features=10, bias=True)
  )
  (loss_fn): SmoothL1Loss()
)
[2021-06-17 04:39:46,624 PID:38688 INFO __init__.py __init__][2021-06-17 04:39:46,624 PID:38689 INFO __init__.py __init__][2021-06-17 04:39:46,624 PID:38687 INFO __init__.py __init__][2021-06-17 04:39:46,624 PID:38690 INFO __init__.py __init__] Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_043903',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/graph/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'info_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/info/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'log_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/log/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/model/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'random_seed': 1623894945,
 'resume': False,
 'rigorous_eval': 0,
 'session': 1,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'epsilon_greedy',
               'center_return': True,
               'explore_var_spec': {'end_step': 1000000,
                                    'end_val': 0.01,
                                    'name': 'linear_decay',
                                    'start_step': 10000,
                                    'start_value': 1.0},
               'gamma': 0.99,
               'name': 'DoubleDQN',
               'training_batch_iter': 1,
               'training_frequency': 32,
               'training_iter': 4,
               'training_start_step': 10000},
 'memory': {'alpha': 0.6000000000000001,
            'batch_size': 32,
            'epsilon': 10000.0,
            'max_size': 10000,
            'name': 'PrioritizedReplay',
            'use_cer': False},
 'name': 'DuelingDDQN',
 'net': {'batch_norm': False,
         'clip_grad_val': 10.0,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': None,
         'loss_spec': {'name': 'SmoothL1Loss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'DuelingConvNet',
         'update_frequency': 1000,
         'update_type': 'replace'}}
- name = DuelingDDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7ff4b545c5f8>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7ff51940add8>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.prioritized.PrioritizedReplay object at 0x7ff4b545c6a0>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.DoubleDQN object at 0x7ff4b545c7f0>
 Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_043903',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/graph/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'info_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/info/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'log_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/log/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/model/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'random_seed': 1623895945,
 'resume': False,
 'rigorous_eval': 0,
 'session': 2,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'epsilon_greedy',
               'center_return': True,
               'explore_var_spec': {'end_step': 1000000,
                                    'end_val': 0.01,
                                    'name': 'linear_decay',
                                    'start_step': 10000,
                                    'start_value': 1.0},
               'gamma': 0.99,
               'name': 'DoubleDQN',
               'training_batch_iter': 1,
               'training_frequency': 32,
               'training_iter': 4,
               'training_start_step': 10000},
 'memory': {'alpha': 0.6000000000000001,
            'batch_size': 32,
            'epsilon': 10000.0,
            'max_size': 10000,
            'name': 'PrioritizedReplay',
            'use_cer': False},
 'name': 'DuelingDDQN',
 'net': {'batch_norm': False,
         'clip_grad_val': 10.0,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': None,
         'loss_spec': {'name': 'SmoothL1Loss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'DuelingConvNet',
         'update_frequency': 1000,
         'update_type': 'replace'}}
- name = DuelingDDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7f91bde9c588>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7f9221e49e10>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.prioritized.PrioritizedReplay object at 0x7f91bde9c780>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.DoubleDQN object at 0x7f91be422710>
 Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_043903',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/graph/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'info_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/info/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'log_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/log/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/model/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'random_seed': 1623896945,
 'resume': False,
 'rigorous_eval': 0,
 'session': 3,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'epsilon_greedy',
               'center_return': True,
               'explore_var_spec': {'end_step': 1000000,
                                    'end_val': 0.01,
                                    'name': 'linear_decay',
                                    'start_step': 10000,
                                    'start_value': 1.0},
               'gamma': 0.99,
               'name': 'DoubleDQN',
               'training_batch_iter': 1,
               'training_frequency': 32,
               'training_iter': 4,
               'training_start_step': 10000},
 'memory': {'alpha': 0.6000000000000001,
            'batch_size': 32,
            'epsilon': 10000.0,
            'max_size': 10000,
            'name': 'PrioritizedReplay',
            'use_cer': False},
 'name': 'DuelingDDQN',
 'net': {'batch_norm': False,
         'clip_grad_val': 10.0,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': None,
         'loss_spec': {'name': 'SmoothL1Loss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'DuelingConvNet',
         'update_frequency': 1000,
         'update_type': 'replace'}}
- name = DuelingDDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7fc100a805c0>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7fc164a42198>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.prioritized.PrioritizedReplay object at 0x7fc100a807f0>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.DoubleDQN object at 0x7fc100a804a8>
 Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_043903',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/graph/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'info_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/info/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'log_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/log/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/model/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'random_seed': 1623893945,
 'resume': False,
 'rigorous_eval': 0,
 'session': 0,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'epsilon_greedy',
               'center_return': True,
               'explore_var_spec': {'end_step': 1000000,
                                    'end_val': 0.01,
                                    'name': 'linear_decay',
                                    'start_step': 10000,
                                    'start_value': 1.0},
               'gamma': 0.99,
               'name': 'DoubleDQN',
               'training_batch_iter': 1,
               'training_frequency': 32,
               'training_iter': 4,
               'training_start_step': 10000},
 'memory': {'alpha': 0.6000000000000001,
            'batch_size': 32,
            'epsilon': 10000.0,
            'max_size': 10000,
            'name': 'PrioritizedReplay',
            'use_cer': False},
 'name': 'DuelingDDQN',
 'net': {'batch_norm': False,
         'clip_grad_val': 10.0,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': None,
         'loss_spec': {'name': 'SmoothL1Loss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'DuelingConvNet',
         'update_frequency': 1000,
         'update_type': 'replace'}}
- name = DuelingDDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7f3658600588>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7f36bc5bb0f0>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.prioritized.PrioritizedReplay object at 0x7f3658600780>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.DoubleDQN object at 0x7f36586007f0>
[2021-06-17 04:39:46,652 PID:38688 INFO logger.py info] Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_043903',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/graph/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'info_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/info/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'log_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/log/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/model/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/Dueling_DDQN_PrioritizedReplay_t0_s1',
 'random_seed': 1623894945,
 'resume': False,
 'rigorous_eval': 0,
 'session': 1,
 'trial': 0}
- index = 1
- agent = <slm_lab.agent.Agent object at 0x7ff4b545c5f8>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7ff51940add8>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7ff51940add8>
[2021-06-17 04:39:46,652 PID:38689 INFO logger.py info][2021-06-17 04:39:46,652 PID:38690 INFO logger.py info] Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_043903',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/graph/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'info_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/info/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'log_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/log/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/model/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/Dueling_DDQN_PrioritizedReplay_t0_s2',
 'random_seed': 1623895945,
 'resume': False,
 'rigorous_eval': 0,
 'session': 2,
 'trial': 0}
- index = 2
- agent = <slm_lab.agent.Agent object at 0x7f91bde9c588>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7f9221e49e10>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7f9221e49e10>
 Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_043903',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/graph/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'info_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/info/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'log_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/log/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/model/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/Dueling_DDQN_PrioritizedReplay_t0_s3',
 'random_seed': 1623896945,
 'resume': False,
 'rigorous_eval': 0,
 'session': 3,
 'trial': 0}
- index = 3
- agent = <slm_lab.agent.Agent object at 0x7fc100a805c0>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7fc164a42198>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7fc164a42198>
[2021-06-17 04:39:46,652 PID:38688 INFO logger.py info] Running RL loop for trial 0 session 1
[2021-06-17 04:39:46,652 PID:38689 INFO logger.py info][2021-06-17 04:39:46,652 PID:38690 INFO logger.py info] Running RL loop for trial 0 session 2
 Running RL loop for trial 0 session 3
[2021-06-17 04:39:46,652 PID:38687 INFO logger.py info] Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_043903',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/graph/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'info_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/info/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'log_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/log/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/model/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'prepath': 'data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903/Dueling_DDQN_PrioritizedReplay_t0_s0',
 'random_seed': 1623893945,
 'resume': False,
 'rigorous_eval': 0,
 'session': 0,
 'trial': 0}
- index = 0
- agent = <slm_lab.agent.Agent object at 0x7f3658600588>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7f36bc5bb0f0>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7f36bc5bb0f0>
[2021-06-17 04:39:46,652 PID:38687 INFO logger.py info] Running RL loop for trial 0 session 0
[2021-06-17 04:39:49,965 PID:38690 INFO __init__.py log_summary][2021-06-17 04:39:49,965 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan

[2021-06-17 04:39:49,965 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:39:49,965 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:42:15,252 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 10000  wall_t: 140  opt_step: 0  frame: 10000  fps: 71.4286  total_reward: 266.667  total_reward_ma: 266.667  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:42:15,234 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 10000  wall_t: 144  opt_step: 0  frame: 10000  fps: 69.4444  total_reward: 166.667  total_reward_ma: 166.667  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:42:15,158 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 10000  wall_t: 153  opt_step: 0  frame: 10000  fps: 65.3595  total_reward: 0  total_reward_ma: 0  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:42:15,782 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 10000  wall_t: 141  opt_step: 0  frame: 10000  fps: 70.922  total_reward: 100  total_reward_ma: 100  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:44:05,497 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 20000  wall_t: 299  opt_step: 7512  frame: 20000  fps: 66.8896  total_reward: 450  total_reward_ma: 308.333  loss: 0.00391564  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:44:05,540 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 20000  wall_t: 299  opt_step: 7512  frame: 20000  fps: 66.8896  total_reward: 200  total_reward_ma: 150  loss: 4.39438e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:44:05,707 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 20000  wall_t: 299  opt_step: 7512  frame: 20000  fps: 66.8896  total_reward: 533.333  total_reward_ma: 266.667  loss: 1.54886e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:44:05,847 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 20000  wall_t: 299  opt_step: 7512  frame: 20000  fps: 66.8896  total_reward: 620  total_reward_ma: 443.333  loss: 1.17695e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:44:15,649 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 443.333  strength: -199.667  max_strength: -23  final_strength: -23  sample_efficiency: 9.71202e-05  training_efficiency: 7.6672e-06  stability: 1
[2021-06-17 04:44:15,728 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 150  strength: -493  max_strength: -443  final_strength: -443  sample_efficiency: 7.75355e-05  training_efficiency: 5.98096e-05  stability: 1
[2021-06-17 04:44:15,856 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 266.667  strength: -376.333  max_strength: -109.667  final_strength: -109.667  sample_efficiency: 9.27148e-05  training_efficiency: 1.93962e-05  stability: 1
[2021-06-17 04:44:16,109 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 308.333  strength: -334.667  max_strength: -193  final_strength: -193  sample_efficiency: 8.55827e-05  training_efficiency: 3.83848e-05  stability: 1
[2021-06-17 04:45:03,682 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 30000  wall_t: 357  opt_step: 15024  frame: 30000  fps: 84.0336  total_reward: 366.667  total_reward_ma: 327.778  loss: 4.9711e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:45:03,685 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 30000  wall_t: 357  opt_step: 15024  frame: 30000  fps: 84.0336  total_reward: 800  total_reward_ma: 562.222  loss: 0.00385817  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:45:04,102 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 327.778  strength: -315.222  max_strength: -193  final_strength: -276.333  sample_efficiency: 7.03149e-05  training_efficiency: 4.66179e-05  stability: 0.875498
[2021-06-17 04:45:04,122 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 30000  wall_t: 357  opt_step: 15024  frame: 30000  fps: 84.0336  total_reward: 612.5  total_reward_ma: 381.944  loss: 4.18326e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:45:04,175 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 562.222  strength: -80.7778  max_strength: 157  final_strength: 157  sample_efficiency: 0.000138446  training_efficiency: -3.04877e-05  stability: 1
[2021-06-17 04:45:04,179 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 30000  wall_t: 357  opt_step: 15024  frame: 30000  fps: 84.0336  total_reward: 937.5  total_reward_ma: 412.5  loss: 3.35678e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:45:04,601 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 412.5  strength: -230.5  max_strength: 294.5  final_strength: 294.5  sample_efficiency: 9.63606e-05  training_efficiency: 5.69347e-05  stability: 1
[2021-06-17 04:45:04,630 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 381.944  strength: -261.056  max_strength: -30.5  final_strength: -30.5  sample_efficiency: 9.04022e-05  training_efficiency: 2.1233e-05  stability: 1
[2021-06-17 04:45:50,461 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 40000  wall_t: 404  opt_step: 22536  frame: 40000  fps: 99.0099  total_reward: 650  total_reward_ma: 584.167  loss: 0.00391759  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:45:50,702 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 584.167  strength: -58.8333  max_strength: 157  final_strength: 7  sample_efficiency: 0.00014182  training_efficiency: -3.27144e-05  stability: 0.381018
[2021-06-17 04:45:51,437 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 40000  wall_t: 405  opt_step: 22536  frame: 40000  fps: 98.7654  total_reward: 814.286  total_reward_ma: 449.405  loss: 5.02932e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:45:51,615 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 449.405  strength: -193.595  max_strength: 171.286  final_strength: 171.286  sample_efficiency: 8.03381e-05  training_efficiency: 4.71144e-05  stability: 0.911879
[2021-06-17 04:45:51,813 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 40000  wall_t: 405  opt_step: 22536  frame: 40000  fps: 98.7654  total_reward: 887.5  total_reward_ma: 508.333  loss: 1.661e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:45:52,054 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 40000  wall_t: 405  opt_step: 22536  frame: 40000  fps: 98.7654  total_reward: 937.5  total_reward_ma: 543.75  loss: 0.0038268  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:45:52,211 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 508.333  strength: -134.667  max_strength: 244.5  final_strength: 244.5  sample_efficiency: 0.000120088  training_efficiency: 1.07295e-05  stability: 1
[2021-06-17 04:45:52,441 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 543.75  strength: -99.25  max_strength: 294.5  final_strength: 294.5  sample_efficiency: 0.000149297  training_efficiency: 6.62528e-05  stability: 1
[2021-06-17 04:46:38,172 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 50000  wall_t: 452  opt_step: 30048  frame: 50000  fps: 110.619  total_reward: 787.5  total_reward_ma: 624.833  loss: 1.95603e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:46:38,410 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 624.833  strength: -18.1667  max_strength: 157  final_strength: 144.5  sample_efficiency: 0.000335615  training_efficiency: -0.0001377  stability: 0.362606
[2021-06-17 04:46:38,948 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 50000  wall_t: 452  opt_step: 30048  frame: 50000  fps: 110.619  total_reward: 837.5  total_reward_ma: 527.024  loss: 1.45466e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:46:39,210 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 527.024  strength: -115.976  max_strength: 194.5  final_strength: 194.5  sample_efficiency: 0.000100576  training_efficiency: 5.17546e-05  stability: 0.892387
[2021-06-17 04:46:39,617 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 50000  wall_t: 453  opt_step: 30048  frame: 50000  fps: 110.375  total_reward: 550  total_reward_ma: 516.667  loss: 1.66191e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:46:39,669 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 50000  wall_t: 453  opt_step: 30048  frame: 50000  fps: 110.375  total_reward: 787.5  total_reward_ma: 592.5  loss: 0.00389023  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:46:39,980 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 516.667  strength: -126.333  max_strength: 244.5  final_strength: -93  sample_efficiency: 0.000105352  training_efficiency: 1.40496e-05  stability: 0.373453
[2021-06-17 04:46:39,993 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 592.5  strength: -50.5  max_strength: 294.5  final_strength: 144.5  sample_efficiency: 0.00022329  training_efficiency: 8.51223e-05  stability: 0.622166
[2021-06-17 04:47:25,168 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 60000  wall_t: 499  opt_step: 37560  frame: 60000  fps: 120.24  total_reward: 775  total_reward_ma: 649.861  loss: 4.81343e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:47:25,323 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 649.861  strength: 6.86111  max_strength: 157  final_strength: 132  sample_efficiency: -0.000687085  training_efficiency: 0.000389202  stability: -0.788991
[2021-06-17 04:47:26,162 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 60000  wall_t: 499  opt_step: 37560  frame: 60000  fps: 120.24  total_reward: 975  total_reward_ma: 601.687  loss: 0.00386722  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:47:26,427 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 601.687  strength: -41.3135  max_strength: 332  final_strength: 332  sample_efficiency: 0.000212961  training_efficiency: 8.54132e-05  stability: 0.856292
[2021-06-17 04:47:26,762 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 60000  wall_t: 500  opt_step: 37560  frame: 60000  fps: 120  total_reward: 700  total_reward_ma: 610.417  loss: 0.0038535  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:47:26,830 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 60000  wall_t: 500  opt_step: 37560  frame: 60000  fps: 120  total_reward: 512.5  total_reward_ma: 515.972  loss: 0.0038205  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:47:27,187 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 515.972  strength: -127.028  max_strength: 244.5  final_strength: -130.5  sample_efficiency: 9.01673e-05  training_efficiency: 1.62027e-05  stability: 0.406332
[2021-06-17 04:47:27,233 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 610.417  strength: -32.5833  max_strength: 294.5  final_strength: 57  sample_efficiency: 0.000283534  training_efficiency: 0.000102178  stability: 0.0594059
[2021-06-17 04:48:11,784 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 70000  wall_t: 545  opt_step: 45072  frame: 70000  fps: 128.44  total_reward: 1012.5  total_reward_ma: 701.667  loss: 2.20691e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:48:12,094 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 701.667  strength: 58.6667  max_strength: 369.5  final_strength: 369.5  sample_efficiency: -5.60222e-05  training_efficiency: 5.89776e-05  stability: -2.94737
[2021-06-17 04:48:13,490 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 70000  wall_t: 547  opt_step: 45072  frame: 70000  fps: 127.971  total_reward: 1712.5  total_reward_ma: 760.374  loss: 3.26938e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:48:13,766 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 760.374  strength: 117.374  max_strength: 1069.5  final_strength: 1069.5  sample_efficiency: -4.56542e-05  training_efficiency: 3.1114e-06  stability: 0.663817
[2021-06-17 04:48:13,896 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 70000  wall_t: 547  opt_step: 45072  frame: 70000  fps: 127.971  total_reward: 625  total_reward_ma: 531.548  loss: 7.48039e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:48:14,062 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 531.548  strength: -111.452  max_strength: 244.5  final_strength: -18  sample_efficiency: 8.84166e-05  training_efficiency: 1.63407e-05  stability: 0.507982
[2021-06-17 04:48:14,503 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 70000  wall_t: 548  opt_step: 45072  frame: 70000  fps: 127.737  total_reward: 537.5  total_reward_ma: 600  loss: 0.00390715  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:48:14,818 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 600  strength: -43  max_strength: 294.5  final_strength: -105.5  sample_efficiency: 0.000189163  training_efficiency: 7.41412e-05  stability: -1.04604
[2021-06-17 04:48:59,141 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 80000  wall_t: 592  opt_step: 52584  frame: 80000  fps: 135.135  total_reward: 1262.5  total_reward_ma: 771.771  loss: 0.00389714  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:48:59,536 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 771.771  strength: 128.771  max_strength: 619.5  final_strength: 619.5  sample_efficiency: -1.48157e-05  training_efficiency: 3.49471e-05  stability: 0.604302
[2021-06-17 04:49:00,665 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 80000  wall_t: 594  opt_step: 52584  frame: 80000  fps: 134.68  total_reward: 1700  total_reward_ma: 877.827  loss: 0.0039151  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:49:00,929 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 80000  wall_t: 594  opt_step: 52584  frame: 80000  fps: 134.68  total_reward: 900  total_reward_ma: 577.604  loss: 1.02397e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:49:01,141 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 877.827  strength: 234.827  max_strength: 1069.5  final_strength: 1057  sample_efficiency: -1.29339e-05  training_efficiency: 1.20608e-05  stability: 0.88336
[2021-06-17 04:49:01,446 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 577.604  strength: -65.3958  max_strength: 257  final_strength: 257  sample_efficiency: 0.00012571  training_efficiency: 1.50259e-05  stability: 0.519333
[2021-06-17 04:49:01,705 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 80000  wall_t: 595  opt_step: 52584  frame: 80000  fps: 134.454  total_reward: 912.5  total_reward_ma: 639.062  loss: 3.64763e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:49:02,126 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 639.062  strength: -3.9375  max_strength: 294.5  final_strength: 269.5  sample_efficiency: 0.00170061  training_efficiency: 0.000545758  stability: -0.328904
[2021-06-17 04:49:46,634 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 90000  wall_t: 640  opt_step: 60096  frame: 90000  fps: 140.625  total_reward: 975  total_reward_ma: 794.352  loss: 3.26505e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:49:46,829 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 794.352  strength: 151.352  max_strength: 619.5  final_strength: 332  sample_efficiency: -8.49661e-06  training_efficiency: 3.04851e-05  stability: 0.563177
[2021-06-17 04:49:47,840 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 90000  wall_t: 641  opt_step: 60096  frame: 90000  fps: 140.406  total_reward: 550  total_reward_ma: 841.402  loss: 2.94845e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:49:47,980 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 841.402  strength: 198.402  max_strength: 1069.5  final_strength: -93  sample_efficiency: -1.41862e-05  training_efficiency: 1.18223e-05  stability: 0.336836
[2021-06-17 04:49:48,449 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 90000  wall_t: 642  opt_step: 60096  frame: 90000  fps: 140.187  total_reward: 775  total_reward_ma: 599.537  loss: 2.46419e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:49:48,683 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 599.537  strength: -43.463  max_strength: 257  final_strength: 132  sample_efficiency: 0.000164381  training_efficiency: 1.44813e-05  stability: 0.0442817
[2021-06-17 04:49:49,232 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 90000  wall_t: 643  opt_step: 60096  frame: 90000  fps: 139.969  total_reward: 500  total_reward_ma: 623.611  loss: 0.00761274  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:49:49,332 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 623.611  strength: -19.3889  max_strength: 294.5  final_strength: -143  sample_efficiency: 0.000316092  training_efficiency: 0.000112154  stability: -24.7937
[2021-06-17 04:50:34,124 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 100000  wall_t: 687  opt_step: 67608  frame: 100000  fps: 145.56  total_reward: 387.5  total_reward_ma: 753.667  loss: 2.7946e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:50:34,273 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 753.667  strength: 110.667  max_strength: 619.5  final_strength: -255.5  sample_efficiency: -1.2767e-05  training_efficiency: 3.41084e-05  stability: 0.238346
[2021-06-17 04:50:35,695 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 100000  wall_t: 689  opt_step: 67608  frame: 100000  fps: 145.138  total_reward: 450  total_reward_ma: 802.262  loss: 4.58358e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:50:35,783 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 100000  wall_t: 689  opt_step: 67608  frame: 100000  fps: 145.138  total_reward: 537.5  total_reward_ma: 593.333  loss: 7.47365e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:50:35,944 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 802.262  strength: 159.262  max_strength: 1069.5  final_strength: -193  sample_efficiency: -1.71172e-05  training_efficiency: 1.14625e-05  stability: 0.246293
[2021-06-17 04:50:35,971 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 593.333  strength: -49.6667  max_strength: 257  final_strength: -105.5  sample_efficiency: 0.000131588  training_efficiency: 1.45471e-05  stability: -0.885386
[2021-06-17 04:50:36,775 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 100000  wall_t: 690  opt_step: 67608  frame: 100000  fps: 144.928  total_reward: 887.5  total_reward_ma: 650  loss: 2.40974e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:50:37,020 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 650  strength: 7  max_strength: 294.5  final_strength: 244.5  sample_efficiency: -0.000753044  training_efficiency: -0.000227921  stability: -3.65616
[2021-06-17 04:51:21,440 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 110000  wall_t: 735  opt_step: 75120  frame: 110000  fps: 149.66  total_reward: 412.5  total_reward_ma: 722.651  loss: 8.01212e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:51:21,608 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 722.651  strength: 79.6515  max_strength: 619.5  final_strength: -230.5  sample_efficiency: -1.85173e-05  training_efficiency: 3.95795e-05  stability: 0.0624999
[2021-06-17 04:51:22,759 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 110000  wall_t: 736  opt_step: 75120  frame: 110000  fps: 149.457  total_reward: 750  total_reward_ma: 607.576  loss: 5.38397e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:51:23,003 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 607.576  strength: -35.4242  max_strength: 257  final_strength: 107  sample_efficiency: 0.000165225  training_efficiency: 1.48862e-05  stability: -0.484899
[2021-06-17 04:51:23,016 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 110000  wall_t: 736  opt_step: 75120  frame: 110000  fps: 149.457  total_reward: 762.5  total_reward_ma: 798.647  loss: 2.00343e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:51:23,131 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 798.647  strength: 155.647  max_strength: 1069.5  final_strength: 119.5  sample_efficiency: -1.5288e-05  training_efficiency: 1.15916e-05  stability: 0.154956
[2021-06-17 04:51:24,537 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 110000  wall_t: 738  opt_step: 75120  frame: 110000  fps: 149.051  total_reward: 675  total_reward_ma: 652.273  loss: 1.727e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:51:24,700 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 652.273  strength: 9.27273  max_strength: 294.5  final_strength: 32  sample_efficiency: -0.000513943  training_efficiency: -0.00015224  stability: -13.6429
[2021-06-17 04:52:08,631 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 120000  wall_t: 782  opt_step: 82632  frame: 120000  fps: 153.453  total_reward: 400  total_reward_ma: 695.764  loss: 1.35283e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:52:08,734 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 695.764  strength: 52.7639  max_strength: 619.5  final_strength: -243  sample_efficiency: -2.88222e-05  training_efficiency: 5.0125e-05  stability: -0.198402
[2021-06-17 04:52:09,967 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 120000  wall_t: 783  opt_step: 82632  frame: 120000  fps: 153.257  total_reward: 837.5  total_reward_ma: 801.885  loss: 2.23349e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:52:10,021 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 120000  wall_t: 783  opt_step: 82632  frame: 120000  fps: 153.257  total_reward: 837.5  total_reward_ma: 626.736  loss: 4.21924e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:52:10,188 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 801.885  strength: 158.885  max_strength: 1069.5  final_strength: 194.5  sample_efficiency: -1.28783e-05  training_efficiency: 1.16436e-05  stability: 0.213937
[2021-06-17 04:52:10,281 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 626.736  strength: -16.2639  max_strength: 257  final_strength: 194.5  sample_efficiency: 0.000321581  training_efficiency: 1.76611e-05  stability: -0.892643
[2021-06-17 04:52:12,048 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 120000  wall_t: 785  opt_step: 82632  frame: 120000  fps: 152.866  total_reward: 425  total_reward_ma: 633.333  loss: 5.50925e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:52:12,141 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 633.333  strength: -9.66667  max_strength: 294.5  final_strength: -218  sample_efficiency: 0.000467576  training_efficiency: 0.000156609  stability: -11.5
[2021-06-17 04:52:55,510 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 130000  wall_t: 829  opt_step: 90144  frame: 130000  fps: 156.815  total_reward: 675  total_reward_ma: 694.167  loss: 1.89273e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:52:55,618 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 694.167  strength: 51.1667  max_strength: 619.5  final_strength: 32  sample_efficiency: -2.70655e-05  training_efficiency: 4.82472e-05  stability: -0.658331
[2021-06-17 04:52:57,543 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 130000  wall_t: 831  opt_step: 90144  frame: 130000  fps: 156.438  total_reward: 662.5  total_reward_ma: 791.163  loss: 2.26354e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:52:57,630 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 791.163  strength: 148.163  max_strength: 1069.5  final_strength: 19.5  sample_efficiency: -1.26701e-05  training_efficiency: 1.1638e-05  stability: 0.20234
[2021-06-17 04:52:57,836 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 130000  wall_t: 831  opt_step: 90144  frame: 130000  fps: 156.438  total_reward: 1012.5  total_reward_ma: 656.41  loss: 3.36572e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:52:58,043 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 656.41  strength: 13.4103  max_strength: 369.5  final_strength: 369.5  sample_efficiency: -0.000343707  training_efficiency: 3.74075e-06  stability: -2.77882
[2021-06-17 04:52:59,764 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 130000  wall_t: 833  opt_step: 90144  frame: 130000  fps: 156.062  total_reward: 487.5  total_reward_ma: 622.115  loss: 5.5627e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:52:59,884 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 622.115  strength: -20.8846  max_strength: 294.5  final_strength: -155.5  sample_efficiency: 0.000204181  training_efficiency: 7.3266e-05  stability: -9.99138
[2021-06-17 04:53:43,157 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 140000  wall_t: 876  opt_step: 97656  frame: 140000  fps: 159.817  total_reward: 562.5  total_reward_ma: 684.762  loss: 3.53751e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:53:43,256 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 684.762  strength: 41.7619  max_strength: 619.5  final_strength: -80.5  sample_efficiency: -3.17755e-05  training_efficiency: 5.34803e-05  stability: -0.747682
[2021-06-17 04:53:44,973 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 140000  wall_t: 878  opt_step: 97656  frame: 140000  fps: 159.453  total_reward: 675  total_reward_ma: 657.738  loss: 1.85039e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:53:45,210 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 657.738  strength: 14.7381  max_strength: 369.5  final_strength: 32  sample_efficiency: -0.000289294  training_efficiency: 4.74872e-06  stability: -5.16635
[2021-06-17 04:53:45,211 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 140000  wall_t: 879  opt_step: 97656  frame: 140000  fps: 159.272  total_reward: 525  total_reward_ma: 772.151  loss: 7.36411e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:53:45,309 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 772.151  strength: 129.151  max_strength: 1069.5  final_strength: -118  sample_efficiency: -1.39631e-05  training_efficiency: 1.17293e-05  stability: 0.139029
[2021-06-17 04:53:47,267 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 140000  wall_t: 881  opt_step: 97656  frame: 140000  fps: 158.91  total_reward: 550  total_reward_ma: 616.964  loss: 4.42955e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:53:47,357 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 616.964  strength: -26.0357  max_strength: 294.5  final_strength: -93  sample_efficiency: 0.000153908  training_efficiency: 5.71853e-05  stability: -3.69613
[2021-06-17 04:54:30,395 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 150000  wall_t: 924  opt_step: 105168  frame: 150000  fps: 162.338  total_reward: 700  total_reward_ma: 685.778  loss: 6.2575e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:54:30,497 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 685.778  strength: 42.7778  max_strength: 619.5  final_strength: 57  sample_efficiency: -2.83607e-05  training_efficiency: 4.95742e-05  stability: -0.988313
[2021-06-17 04:54:32,194 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 150000  wall_t: 926  opt_step: 105168  frame: 150000  fps: 161.987  total_reward: 400  total_reward_ma: 747.341  loss: 1.10344e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:54:32,291 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 747.341  strength: 104.341  max_strength: 1069.5  final_strength: -243  sample_efficiency: -1.7166e-05  training_efficiency: 1.20741e-05  stability: 0.013708
[2021-06-17 04:54:32,368 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 150000  wall_t: 926  opt_step: 105168  frame: 150000  fps: 161.987  total_reward: 487.5  total_reward_ma: 646.389  loss: 0.00395274  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:54:32,458 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 646.389  strength: 3.38889  max_strength: 369.5  final_strength: -155.5  sample_efficiency: -0.00119464  training_efficiency: -9.81183e-06  stability: -5.11874
[2021-06-17 04:54:34,822 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 150000  wall_t: 928  opt_step: 105168  frame: 150000  fps: 161.638  total_reward: 537.5  total_reward_ma: 611.667  loss: 0.00391043  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:54:34,920 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 611.667  strength: -31.3333  max_strength: 294.5  final_strength: -105.5  sample_efficiency: 0.000120857  training_efficiency: 4.64834e-05  stability: -2.53224
[2021-06-17 04:55:17,440 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 160000  wall_t: 971  opt_step: 112680  frame: 160000  fps: 164.779  total_reward: 762.5  total_reward_ma: 690.573  loss: 1.04665e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:55:17,557 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 690.573  strength: 47.5729  max_strength: 619.5  final_strength: 119.5  sample_efficiency: -2.29269e-05  training_efficiency: 4.31846e-05  stability: -0.811688
[2021-06-17 04:55:19,150 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 160000  wall_t: 972  opt_step: 112680  frame: 160000  fps: 164.609  total_reward: 525  total_reward_ma: 638.802  loss: 7.97009e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:55:19,243 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 638.802  strength: -4.19792  max_strength: 369.5  final_strength: -118  sample_efficiency: 0.000915114  training_efficiency: 2.30171e-05  stability: -23.8361
[2021-06-17 04:55:19,261 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 160000  wall_t: 973  opt_step: 112680  frame: 160000  fps: 164.44  total_reward: 462.5  total_reward_ma: 729.539  loss: 0.00419605  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:55:19,347 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 729.539  strength: 86.5387  max_strength: 1069.5  final_strength: -180.5  sample_efficiency: -2.02186e-05  training_efficiency: 1.24911e-05  stability: -0.139424
[2021-06-17 04:55:22,456 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 160000  wall_t: 976  opt_step: 112680  frame: 160000  fps: 163.934  total_reward: 312.5  total_reward_ma: 592.969  loss: 0.000654877  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:55:22,601 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 592.969  strength: -50.0312  max_strength: 294.5  final_strength: -330.5  sample_efficiency: 7.35394e-05  training_efficiency: 3.0956e-05  stability: -2.21809
[2021-06-17 04:56:04,995 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 170000  wall_t: 1018  opt_step: 120192  frame: 170000  fps: 166.994  total_reward: 900  total_reward_ma: 702.892  loss: 1.58869e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:56:05,157 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 702.892  strength: 59.8922  max_strength: 619.5  final_strength: 257  sample_efficiency: -1.5655e-05  training_efficiency: 3.43843e-05  stability: -0.527261
[2021-06-17 04:56:06,243 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 170000  wall_t: 1020  opt_step: 120192  frame: 170000  fps: 166.667  total_reward: 437.5  total_reward_ma: 626.961  loss: 0.00386851  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:56:06,332 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 626.961  strength: -16.0392  max_strength: 369.5  final_strength: -205.5  sample_efficiency: 0.000229856  training_efficiency: 1.19404e-05  stability: -19.0992
[2021-06-17 04:56:06,823 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 170000  wall_t: 1020  opt_step: 120192  frame: 170000  fps: 166.667  total_reward: 425  total_reward_ma: 711.625  loss: 5.23015e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:56:06,919 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 711.625  strength: 68.6246  max_strength: 1069.5  final_strength: -218  sample_efficiency: -2.50959e-05  training_efficiency: 1.32706e-05  stability: -0.315043
[2021-06-17 04:56:09,835 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 170000  wall_t: 1023  opt_step: 120192  frame: 170000  fps: 166.178  total_reward: 512.5  total_reward_ma: 588.235  loss: 8.43559e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:56:09,945 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 588.235  strength: -54.7647  max_strength: 294.5  final_strength: -130.5  sample_efficiency: 6.40558e-05  training_efficiency: 2.77831e-05  stability: -0.889444
[2021-06-17 04:56:52,330 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 180000  wall_t: 1066  opt_step: 127704  frame: 180000  fps: 168.856  total_reward: 737.5  total_reward_ma: 704.815  loss: 1.22378e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:56:52,446 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 704.815  strength: 61.8148  max_strength: 619.5  final_strength: 94.5  sample_efficiency: -1.38536e-05  training_efficiency: 3.2129e-05  stability: -0.301359
[2021-06-17 04:56:52,807 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 180000  wall_t: 1066  opt_step: 127704  frame: 180000  fps: 168.856  total_reward: 425  total_reward_ma: 615.741  loss: 0.00025148  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:56:52,858 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 615.741  strength: -27.2593  max_strength: 369.5  final_strength: -218  sample_efficiency: 0.000130201  training_efficiency: 1.01144e-05  stability: -3.99694
[2021-06-17 04:56:53,652 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 180000  wall_t: 1067  opt_step: 127704  frame: 180000  fps: 168.697  total_reward: 562.5  total_reward_ma: 703.34  loss: 0.0116762  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:56:53,746 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 703.34  strength: 60.3399  max_strength: 1069.5  final_strength: -80.5  sample_efficiency: -2.73677e-05  training_efficiency: 1.36738e-05  stability: -0.560778
[2021-06-17 04:56:56,873 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 180000  wall_t: 1070  opt_step: 127704  frame: 180000  fps: 168.224  total_reward: 875  total_reward_ma: 604.167  loss: 5.76261e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:56:56,971 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 604.167  strength: -38.8333  max_strength: 294.5  final_strength: 232  sample_efficiency: 8.34722e-05  training_efficiency: 3.44053e-05  stability: -0.624597
[2021-06-17 04:57:39,221 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 190000  wall_t: 1113  opt_step: 135216  frame: 190000  fps: 170.71  total_reward: 950  total_reward_ma: 717.719  loss: 6.68195e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:57:39,332 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 717.719  strength: 74.7193  max_strength: 619.5  final_strength: 307  sample_efficiency: -9.71965e-06  training_efficiency: 2.67805e-05  stability: -0.190833
[2021-06-17 04:57:39,680 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 190000  wall_t: 1113  opt_step: 135216  frame: 190000  fps: 170.71  total_reward: 962.5  total_reward_ma: 633.991  loss: 3.85541e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:57:39,795 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 633.991  strength: -9.00877  max_strength: 369.5  final_strength: 319.5  sample_efficiency: 0.000363409  training_efficiency: 1.51895e-05  stability: -1.77683
[2021-06-17 04:57:40,409 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 190000  wall_t: 1114  opt_step: 135216  frame: 190000  fps: 170.557  total_reward: 512.5  total_reward_ma: 693.296  loss: 4.44773e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:57:40,498 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 693.296  strength: 50.2957  max_strength: 1069.5  final_strength: -130.5  sample_efficiency: -3.18238e-05  training_efficiency: 1.45311e-05  stability: -0.722494
[2021-06-17 04:57:43,813 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 190000  wall_t: 1117  opt_step: 135216  frame: 190000  fps: 170.098  total_reward: 625  total_reward_ma: 605.263  loss: 1.23063e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:57:43,892 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 605.263  strength: -37.7368  max_strength: 294.5  final_strength: -18  sample_efficiency: 8.15087e-05  training_efficiency: 3.37273e-05  stability: -1.52146
[2021-06-17 04:58:26,366 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 200000  wall_t: 1160  opt_step: 142728  frame: 200000  fps: 172.414  total_reward: 950  total_reward_ma: 729.333  loss: 1.4725e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:58:26,466 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 729.333  strength: 86.3333  max_strength: 619.5  final_strength: 307  sample_efficiency: -7.10251e-06  training_efficiency: 2.32646e-05  stability: 0.0666823
[2021-06-17 04:58:27,336 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 200000  wall_t: 1161  opt_step: 142728  frame: 200000  fps: 172.265  total_reward: 675  total_reward_ma: 636.042  loss: 1.80318e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:58:27,410 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 636.042  strength: -6.95833  max_strength: 369.5  final_strength: 32  sample_efficiency: 0.000445822  training_efficiency: 1.70711e-05  stability: -8.63973
[2021-06-17 04:58:27,671 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 200000  wall_t: 1161  opt_step: 142728  frame: 200000  fps: 172.265  total_reward: 462.5  total_reward_ma: 681.756  loss: 0.003849  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:58:27,750 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 681.756  strength: 38.7559  max_strength: 1069.5  final_strength: -180.5  sample_efficiency: -4.03989e-05  training_efficiency: 1.62834e-05  stability: -1.01004
[2021-06-17 04:58:31,076 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 200000  wall_t: 1164  opt_step: 142728  frame: 200000  fps: 171.821  total_reward: 637.5  total_reward_ma: 606.875  loss: 8.06267e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:58:31,174 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 606.875  strength: -36.125  max_strength: 294.5  final_strength: -5.5  sample_efficiency: 8.09263e-05  training_efficiency: 3.35239e-05  stability: -1.45816
[2021-06-17 04:59:13,850 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 210000  wall_t: 1207  opt_step: 150240  frame: 210000  fps: 173.985  total_reward: 862.5  total_reward_ma: 735.675  loss: 0.00383712  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:59:13,942 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 735.675  strength: 92.6746  max_strength: 619.5  final_strength: 219.5  sample_efficiency: -5.76437e-06  training_efficiency: 2.13914e-05  stability: 0.18195
[2021-06-17 04:59:14,499 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 210000  wall_t: 1208  opt_step: 150240  frame: 210000  fps: 173.841  total_reward: 787.5  total_reward_ma: 643.254  loss: 5.00665e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:59:14,579 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 643.254  strength: 0.253967  max_strength: 369.5  final_strength: 144.5  sample_efficiency: -0.0115042  training_efficiency: -0.000265114  stability: -10.8563
[2021-06-17 04:59:14,815 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 210000  wall_t: 1208  opt_step: 150240  frame: 210000  fps: 173.841  total_reward: 312.5  total_reward_ma: 664.172  loss: 7.1617e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:59:14,946 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 664.172  strength: 21.1723  max_strength: 1069.5  final_strength: -330.5  sample_efficiency: -7.39684e-05  training_efficiency: 2.34398e-05  stability: -1.67163
[2021-06-17 04:59:18,338 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 210000  wall_t: 1212  opt_step: 150240  frame: 210000  fps: 173.267  total_reward: 475  total_reward_ma: 600.595  loss: 3.77844e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 04:59:18,426 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 600.595  strength: -42.4048  max_strength: 294.5  final_strength: -168  sample_efficiency: 6.65573e-05  training_efficiency: 2.8455e-05  stability: -1.66436
[2021-06-17 05:00:01,192 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 220000  wall_t: 1255  opt_step: 157752  frame: 220000  fps: 175.299  total_reward: 800  total_reward_ma: 738.598  loss: 5.38125e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:00:01,300 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 738.598  strength: 95.5985  max_strength: 619.5  final_strength: 157  sample_efficiency: -4.99475e-06  training_efficiency: 2.02678e-05  stability: 0.2421
[2021-06-17 05:00:01,425 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 220000  wall_t: 1255  opt_step: 157752  frame: 220000  fps: 175.299  total_reward: 562.5  total_reward_ma: 639.583  loss: 2.57486e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:00:01,491 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 639.583  strength: -3.41667  max_strength: 369.5  final_strength: -80.5  sample_efficiency: 0.000821127  training_efficiency: 2.55995e-05  stability: -350.564
[2021-06-17 05:00:01,989 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 220000  wall_t: 1255  opt_step: 157752  frame: 220000  fps: 175.299  total_reward: 312.5  total_reward_ma: 648.187  loss: 0.00671617  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:00:02,093 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 648.187  strength: 5.18722  max_strength: 1069.5  final_strength: -330.5  sample_efficiency: -0.000301352  training_efficiency: 7.29651e-05  stability: -3.65755
[2021-06-17 05:00:05,776 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 220000  wall_t: 1259  opt_step: 157752  frame: 220000  fps: 174.742  total_reward: 450  total_reward_ma: 593.75  loss: 2.54189e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:00:05,873 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 593.75  strength: -49.25  max_strength: 294.5  final_strength: -193  sample_efficiency: 5.55113e-05  training_efficiency: 2.45156e-05  stability: -1.18978
[2021-06-17 05:00:48,286 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 230000  wall_t: 1302  opt_step: 165264  frame: 230000  fps: 176.651  total_reward: 875  total_reward_ma: 744.529  loss: 8.93227e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:00:48,354 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 744.529  strength: 101.529  max_strength: 619.5  final_strength: 232  sample_efficiency: -4.06656e-06  training_efficiency: 1.88553e-05  stability: 0.298677
[2021-06-17 05:00:48,593 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 230000  wall_t: 1302  opt_step: 165264  frame: 230000  fps: 176.651  total_reward: 525  total_reward_ma: 634.601  loss: 0.0038714  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:00:48,674 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 634.601  strength: -8.39855  max_strength: 369.5  final_strength: -118  sample_efficiency: 0.00032218  training_efficiency: 1.36578e-05  stability: -24.4435
[2021-06-17 05:00:49,283 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 230000  wall_t: 1303  opt_step: 165264  frame: 230000  fps: 176.516  total_reward: 425  total_reward_ma: 638.483  loss: 0.00416215  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:00:49,387 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 638.483  strength: -4.51656  max_strength: 1069.5  final_strength: -218  sample_efficiency: 0.000340176  training_efficiency: -6.74579e-05  stability: -17.1463
[2021-06-17 05:00:52,830 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 230000  wall_t: 1306  opt_step: 165264  frame: 230000  fps: 176.11  total_reward: 437.5  total_reward_ma: 586.957  loss: 0.00391714  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:00:52,929 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 586.957  strength: -56.0435  max_strength: 294.5  final_strength: -205.5  sample_efficiency: 4.73546e-05  training_efficiency: 2.15718e-05  stability: -0.81126
[2021-06-17 05:01:35,238 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 240000  wall_t: 1349  opt_step: 172776  frame: 240000  fps: 177.91  total_reward: 675  total_reward_ma: 741.632  loss: 1.46299e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:01:35,308 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 741.632  strength: 98.6319  max_strength: 619.5  final_strength: 32  sample_efficiency: -3.95526e-06  training_efficiency: 1.86787e-05  stability: 0.282706
[2021-06-17 05:01:35,980 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 240000  wall_t: 1349  opt_step: 172776  frame: 240000  fps: 177.91  total_reward: 712.5  total_reward_ma: 637.847  loss: 0.00390926  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:01:36,052 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 637.847  strength: -5.15278  max_strength: 369.5  final_strength: 69.5  sample_efficiency: 0.000500902  training_efficiency: 1.80807e-05  stability: -8.90078
[2021-06-17 05:01:36,520 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 240000  wall_t: 1350  opt_step: 172776  frame: 240000  fps: 177.778  total_reward: 1012.5  total_reward_ma: 654.067  loss: 6.70345e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:01:36,604 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 654.067  strength: 11.0675  max_strength: 1069.5  final_strength: 369.5  sample_efficiency: -0.000127243  training_efficiency: 3.44335e-05  stability: -18.9347
[2021-06-17 05:01:40,500 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 240000  wall_t: 1354  opt_step: 172776  frame: 240000  fps: 177.253  total_reward: 487.5  total_reward_ma: 582.812  loss: 2.23573e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:01:40,609 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 582.812  strength: -60.1875  max_strength: 294.5  final_strength: -155.5  sample_efficiency: 4.27054e-05  training_efficiency: 1.98727e-05  stability: -0.522498
[2021-06-17 05:02:22,421 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 250000  wall_t: 1396  opt_step: 180288  frame: 250000  fps: 179.083  total_reward: 700  total_reward_ma: 739.967  loss: 0.00386738  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:02:22,485 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 739.967  strength: 96.9667  max_strength: 619.5  final_strength: 57  sample_efficiency: -3.76821e-06  training_efficiency: 1.83699e-05  stability: 0.292403
[2021-06-17 05:02:23,134 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 250000  wall_t: 1396  opt_step: 180288  frame: 250000  fps: 179.083  total_reward: 600  total_reward_ma: 636.333  loss: 8.47577e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:02:23,200 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 636.333  strength: -6.66667  max_strength: 369.5  final_strength: -43  sample_efficiency: 0.000372701  training_efficiency: 1.4847e-05  stability: -15.3747
[2021-06-17 05:02:23,878 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 250000  wall_t: 1397  opt_step: 180288  frame: 250000  fps: 178.955  total_reward: 762.5  total_reward_ma: 658.405  loss: 0.00389281  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:02:23,963 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 658.405  strength: 15.4048  max_strength: 1069.5  final_strength: 119.5  sample_efficiency: -8.65194e-05  training_efficiency: 2.54701e-05  stability: -7.73745
[2021-06-17 05:02:28,073 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 250000  wall_t: 1401  opt_step: 180288  frame: 250000  fps: 178.444  total_reward: 625  total_reward_ma: 584.5  loss: 1.05275e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:02:28,186 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 584.5  strength: -58.5  max_strength: 294.5  final_strength: -18  sample_efficiency: 4.2229e-05  training_efficiency: 1.96964e-05  stability: -0.358602
[2021-06-17 05:03:10,002 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 260000  wall_t: 1443  opt_step: 187800  frame: 260000  fps: 180.18  total_reward: 550  total_reward_ma: 732.66  loss: 7.33672e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:03:10,074 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 732.66  strength: 89.6602  max_strength: 619.5  final_strength: -93  sample_efficiency: -4.07197e-06  training_efficiency: 1.88903e-05  stability: 0.247164
[2021-06-17 05:03:10,288 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 260000  wall_t: 1444  opt_step: 187800  frame: 260000  fps: 180.055  total_reward: 625  total_reward_ma: 635.897  loss: 0.00039479  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:03:10,339 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 635.897  strength: -7.10256  max_strength: 369.5  final_strength: -18  sample_efficiency: 0.000336748  training_efficiency: 1.39188e-05  stability: -11.15
[2021-06-17 05:03:10,835 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 260000  wall_t: 1444  opt_step: 187800  frame: 260000  fps: 180.055  total_reward: 762.5  total_reward_ma: 662.408  loss: 2.55788e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:03:10,910 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 662.408  strength: 19.4084  max_strength: 1069.5  final_strength: 119.5  sample_efficiency: -6.51197e-05  training_efficiency: 2.06995e-05  stability: -5.02628
[2021-06-17 05:03:15,667 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 260000  wall_t: 1449  opt_step: 187800  frame: 260000  fps: 179.434  total_reward: 425  total_reward_ma: 578.365  loss: 8.93498e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:03:15,735 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 578.365  strength: -64.6346  max_strength: 294.5  final_strength: -218  sample_efficiency: 3.72499e-05  training_efficiency: 1.7832e-05  stability: -0.478632
[2021-06-17 05:03:56,783 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 270000  wall_t: 1490  opt_step: 195312  frame: 270000  fps: 181.208  total_reward: 550  total_reward_ma: 725.895  loss: 1.01481e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:03:56,872 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 725.895  strength: 82.8951  max_strength: 619.5  final_strength: -93  sample_efficiency: -4.39507e-06  training_efficiency: 1.94625e-05  stability: 0.21713
[2021-06-17 05:03:57,563 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 270000  wall_t: 1491  opt_step: 195312  frame: 270000  fps: 181.087  total_reward: 475  total_reward_ma: 629.938  loss: 5.9389e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:03:57,625 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 629.938  strength: -13.0617  max_strength: 369.5  final_strength: -168  sample_efficiency: 0.000178095  training_efficiency: 9.72732e-06  stability: -10.778
[2021-06-17 05:03:58,107 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 270000  wall_t: 1491  opt_step: 195312  frame: 270000  fps: 181.087  total_reward: 925  total_reward_ma: 672.134  loss: 0.000127667  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:03:58,177 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 672.134  strength: 29.134  max_strength: 1069.5  final_strength: 282  sample_efficiency: -4.04468e-05  training_efficiency: 1.51143e-05  stability: -3.59918
[2021-06-17 05:04:02,843 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 270000  wall_t: 1496  opt_step: 195312  frame: 270000  fps: 180.481  total_reward: 825  total_reward_ma: 587.5  loss: 2.22729e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:04:02,922 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 587.5  strength: -55.5  max_strength: 294.5  final_strength: 182  sample_efficiency: 4.13242e-05  training_efficiency: 1.9376e-05  stability: -0.286819
[2021-06-17 05:04:43,670 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 280000  wall_t: 1537  opt_step: 202824  frame: 280000  fps: 182.173  total_reward: 687.5  total_reward_ma: 724.524  loss: 0.00385758  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:04:43,729 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 724.524  strength: 81.5238  max_strength: 619.5  final_strength: 44.5  sample_efficiency: -4.23976e-06  training_efficiency: 1.91792e-05  stability: 0.1846
[2021-06-17 05:04:44,494 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 280000  wall_t: 1538  opt_step: 202824  frame: 280000  fps: 182.055  total_reward: 837.5  total_reward_ma: 637.351  loss: 2.28799e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:04:44,578 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 280000  wall_t: 1538  opt_step: 202824  frame: 280000  fps: 182.055  total_reward: 837.5  total_reward_ma: 678.04  loss: 8.52833e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:04:44,626 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 637.351  strength: -5.64881  max_strength: 369.5  final_strength: 194.5  sample_efficiency: 0.00039271  training_efficiency: 1.56262e-05  stability: -5.1673
[2021-06-17 05:04:44,648 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 678.04  strength: 35.04  max_strength: 1069.5  final_strength: 194.5  sample_efficiency: -3.17205e-05  training_efficiency: 1.30954e-05  stability: -2.06163
[2021-06-17 05:04:49,812 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 280000  wall_t: 1543  opt_step: 202824  frame: 280000  fps: 181.465  total_reward: 887.5  total_reward_ma: 598.214  loss: 1.13695e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:04:49,914 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 598.214  strength: -44.7857  max_strength: 294.5  final_strength: 244.5  sample_efficiency: 4.86851e-05  training_efficiency: 2.21925e-05  stability: -0.44311
[2021-06-17 05:05:30,724 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 290000  wall_t: 1584  opt_step: 210336  frame: 290000  fps: 183.081  total_reward: 1100  total_reward_ma: 737.471  loss: 3.76454e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:05:30,792 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 737.471  strength: 94.4713  max_strength: 619.5  final_strength: 457  sample_efficiency: -2.95733e-06  training_efficiency: 1.6773e-05  stability: 0.200496
[2021-06-17 05:05:31,496 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 290000  wall_t: 1585  opt_step: 210336  frame: 290000  fps: 182.965  total_reward: 962.5  total_reward_ma: 687.849  loss: 5.01303e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:05:31,551 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 290000  wall_t: 1585  opt_step: 210336  frame: 290000  fps: 182.965  total_reward: 462.5  total_reward_ma: 631.322  loss: 4.90561e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:05:31,565 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 687.849  strength: 44.8489  max_strength: 1069.5  final_strength: 319.5  sample_efficiency: -2.30812e-05  training_efficiency: 1.10464e-05  stability: -1.45468
[2021-06-17 05:05:31,648 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 631.322  strength: -11.6782  max_strength: 369.5  final_strength: -180.5  sample_efficiency: 0.000185244  training_efficiency: 9.83177e-06  stability: -15.1222
[2021-06-17 05:05:37,908 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 290000  wall_t: 1591  opt_step: 210336  frame: 290000  fps: 182.275  total_reward: 862.5  total_reward_ma: 607.328  loss: 2.56159e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:05:37,986 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 607.328  strength: -35.6724  max_strength: 294.5  final_strength: 219.5  sample_efficiency: 5.82834e-05  training_efficiency: 2.58926e-05  stability: -0.744418
[2021-06-17 05:06:17,864 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 300000  wall_t: 1631  opt_step: 217848  frame: 300000  fps: 183.936  total_reward: 600  total_reward_ma: 732.889  loss: 3.02793e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:06:17,916 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 732.889  strength: 89.8889  max_strength: 619.5  final_strength: -43  sample_efficiency: -3.05764e-06  training_efficiency: 1.69673e-05  stability: 0.151357
[2021-06-17 05:06:18,705 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 300000  wall_t: 1632  opt_step: 217848  frame: 300000  fps: 183.824  total_reward: 987.5  total_reward_ma: 697.837  loss: 4.13493e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:06:18,757 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 300000  wall_t: 1632  opt_step: 217848  frame: 300000  fps: 183.824  total_reward: 337.5  total_reward_ma: 621.528  loss: 0.00398404  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:06:18,857 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 697.837  strength: 54.8373  max_strength: 1069.5  final_strength: 344.5  sample_efficiency: -1.75498e-05  training_efficiency: 9.69445e-06  stability: -0.851683
[2021-06-17 05:06:18,873 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 621.528  strength: -21.4722  max_strength: 369.5  final_strength: -305.5  sample_efficiency: 9.89719e-05  training_efficiency: 7.346e-06  stability: -6.89862
[2021-06-17 05:06:25,773 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 300000  wall_t: 1639  opt_step: 217848  frame: 300000  fps: 183.038  total_reward: 962.5  total_reward_ma: 619.167  loss: 9.69378e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:06:25,863 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 619.167  strength: -23.8333  max_strength: 319.5  final_strength: 319.5  sample_efficiency: 8.28381e-05  training_efficiency: 3.54115e-05  stability: -1.11455
[2021-06-17 05:07:04,769 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 310000  wall_t: 1678  opt_step: 225360  frame: 310000  fps: 184.744  total_reward: 800  total_reward_ma: 735.054  loss: 4.3691e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:07:04,820 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 735.054  strength: 92.0538  max_strength: 619.5  final_strength: 157  sample_efficiency: -2.71194e-06  training_efficiency: 1.62779e-05  stability: 0.137824
[2021-06-17 05:07:05,767 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 310000  wall_t: 1679  opt_step: 225360  frame: 310000  fps: 184.634  total_reward: 475  total_reward_ma: 616.801  loss: 0.00385119  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:07:05,842 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 616.801  strength: -26.1989  max_strength: 369.5  final_strength: -168  sample_efficiency: 7.91664e-05  training_efficiency: 6.74433e-06  stability: -3.15265
[2021-06-17 05:07:06,032 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 310000  wall_t: 1679  opt_step: 225360  frame: 310000  fps: 184.634  total_reward: 850  total_reward_ma: 702.746  loss: 1.80485e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:07:06,108 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 702.746  strength: 59.7458  max_strength: 1069.5  final_strength: 207  sample_efficiency: -1.52278e-05  training_efficiency: 9.1069e-06  stability: -0.547507
[2021-06-17 05:07:13,065 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 310000  wall_t: 1686  opt_step: 225360  frame: 310000  fps: 183.867  total_reward: 1212.5  total_reward_ma: 638.306  loss: 0.00389024  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:07:13,142 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 638.306  strength: -4.69355  max_strength: 569.5  final_strength: 569.5  sample_efficiency: 0.000394447  training_efficiency: 0.000156647  stability: -2.05944
[2021-06-17 05:07:52,345 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 320000  wall_t: 1726  opt_step: 232872  frame: 320000  fps: 185.4  total_reward: 1125  total_reward_ma: 747.24  loss: 1.90404e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:07:52,413 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 747.24  strength: 104.24  max_strength: 619.5  final_strength: 482  sample_efficiency: -1.86851e-06  training_efficiency: 1.45463e-05  stability: 0.185259
[2021-06-17 05:07:54,045 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 320000  wall_t: 1727  opt_step: 232872  frame: 320000  fps: 185.292  total_reward: 737.5  total_reward_ma: 620.573  loss: 0.000338276  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:07:54,105 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 620.573  strength: -22.4271  max_strength: 369.5  final_strength: 94.5  sample_efficiency: 8.91793e-05  training_efficiency: 7.06695e-06  stability: -2.29366
[2021-06-17 05:07:54,761 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 320000  wall_t: 1728  opt_step: 232872  frame: 320000  fps: 185.185  total_reward: 887.5  total_reward_ma: 708.519  loss: 5.07973e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:07:54,829 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 708.519  strength: 65.5193  max_strength: 1069.5  final_strength: 244.5  sample_efficiency: -1.30876e-05  training_efficiency: 8.54566e-06  stability: -0.374552
[2021-06-17 05:08:01,159 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 320000  wall_t: 1734  opt_step: 232872  frame: 320000  fps: 184.544  total_reward: 1575  total_reward_ma: 667.578  loss: 0.00393529  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:08:01,326 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 667.578  strength: 24.5781  max_strength: 932  final_strength: 932  sample_efficiency: -6.92684e-05  training_efficiency: -2.38906e-05  stability: -14.0344
[2021-06-17 05:08:39,184 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 330000  wall_t: 1773  opt_step: 240384  frame: 330000  fps: 186.125  total_reward: 1175  total_reward_ma: 760.202  loss: 6.65364e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:08:39,254 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 760.202  strength: 117.202  max_strength: 619.5  final_strength: 532  sample_efficiency: -1.19468e-06  training_efficiency: 1.31176e-05  stability: 0.302988
[2021-06-17 05:08:41,290 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 330000  wall_t: 1775  opt_step: 240384  frame: 330000  fps: 185.915  total_reward: 800  total_reward_ma: 626.01  loss: 0.00383723  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:08:41,410 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 626.01  strength: -16.9899  max_strength: 369.5  final_strength: 157  sample_efficiency: 0.000113303  training_efficiency: 7.88097e-06  stability: -2.72736
[2021-06-17 05:08:41,876 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 330000  wall_t: 1775  opt_step: 240384  frame: 330000  fps: 185.915  total_reward: 1350  total_reward_ma: 727.958  loss: 4.19745e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:08:41,950 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 727.958  strength: 84.9582  max_strength: 1069.5  final_strength: 707  sample_efficiency: -9.02308e-06  training_efficiency: 7.43971e-06  stability: -0.214257
[2021-06-17 05:08:48,622 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 330000  wall_t: 1782  opt_step: 240384  frame: 330000  fps: 185.185  total_reward: 1475  total_reward_ma: 692.045  loss: 0.00781175  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:08:48,741 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 692.045  strength: 49.0455  max_strength: 932  final_strength: 832  sample_efficiency: -3.21028e-05  training_efficiency: -9.47102e-06  stability: -1.90846
[2021-06-17 05:09:26,434 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 340000  wall_t: 1820  opt_step: 247896  frame: 340000  fps: 186.813  total_reward: 687.5  total_reward_ma: 758.064  loss: 6.10679e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:09:26,503 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 758.064  strength: 115.064  max_strength: 619.5  final_strength: 44.5  sample_efficiency: -1.14763e-06  training_efficiency: 1.30143e-05  stability: 0.272817
[2021-06-17 05:09:28,476 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 340000  wall_t: 1822  opt_step: 247896  frame: 340000  fps: 186.608  total_reward: 937.5  total_reward_ma: 635.172  loss: 0.00724661  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:09:28,544 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 635.172  strength: -7.82843  max_strength: 369.5  final_strength: 294.5  sample_efficiency: 0.000235413  training_efficiency: 1.21375e-05  stability: -3.77111
[2021-06-17 05:09:28,888 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 340000  wall_t: 1822  opt_step: 247896  frame: 340000  fps: 186.608  total_reward: 712.5  total_reward_ma: 727.503  loss: 0.000933681  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:09:28,955 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 727.503  strength: 84.5035  max_strength: 1069.5  final_strength: 69.5  sample_efficiency: -8.73366e-06  training_efficiency: 7.35733e-06  stability: -0.135437
[2021-06-17 05:09:35,924 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 340000  wall_t: 1829  opt_step: 247896  frame: 340000  fps: 185.894  total_reward: 1087.5  total_reward_ma: 703.676  loss: 5.05803e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:09:36,104 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 703.676  strength: 60.6765  max_strength: 932  final_strength: 444.5  sample_efficiency: -2.45521e-05  training_efficiency: -6.5612e-06  stability: -0.652765
[2021-06-17 05:10:13,527 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 350000  wall_t: 1867  opt_step: 255408  frame: 350000  fps: 187.467  total_reward: 1125  total_reward_ma: 768.548  loss: 4.40504e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:10:13,629 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 768.548  strength: 125.548  max_strength: 619.5  final_strength: 482  sample_efficiency: -7.08346e-07  training_efficiency: 1.20162e-05  stability: 0.281089
[2021-06-17 05:10:15,992 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 350000  wall_t: 1869  opt_step: 255408  frame: 350000  fps: 187.266  total_reward: 800  total_reward_ma: 639.881  loss: 3.79255e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:10:16,016 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 350000  wall_t: 1869  opt_step: 255408  frame: 350000  fps: 187.266  total_reward: 562.5  total_reward_ma: 722.789  loss: 0.0114586  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:10:16,140 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 639.881  strength: -3.11905  max_strength: 369.5  final_strength: 157  sample_efficiency: 0.000569867  training_efficiency: 2.39624e-05  stability: -9.56669
[2021-06-17 05:10:16,149 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 722.789  strength: 79.7891  max_strength: 1069.5  final_strength: -80.5  sample_efficiency: -9.06778e-06  training_efficiency: 7.45655e-06  stability: -0.160179
[2021-06-17 05:10:23,055 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 350000  wall_t: 1876  opt_step: 255408  frame: 350000  fps: 186.567  total_reward: 462.5  total_reward_ma: 696.786  loss: 2.52685e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:10:23,108 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 696.786  strength: 53.7857  max_strength: 932  final_strength: -180.5  sample_efficiency: -2.71802e-05  training_efficiency: -7.56572e-06  stability: -0.599612
[2021-06-17 05:11:01,513 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 360000  wall_t: 1915  opt_step: 262920  frame: 360000  fps: 187.99  total_reward: 1112.5  total_reward_ma: 778.102  loss: 0.00349443  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:11:01,581 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 778.102  strength: 135.102  max_strength: 619.5  final_strength: 469.5  sample_efficiency: -3.71823e-07  training_efficiency: 1.12234e-05  stability: 0.357102
[2021-06-17 05:11:03,397 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 360000  wall_t: 1917  opt_step: 262920  frame: 360000  fps: 187.793  total_reward: 725  total_reward_ma: 642.245  loss: 0.00772137  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:11:03,754 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 642.245  strength: -0.75463  max_strength: 369.5  final_strength: 82  sample_efficiency: 0.00228157  training_efficiency: 8.48102e-05  stability: -25.4504
[2021-06-17 05:11:04,090 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 360000  wall_t: 1917  opt_step: 262920  frame: 360000  fps: 187.793  total_reward: 825  total_reward_ma: 725.628  loss: 0.0076985  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:11:04,346 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 725.628  strength: 82.6283  max_strength: 1069.5  final_strength: 182  sample_efficiency: -8.34302e-06  training_efficiency: 7.23303e-06  stability: -0.193623
[2021-06-17 05:11:11,077 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 360000  wall_t: 1924  opt_step: 262920  frame: 360000  fps: 187.11  total_reward: 662.5  total_reward_ma: 695.833  loss: 1.16644e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:11:11,145 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 695.833  strength: 52.8333  max_strength: 932  final_strength: 19.5  sample_efficiency: -2.68731e-05  training_efficiency: -7.44916e-06  stability: -0.752988
[2021-06-17 05:11:49,240 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 370000  wall_t: 1963  opt_step: 270432  frame: 370000  fps: 188.487  total_reward: 1150  total_reward_ma: 788.153  loss: 6.86606e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:11:49,324 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 788.153  strength: 145.153  max_strength: 619.5  final_strength: 507  sample_efficiency: -8.15823e-08  training_efficiency: 1.0513e-05  stability: 0.419162
[2021-06-17 05:11:51,210 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 370000  wall_t: 1965  opt_step: 270432  frame: 370000  fps: 188.295  total_reward: 787.5  total_reward_ma: 646.171  loss: 4.84903e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:11:51,339 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 646.171  strength: 3.17117  max_strength: 369.5  final_strength: 144.5  sample_efficiency: -0.000524934  training_efficiency: -1.50825e-05  stability: -105.288
[2021-06-17 05:11:51,970 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 370000  wall_t: 1965  opt_step: 270432  frame: 370000  fps: 188.295  total_reward: 787.5  total_reward_ma: 727.301  loss: 5.24807e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:11:52,044 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 727.301  strength: 84.3005  max_strength: 1069.5  final_strength: 144.5  sample_efficiency: -7.8313e-06  training_efficiency: 7.06925e-06  stability: -0.133198
[2021-06-17 05:11:58,403 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 370000  wall_t: 1972  opt_step: 270432  frame: 370000  fps: 187.627  total_reward: 437.5  total_reward_ma: 688.851  loss: 1.62613e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:11:58,489 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 688.851  strength: 45.8514  max_strength: 932  final_strength: -205.5  sample_efficiency: -3.04556e-05  training_efficiency: -8.79941e-06  stability: -0.853312
[2021-06-17 05:12:35,931 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 380000  wall_t: 2009  opt_step: 277944  frame: 380000  fps: 189.149  total_reward: 662.5  total_reward_ma: 784.846  loss: 3.60413e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:12:35,996 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 784.846  strength: 141.846  max_strength: 619.5  final_strength: 19.5  sample_efficiency: -7.17669e-08  training_efficiency: 1.0488e-05  stability: 0.383224
[2021-06-17 05:12:38,038 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 380000  wall_t: 2011  opt_step: 277944  frame: 380000  fps: 188.961  total_reward: 1500  total_reward_ma: 668.64  loss: 0.00385662  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:12:38,194 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 668.64  strength: 25.6404  max_strength: 857  final_strength: 857  sample_efficiency: -6.09e-05  training_efficiency: 1.34828e-06  stability: -23.6094
[2021-06-17 05:12:39,023 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 380000  wall_t: 2012  opt_step: 277944  frame: 380000  fps: 188.867  total_reward: 750  total_reward_ma: 727.898  loss: 3.22574e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:12:39,101 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 727.898  strength: 84.8979  max_strength: 1069.5  final_strength: 107  sample_efficiency: -7.48428e-06  training_efficiency: 6.95412e-06  stability: -0.0927231
[2021-06-17 05:12:45,207 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 380000  wall_t: 2019  opt_step: 277944  frame: 380000  fps: 188.212  total_reward: 475  total_reward_ma: 683.224  loss: 0.00388837  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:12:45,289 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 683.224  strength: 40.2237  max_strength: 932  final_strength: -168  sample_efficiency: -3.40923e-05  training_efficiency: -1.0162e-05  stability: -1.07781
[2021-06-17 05:13:22,544 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 390000  wall_t: 2056  opt_step: 285456  frame: 390000  fps: 189.689  total_reward: 762.5  total_reward_ma: 784.273  loss: 9.08105e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:13:22,599 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 784.273  strength: 141.273  max_strength: 619.5  final_strength: 119.5  sample_efficiency: -1.45971e-08  training_efficiency: 1.03365e-05  stability: 0.385455
[2021-06-17 05:13:25,350 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 390000  wall_t: 2059  opt_step: 285456  frame: 390000  fps: 189.412  total_reward: 887.5  total_reward_ma: 674.252  loss: 0.00414559  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:13:25,481 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 674.252  strength: 31.2521  max_strength: 857  final_strength: 244.5  sample_efficiency: -4.8169e-05  training_efficiency: 1.78055e-06  stability: -2.5922
[2021-06-17 05:13:26,097 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 390000  wall_t: 2059  opt_step: 285456  frame: 390000  fps: 189.412  total_reward: 512.5  total_reward_ma: 722.375  loss: 0.0039357  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:13:26,180 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 722.375  strength: 79.3748  max_strength: 1069.5  final_strength: -130.5  sample_efficiency: -7.90788e-06  training_efficiency: 7.0996e-06  stability: -0.130099
[2021-06-17 05:13:32,874 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 390000  wall_t: 2066  opt_step: 285456  frame: 390000  fps: 188.771  total_reward: 887.5  total_reward_ma: 688.462  loss: 4.11533e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:13:32,955 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 688.462  strength: 45.4615  max_strength: 932  final_strength: 244.5  sample_efficiency: -2.90373e-05  training_efficiency: -8.27756e-06  stability: -1.30618
[2021-06-17 05:14:09,324 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 400000  wall_t: 2103  opt_step: 292968  frame: 400000  fps: 190.204  total_reward: 650  total_reward_ma: 780.917  loss: 0.000106329  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:14:09,372 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 780.917  strength: 137.917  max_strength: 619.5  final_strength: 7  sample_efficiency: -1.14065e-08  training_efficiency: 1.03277e-05  stability: 0.378365
[2021-06-17 05:14:12,576 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 400000  wall_t: 2106  opt_step: 292968  frame: 400000  fps: 189.934  total_reward: 925  total_reward_ma: 680.521  loss: 0.00752513  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:14:12,731 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 680.521  strength: 37.5208  max_strength: 857  final_strength: 282  sample_efficiency: -3.86485e-05  training_efficiency: 2.08735e-06  stability: -1.8716
[2021-06-17 05:14:13,099 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 400000  wall_t: 2106  opt_step: 292968  frame: 400000  fps: 189.934  total_reward: 575  total_reward_ma: 718.69  loss: 0.00386882  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:14:13,206 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 718.69  strength: 75.6905  max_strength: 1069.5  final_strength: -68  sample_efficiency: -8.14164e-06  training_efficiency: 7.18239e-06  stability: -0.17774
[2021-06-17 05:14:20,190 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 400000  wall_t: 2114  opt_step: 292968  frame: 400000  fps: 189.215  total_reward: 1025  total_reward_ma: 696.875  loss: 0.00390624  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:14:20,341 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 696.875  strength: 53.875  max_strength: 932  final_strength: 382  sample_efficiency: -2.3447e-05  training_efficiency: -6.20521e-06  stability: -0.988156
[2021-06-17 05:14:56,144 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 410000  wall_t: 2149  opt_step: 300480  frame: 410000  fps: 190.786  total_reward: 1100  total_reward_ma: 788.699  loss: 7.96839e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:14:56,249 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 788.699  strength: 145.699  max_strength: 619.5  final_strength: 457  sample_efficiency: 1.76057e-07  training_efficiency: 9.79221e-06  stability: 0.379154
[2021-06-17 05:15:00,020 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 410000  wall_t: 2153  opt_step: 300480  frame: 410000  fps: 190.432  total_reward: 525  total_reward_ma: 676.728  loss: 0.00391434  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:15:00,093 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 676.728  strength: 33.7276  max_strength: 857  final_strength: -118  sample_efficiency: -4.21546e-05  training_efficiency: 1.98148e-06  stability: -1.59856
[2021-06-17 05:15:00,227 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 410000  wall_t: 2154  opt_step: 300480  frame: 410000  fps: 190.344  total_reward: 562.5  total_reward_ma: 714.881  loss: 0.00390239  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:15:00,389 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 714.881  strength: 71.881  max_strength: 1069.5  final_strength: -80.5  sample_efficiency: -8.43065e-06  training_efficiency: 7.28767e-06  stability: -0.20832
[2021-06-17 05:15:07,835 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 410000  wall_t: 2161  opt_step: 300480  frame: 410000  fps: 189.727  total_reward: 1137.5  total_reward_ma: 707.622  loss: 1.42015e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:15:08,050 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 707.622  strength: 64.6219  max_strength: 932  final_strength: 494.5  sample_efficiency: -1.86156e-05  training_efficiency: -4.42594e-06  stability: -0.635731
[2021-06-17 05:15:44,044 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 420000  wall_t: 2197  opt_step: 307992  frame: 420000  fps: 191.17  total_reward: 1125  total_reward_ma: 796.706  loss: 0.00720545  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:15:44,301 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 796.706  strength: 153.706  max_strength: 619.5  final_strength: 482  sample_efficiency: 3.40682e-07  training_efficiency: 9.30351e-06  stability: 0.42665
[2021-06-17 05:15:47,231 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 420000  wall_t: 2201  opt_step: 307992  frame: 420000  fps: 190.822  total_reward: 537.5  total_reward_ma: 673.413  loss: 0.000100415  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:15:47,308 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 673.413  strength: 30.4127  max_strength: 857  final_strength: -105.5  sample_efficiency: -4.5833e-05  training_efficiency: 1.87697e-06  stability: -1.8203
[2021-06-17 05:15:47,752 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 420000  wall_t: 2201  opt_step: 307992  frame: 420000  fps: 190.822  total_reward: 687.5  total_reward_ma: 714.229  loss: 2.73916e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:15:47,880 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 714.229  strength: 71.229  max_strength: 1069.5  final_strength: 44.5  sample_efficiency: -8.26983e-06  training_efficiency: 7.22757e-06  stability: -0.241325
[2021-06-17 05:15:55,315 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 420000  wall_t: 2209  opt_step: 307992  frame: 420000  fps: 190.131  total_reward: 912.5  total_reward_ma: 712.5  loss: 0.00391021  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:15:55,549 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 712.5  strength: 69.5  max_strength: 932  final_strength: 269.5  sample_efficiency: -1.66771e-05  training_efficiency: -3.71754e-06  stability: -0.415361
[2021-06-17 05:16:31,382 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 430000  wall_t: 2245  opt_step: 315504  frame: 430000  fps: 191.537  total_reward: 900  total_reward_ma: 799.108  loss: 6.84626e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:16:31,551 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 799.108  strength: 156.109  max_strength: 619.5  final_strength: 257  sample_efficiency: 4.16675e-07  training_efficiency: 9.06867e-06  stability: 0.434605
[2021-06-17 05:16:34,235 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 430000  wall_t: 2248  opt_step: 315504  frame: 430000  fps: 191.281  total_reward: 700  total_reward_ma: 674.031  loss: 0.00310395  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:16:34,340 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 674.031  strength: 31.031  max_strength: 857  final_strength: 57  sample_efficiency: -4.37758e-05  training_efficiency: 1.93218e-06  stability: -2.05324
[2021-06-17 05:16:34,587 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 430000  wall_t: 2248  opt_step: 315504  frame: 430000  fps: 191.281  total_reward: 600  total_reward_ma: 711.573  loss: 1.59442e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:16:34,674 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 711.573  strength: 68.5725  max_strength: 1069.5  final_strength: -43  sample_efficiency: -8.42435e-06  training_efficiency: 7.28675e-06  stability: -0.252109
[2021-06-17 05:16:43,289 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 430000  wall_t: 2257  opt_step: 315504  frame: 430000  fps: 190.518  total_reward: 937.5  total_reward_ma: 717.733  loss: 5.11405e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:16:43,517 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 717.733  strength: 74.7326  max_strength: 932  final_strength: 294.5  sample_efficiency: -1.49356e-05  training_efficiency: -3.08638e-06  stability: -0.284687
[2021-06-17 05:17:18,782 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 440000  wall_t: 2292  opt_step: 323016  frame: 440000  fps: 191.972  total_reward: 662.5  total_reward_ma: 796.004  loss: 0.000130271  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:17:18,838 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 796.004  strength: 153.004  max_strength: 619.5  final_strength: 19.5  sample_efficiency: 4.22051e-07  training_efficiency: 9.05137e-06  stability: 0.420871
[2021-06-17 05:17:21,344 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 440000  wall_t: 2295  opt_step: 323016  frame: 440000  fps: 191.721  total_reward: 787.5  total_reward_ma: 676.61  loss: 4.96005e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:17:21,442 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 676.61  strength: 33.6098  max_strength: 857  final_strength: 144.5  sample_efficiency: -3.92763e-05  training_efficiency: 2.04589e-06  stability: -1.92281
[2021-06-17 05:17:21,460 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 440000  wall_t: 2295  opt_step: 323016  frame: 440000  fps: 191.721  total_reward: 550  total_reward_ma: 707.9  loss: 0.0117282  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:17:21,595 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 707.9  strength: 64.9004  max_strength: 1069.5  final_strength: -93  sample_efficiency: -8.77272e-06  training_efficiency: 7.42323e-06  stability: -0.287326
[2021-06-17 05:17:30,381 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 440000  wall_t: 2304  opt_step: 323016  frame: 440000  fps: 190.972  total_reward: 650  total_reward_ma: 716.193  loss: 0.000582104  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:17:30,480 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 716.193  strength: 73.1932  max_strength: 932  final_strength: 7  sample_efficiency: -1.48982e-05  training_efficiency: -3.07294e-06  stability: -0.256418
[2021-06-17 05:18:05,705 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 450000  wall_t: 2339  opt_step: 330528  frame: 450000  fps: 192.39  total_reward: 662.5  total_reward_ma: 793.037  loss: 0.000117017  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:18:05,826 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 793.037  strength: 150.037  max_strength: 619.5  final_strength: 19.5  sample_efficiency: 4.27251e-07  training_efficiency: 9.03396e-06  stability: 0.422548
[2021-06-17 05:18:08,163 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 450000  wall_t: 2341  opt_step: 330528  frame: 450000  fps: 192.226  total_reward: 837.5  total_reward_ma: 680.185  loss: 0.000251648  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:18:08,304 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 680.185  strength: 37.1852  max_strength: 857  final_strength: 194.5  sample_efficiency: -3.44527e-05  training_efficiency: 2.15975e-06  stability: -1.63721
[2021-06-17 05:18:08,600 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 450000  wall_t: 2342  opt_step: 330528  frame: 450000  fps: 192.143  total_reward: 512.5  total_reward_ma: 703.558  loss: 3.1418e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:18:08,702 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 703.558  strength: 60.5582  max_strength: 1069.5  final_strength: -130.5  sample_efficiency: -9.29924e-06  training_efficiency: 7.63383e-06  stability: -0.342383
[2021-06-17 05:18:17,902 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 450000  wall_t: 2351  opt_step: 330528  frame: 450000  fps: 191.408  total_reward: 575  total_reward_ma: 713.056  loss: 0.00391082  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:18:18,005 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 713.056  strength: 70.0556  max_strength: 932  final_strength: -68  sample_efficiency: -1.52675e-05  training_efficiency: -3.20448e-06  stability: -0.276976
[2021-06-17 05:18:52,768 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 460000  wall_t: 2386  opt_step: 338040  frame: 460000  fps: 192.791  total_reward: 1112.5  total_reward_ma: 799.982  loss: 0.00029225  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:18:52,947 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 799.982  strength: 156.982  max_strength: 619.5  final_strength: 469.5  sample_efficiency: 5.40814e-07  training_efficiency: 8.63894e-06  stability: 0.424216
[2021-06-17 05:18:55,353 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 460000  wall_t: 2389  opt_step: 338040  frame: 460000  fps: 192.549  total_reward: 1025  total_reward_ma: 687.681  loss: 0.000123097  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:18:55,507 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 460000  wall_t: 2389  opt_step: 338040  frame: 460000  fps: 192.549  total_reward: 600  total_reward_ma: 701.307  loss: 0.000899813  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:18:55,607 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 687.681  strength: 44.6812  max_strength: 857  final_strength: 382  sample_efficiency: -2.76453e-05  training_efficiency: 2.30815e-06  stability: -1.33068
[2021-06-17 05:18:55,786 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 701.307  strength: 58.3069  max_strength: 1069.5  final_strength: -43  sample_efficiency: -9.48318e-06  training_efficiency: 7.70879e-06  stability: -0.406666
[2021-06-17 05:19:04,878 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 460000  wall_t: 2398  opt_step: 338040  frame: 460000  fps: 191.827  total_reward: 600  total_reward_ma: 710.598  loss: 0.00385863  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:19:04,926 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 710.598  strength: 67.5978  max_strength: 932  final_strength: -43  sample_efficiency: -1.55087e-05  training_efficiency: -3.2897e-06  stability: -0.30452
[2021-06-17 05:19:40,125 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 470000  wall_t: 2433  opt_step: 345552  frame: 470000  fps: 193.177  total_reward: 1137.5  total_reward_ma: 807.163  loss: 0.00389755  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:19:40,244 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 807.163  strength: 164.163  max_strength: 619.5  final_strength: 494.5  sample_efficiency: 6.42515e-07  training_efficiency: 8.27074e-06  stability: 0.461652
[2021-06-17 05:19:43,024 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 470000  wall_t: 2436  opt_step: 345552  frame: 470000  fps: 192.939  total_reward: 712.5  total_reward_ma: 688.209  loss: 5.39258e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:19:43,092 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 470000  wall_t: 2436  opt_step: 345552  frame: 470000  fps: 192.939  total_reward: 500  total_reward_ma: 697.024  loss: 4.76912e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:19:43,264 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 697.024  strength: 54.0238  max_strength: 1069.5  final_strength: -143  sample_efficiency: -1.01371e-05  training_efficiency: 7.97996e-06  stability: -0.466502
[2021-06-17 05:19:43,264 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 688.209  strength: 45.2092  max_strength: 857  final_strength: 69.5  sample_efficiency: -2.66715e-05  training_efficiency: 2.32731e-06  stability: -1.04955
[2021-06-17 05:19:52,041 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 470000  wall_t: 2445  opt_step: 345552  frame: 470000  fps: 192.229  total_reward: 787.5  total_reward_ma: 712.234  loss: 0.0039148  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:19:52,125 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 712.234  strength: 69.234  max_strength: 932  final_strength: 144.5  sample_efficiency: -1.47255e-05  training_efficiency: -3.01511e-06  stability: -0.32256
[2021-06-17 05:20:27,687 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 480000  wall_t: 2481  opt_step: 353064  frame: 480000  fps: 193.47  total_reward: 975  total_reward_ma: 810.66  loss: 0.00018145  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:20:27,923 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 810.66  strength: 167.66  max_strength: 619.5  final_strength: 332  sample_efficiency: 7.01955e-07  training_efficiency: 8.04638e-06  stability: 0.475094
[2021-06-17 05:20:30,805 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 480000  wall_t: 2484  opt_step: 353064  frame: 480000  fps: 193.237  total_reward: 462.5  total_reward_ma: 692.138  loss: 0.00387048  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:20:30,843 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 480000  wall_t: 2484  opt_step: 353064  frame: 480000  fps: 193.237  total_reward: 550  total_reward_ma: 685.33  loss: 8.12107e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:20:30,948 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 692.138  strength: 49.1379  max_strength: 1069.5  final_strength: -180.5  sample_efficiency: -1.10723e-05  training_efficiency: 8.3739e-06  stability: -0.563863
[2021-06-17 05:20:30,976 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 685.33  strength: 42.3299  max_strength: 857  final_strength: -93  sample_efficiency: -2.79877e-05  training_efficiency: 2.30419e-06  stability: -1.05898
[2021-06-17 05:20:40,077 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 480000  wall_t: 2493  opt_step: 353064  frame: 480000  fps: 192.539  total_reward: 787.5  total_reward_ma: 713.802  loss: 0.00393494  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:20:40,144 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 713.802  strength: 70.8021  max_strength: 932  final_strength: 144.5  sample_efficiency: -1.40108e-05  training_efficiency: -2.76648e-06  stability: -0.263829
[2021-06-17 05:21:15,180 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 490000  wall_t: 2529  opt_step: 360576  frame: 490000  fps: 193.752  total_reward: 1062.5  total_reward_ma: 815.799  loss: 9.62509e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:21:15,335 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 815.799  strength: 172.799  max_strength: 619.5  final_strength: 419.5  sample_efficiency: 7.68288e-07  training_efficiency: 7.78513e-06  stability: 0.496749
[2021-06-17 05:21:17,726 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 490000  wall_t: 2531  opt_step: 360576  frame: 490000  fps: 193.599  total_reward: 512.5  total_reward_ma: 688.472  loss: 0.000180486  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:21:17,838 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 688.472  strength: 45.4718  max_strength: 1069.5  final_strength: -130.5  sample_efficiency: -1.18403e-05  training_efficiency: 8.70192e-06  stability: -0.683542
[2021-06-17 05:21:18,171 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 490000  wall_t: 2532  opt_step: 360576  frame: 490000  fps: 193.523  total_reward: 612.5  total_reward_ma: 683.844  loss: 0.00594514  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:21:18,208 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 683.844  strength: 40.8435  max_strength: 857  final_strength: -30.5  sample_efficiency: -2.84453e-05  training_efficiency: 2.29704e-06  stability: -1.15323
[2021-06-17 05:21:27,512 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 490000  wall_t: 2541  opt_step: 360576  frame: 490000  fps: 192.837  total_reward: 637.5  total_reward_ma: 712.245  loss: 8.7208e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:21:27,544 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 712.245  strength: 69.2449  max_strength: 932  final_strength: -5.5  sample_efficiency: -1.40368e-05  training_efficiency: -2.77546e-06  stability: -0.25423
[2021-06-17 05:22:02,709 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 500000  wall_t: 2576  opt_step: 368088  frame: 500000  fps: 194.099  total_reward: 1037.5  total_reward_ma: 820.233  loss: 0.00367709  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:22:02,868 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 820.233  strength: 177.233  max_strength: 619.5  final_strength: 394.5  sample_efficiency: 8.23121e-07  training_efficiency: 7.5595e-06  stability: 0.518729
[2021-06-17 05:22:05,224 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 500000  wall_t: 2579  opt_step: 368088  frame: 500000  fps: 193.874  total_reward: 637.5  total_reward_ma: 687.452  loss: 0.00390654  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:22:05,330 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 687.452  strength: 44.4524  max_strength: 1069.5  final_strength: -5.5  sample_efficiency: -1.18746e-05  training_efficiency: 8.71673e-06  stability: -0.782146
[2021-06-17 05:22:05,519 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 500000  wall_t: 2579  opt_step: 368088  frame: 500000  fps: 193.874  total_reward: 462.5  total_reward_ma: 679.417  loss: 0.000113614  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:22:05,630 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 679.417  strength: 36.4167  max_strength: 857  final_strength: -180.5  sample_efficiency: -3.14633e-05  training_efficiency: 2.25544e-06  stability: -1.26099
[2021-06-17 05:22:14,664 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 500000  wall_t: 2588  opt_step: 368088  frame: 500000  fps: 193.199  total_reward: 487.5  total_reward_ma: 707.75  loss: 1.42182e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:22:14,738 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 707.75  strength: 64.75  max_strength: 932  final_strength: -155.5  sample_efficiency: -1.48071e-05  training_efficiency: -3.03926e-06  stability: -0.300472
[2021-06-17 05:22:50,108 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 510000  wall_t: 2623  opt_step: 375600  frame: 510000  fps: 194.434  total_reward: 737.5  total_reward_ma: 818.611  loss: 0.00676789  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:22:50,187 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 818.611  strength: 175.611  max_strength: 619.5  final_strength: 94.5  sample_efficiency: 8.35125e-07  training_efficiency: 7.50783e-06  stability: 0.506301
[2021-06-17 05:22:52,486 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 510000  wall_t: 2626  opt_step: 375600  frame: 510000  fps: 194.212  total_reward: 587.5  total_reward_ma: 685.492  loss: 0.00371012  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:22:52,918 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 510000  wall_t: 2626  opt_step: 375600  frame: 510000  fps: 194.212  total_reward: 475  total_reward_ma: 675.408  loss: 0.00370135  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:22:53,043 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 685.492  strength: 42.4925  max_strength: 1069.5  final_strength: -55.5  sample_efficiency: -1.22289e-05  training_efficiency: 8.87178e-06  stability: -0.809052
[2021-06-17 05:22:53,051 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 675.408  strength: 32.4085  max_strength: 857  final_strength: -168  sample_efficiency: -3.48607e-05  training_efficiency: 2.21407e-06  stability: -1.48513
[2021-06-17 05:23:02,495 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 510000  wall_t: 2636  opt_step: 375600  frame: 510000  fps: 193.475  total_reward: 700  total_reward_ma: 707.598  loss: 1.39341e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:23:02,570 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 707.598  strength: 64.598  max_strength: 932  final_strength: 57  sample_efficiency: -1.4517e-05  training_efficiency: -2.94061e-06  stability: -0.362934
[2021-06-17 05:23:38,221 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 520000  wall_t: 2672  opt_step: 383112  frame: 520000  fps: 194.611  total_reward: 700  total_reward_ma: 816.33  loss: 0.00411672  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:23:38,376 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 816.33  strength: 173.33  max_strength: 619.5  final_strength: 57  sample_efficiency: 8.42005e-07  training_efficiency: 7.47685e-06  stability: 0.507323
[2021-06-17 05:23:41,229 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 520000  wall_t: 2675  opt_step: 383112  frame: 520000  fps: 194.393  total_reward: 675  total_reward_ma: 685.291  loss: 7.40875e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:23:41,329 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 685.291  strength: 42.2908  max_strength: 1069.5  final_strength: 32  sample_efficiency: -1.2023e-05  training_efficiency: 8.78066e-06  stability: -0.855382
[2021-06-17 05:23:41,597 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 520000  wall_t: 2675  opt_step: 383112  frame: 520000  fps: 194.393  total_reward: 675  total_reward_ma: 675.401  loss: 8.33878e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:23:41,702 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 675.401  strength: 32.4006  max_strength: 857  final_strength: 32  sample_efficiency: -3.41621e-05  training_efficiency: 2.2216e-06  stability: -1.73772
[2021-06-17 05:23:50,814 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 520000  wall_t: 2684  opt_step: 383112  frame: 520000  fps: 193.741  total_reward: 550  total_reward_ma: 704.567  loss: 4.33675e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:23:50,912 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 704.567  strength: 61.5673  max_strength: 932  final_strength: -93  sample_efficiency: -1.49945e-05  training_efficiency: -3.10185e-06  stability: -0.384884
[2021-06-17 05:24:25,452 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 530000  wall_t: 2719  opt_step: 390624  frame: 530000  fps: 194.925  total_reward: 812.5  total_reward_ma: 816.258  loss: 5.74326e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:24:25,555 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 816.258  strength: 173.258  max_strength: 619.5  final_strength: 169.5  sample_efficiency: 8.61291e-07  training_efficiency: 7.3861e-06  stability: 0.510438
[2021-06-17 05:24:28,237 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 530000  wall_t: 2722  opt_step: 390624  frame: 530000  fps: 194.71  total_reward: 987.5  total_reward_ma: 690.993  loss: 0.000124355  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:24:28,294 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 690.993  strength: 47.9928  max_strength: 1069.5  final_strength: 344.5  sample_efficiency: -1.01391e-05  training_efficiency: 7.93816e-06  stability: -0.828384
[2021-06-17 05:24:28,674 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 530000  wall_t: 2722  opt_step: 390624  frame: 530000  fps: 194.71  total_reward: 800  total_reward_ma: 677.752  loss: 9.68747e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:24:28,741 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 677.752  strength: 34.7516  max_strength: 857  final_strength: 157  sample_efficiency: -3.10892e-05  training_efficiency: 2.25044e-06  stability: -1.68573
[2021-06-17 05:24:38,462 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 530000  wall_t: 2732  opt_step: 390624  frame: 530000  fps: 193.997  total_reward: 612.5  total_reward_ma: 702.83  loss: 0.0039003  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:24:38,546 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 702.83  strength: 59.8302  max_strength: 932  final_strength: -30.5  sample_efficiency: -1.51569e-05  training_efficiency: -3.15631e-06  stability: -0.425113
[2021-06-17 05:25:13,118 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 540000  wall_t: 2766  opt_step: 398136  frame: 540000  fps: 195.228  total_reward: 712.5  total_reward_ma: 814.336  loss: 0.00373631  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:25:13,194 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 814.336  strength: 171.336  max_strength: 619.5  final_strength: 69.5  sample_efficiency: 8.68731e-07  training_efficiency: 7.34948e-06  stability: 0.508585
[2021-06-17 05:25:15,611 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 540000  wall_t: 2769  opt_step: 398136  frame: 540000  fps: 195.016  total_reward: 1050  total_reward_ma: 697.641  loss: 7.15289e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:25:15,701 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 697.641  strength: 54.6411  max_strength: 1069.5  final_strength: 407  sample_efficiency: -8.48508e-06  training_efficiency: 7.18965e-06  stability: -0.580753
[2021-06-17 05:25:15,942 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 540000  wall_t: 2769  opt_step: 398136  frame: 540000  fps: 195.016  total_reward: 625  total_reward_ma: 676.775  loss: 0.00400183  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:25:15,976 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 676.775  strength: 33.7747  max_strength: 857  final_strength: -18  sample_efficiency: -3.14143e-05  training_efficiency: 2.24787e-06  stability: -1.55181
[2021-06-17 05:25:26,665 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 540000  wall_t: 2780  opt_step: 398136  frame: 540000  fps: 194.245  total_reward: 775  total_reward_ma: 704.167  loss: 7.31184e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:25:26,748 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 704.167  strength: 61.1667  max_strength: 932  final_strength: 132  sample_efficiency: -1.44772e-05  training_efficiency: -2.9298e-06  stability: -0.438821
[2021-06-17 05:26:00,695 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 550000  wall_t: 2814  opt_step: 405648  frame: 550000  fps: 195.451  total_reward: 775  total_reward_ma: 813.621  loss: 0.000164346  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:26:00,783 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 813.621  strength: 170.621  max_strength: 619.5  final_strength: 132  sample_efficiency: 8.82087e-07  training_efficiency: 7.28078e-06  stability: 0.512276
[2021-06-17 05:26:03,049 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 550000  wall_t: 2816  opt_step: 405648  frame: 550000  fps: 195.312  total_reward: 962.5  total_reward_ma: 702.457  loss: 5.34187e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:26:03,074 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 550000  wall_t: 2816  opt_step: 405648  frame: 550000  fps: 195.312  total_reward: 712.5  total_reward_ma: 677.424  loss: 0.00010545  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:26:03,168 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 702.457  strength: 59.4567  max_strength: 1069.5  final_strength: 319.5  sample_efficiency: -7.47842e-06  training_efficiency: 6.72806e-06  stability: -0.392363
[2021-06-17 05:26:03,174 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 677.424  strength: 34.4242  max_strength: 857  final_strength: 69.5  sample_efficiency: -3.01944e-05  training_efficiency: 2.25584e-06  stability: -1.57699
[2021-06-17 05:26:14,033 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 550000  wall_t: 2827  opt_step: 405648  frame: 550000  fps: 194.553  total_reward: 662.5  total_reward_ma: 703.409  loss: 9.82012e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:26:14,107 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 703.409  strength: 60.4091  max_strength: 932  final_strength: 19.5  sample_efficiency: -1.43815e-05  training_efficiency: -2.89813e-06  stability: -0.41538
[2021-06-17 05:26:47,885 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 560000  wall_t: 2861  opt_step: 413160  frame: 560000  fps: 195.736  total_reward: 675  total_reward_ma: 811.146  loss: 0.00016693  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:26:47,988 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 811.146  strength: 168.146  max_strength: 619.5  final_strength: 32  sample_efficiency: 8.85157e-07  training_efficiency: 7.26426e-06  stability: 0.508481
[2021-06-17 05:26:50,007 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 560000  wall_t: 2863  opt_step: 413160  frame: 560000  fps: 195.599  total_reward: 862.5  total_reward_ma: 705.315  loss: 0.00424685  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:26:50,022 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 560000  wall_t: 2863  opt_step: 413160  frame: 560000  fps: 195.599  total_reward: 812.5  total_reward_ma: 679.836  loss: 4.55148e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:26:50,149 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 705.315  strength: 62.3146  max_strength: 1069.5  final_strength: 219.5  sample_efficiency: -6.8957e-06  training_efficiency: 6.4571e-06  stability: -0.286905
[2021-06-17 05:26:50,160 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 679.836  strength: 36.8363  max_strength: 857  final_strength: 169.5  sample_efficiency: -2.75667e-05  training_efficiency: 2.26936e-06  stability: -1.48239
[2021-06-17 05:27:01,261 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 560000  wall_t: 2875  opt_step: 413160  frame: 560000  fps: 194.783  total_reward: 762.5  total_reward_ma: 704.464  loss: 0.00407764  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:27:01,317 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 704.464  strength: 61.4643  max_strength: 932  final_strength: 119.5  sample_efficiency: -1.38202e-05  training_efficiency: -2.71349e-06  stability: -0.407073
[2021-06-17 05:27:35,280 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 570000  wall_t: 2909  opt_step: 420672  frame: 570000  fps: 195.944  total_reward: 350  total_reward_ma: 803.055  loss: 0.000147877  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:27:35,397 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 803.055  strength: 160.056  max_strength: 619.5  final_strength: -293  sample_efficiency: 8.57241e-07  training_efficiency: 7.42121e-06  stability: 0.475636
[2021-06-17 05:27:37,158 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 570000  wall_t: 2910  opt_step: 420672  frame: 570000  fps: 195.876  total_reward: 1225  total_reward_ma: 689.401  loss: 0.000136001  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:27:37,300 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 689.401  strength: 46.4006  max_strength: 857  final_strength: 582  sample_efficiency: -2.11145e-05  training_efficiency: 2.29308e-06  stability: -1.27842
[2021-06-17 05:27:37,450 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 570000  wall_t: 2911  opt_step: 420672  frame: 570000  fps: 195.809  total_reward: 612.5  total_reward_ma: 703.686  loss: 0.000101399  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:27:37,555 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 703.686  strength: 60.6863  max_strength: 1069.5  final_strength: -30.5  sample_efficiency: -6.97197e-06  training_efficiency: 6.49307e-06  stability: -0.277599
[2021-06-17 05:27:48,363 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 570000  wall_t: 2922  opt_step: 420672  frame: 570000  fps: 195.072  total_reward: 812.5  total_reward_ma: 706.36  loss: 4.3727e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:27:48,450 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 706.36  strength: 63.3596  max_strength: 932  final_strength: 169.5  sample_efficiency: -1.30893e-05  training_efficiency: -2.47456e-06  stability: -0.358222
[2021-06-17 05:28:22,642 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 580000  wall_t: 2956  opt_step: 428184  frame: 580000  fps: 196.211  total_reward: 675  total_reward_ma: 800.848  loss: 0.000206306  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:28:22,724 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 800.848  strength: 157.848  max_strength: 619.5  final_strength: 32  sample_efficiency: 8.60271e-07  training_efficiency: 7.40344e-06  stability: 0.458795
[2021-06-17 05:28:24,894 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 580000  wall_t: 2958  opt_step: 428184  frame: 580000  fps: 196.078  total_reward: 1012.5  total_reward_ma: 709.011  loss: 0.000165376  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:28:24,925 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 580000  wall_t: 2958  opt_step: 428184  frame: 580000  fps: 196.078  total_reward: 862.5  total_reward_ma: 692.385  loss: 0.000137447  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:28:24,991 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 709.011  strength: 66.0107  max_strength: 1069.5  final_strength: 369.5  sample_efficiency: -6.13271e-06  training_efficiency: 6.09182e-06  stability: -0.288864
[2021-06-17 05:28:25,113 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 692.385  strength: 49.3851  max_strength: 857  final_strength: 219.5  sample_efficiency: -1.93643e-05  training_efficiency: 2.29633e-06  stability: -0.914109
[2021-06-17 05:28:35,946 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 580000  wall_t: 2969  opt_step: 428184  frame: 580000  fps: 195.352  total_reward: 525  total_reward_ma: 703.233  loss: 8.80973e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:28:36,021 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 703.233  strength: 60.2328  max_strength: 932  final_strength: -118  sample_efficiency: -1.35896e-05  training_efficiency: -2.63703e-06  stability: -0.374083
[2021-06-17 05:29:09,917 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 590000  wall_t: 3003  opt_step: 435696  frame: 590000  fps: 196.47  total_reward: 387.5  total_reward_ma: 793.842  loss: 0.000130657  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:29:09,998 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 793.842  strength: 150.842  max_strength: 619.5  final_strength: -255.5  sample_efficiency: 8.3631e-07  training_efficiency: 7.55009e-06  stability: 0.429284
[2021-06-17 05:29:11,698 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 590000  wall_t: 3005  opt_step: 435696  frame: 590000  fps: 196.339  total_reward: 950  total_reward_ma: 696.751  loss: 0.000169771  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:29:11,793 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 590000  wall_t: 3005  opt_step: 435696  frame: 590000  fps: 196.339  total_reward: 1112.5  total_reward_ma: 715.849  loss: 0.000187034  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:29:11,906 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 696.751  strength: 53.7514  max_strength: 857  final_strength: 307  sample_efficiency: -1.73257e-05  training_efficiency: 2.29622e-06  stability: -0.767427
[2021-06-17 05:29:11,916 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 715.849  strength: 72.8495  max_strength: 1069.5  final_strength: 469.5  sample_efficiency: -5.27767e-06  training_efficiency: 5.6771e-06  stability: -0.164476
[2021-06-17 05:29:23,305 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 590000  wall_t: 3017  opt_step: 435696  frame: 590000  fps: 195.559  total_reward: 550  total_reward_ma: 700.636  loss: 0.00389585  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:29:23,384 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 700.636  strength: 57.6356  max_strength: 932  final_strength: -93  sample_efficiency: -1.40076e-05  training_efficiency: -2.77192e-06  stability: -0.420495
[2021-06-17 05:29:56,698 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 600000  wall_t: 3050  opt_step: 443208  frame: 600000  fps: 196.721  total_reward: 437.5  total_reward_ma: 787.903  loss: 7.52273e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:29:56,798 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 787.903  strength: 144.903  max_strength: 619.5  final_strength: -205.5  sample_efficiency: 8.16683e-07  training_efficiency: 7.67522e-06  stability: 0.412899
[2021-06-17 05:29:58,527 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 600000  wall_t: 3052  opt_step: 443208  frame: 600000  fps: 196.592  total_reward: 825  total_reward_ma: 717.669  loss: 7.88308e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:29:58,637 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 717.669  strength: 74.6687  max_strength: 1069.5  final_strength: 182  sample_efficiency: -4.99556e-06  training_efficiency: 5.53813e-06  stability: -0.104165
[2021-06-17 05:29:58,699 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 600000  wall_t: 3052  opt_step: 443208  frame: 600000  fps: 196.592  total_reward: 1562.5  total_reward_ma: 711.181  loss: 0.000102192  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:29:58,872 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 711.181  strength: 68.1806  max_strength: 919.5  final_strength: 919.5  sample_efficiency: -1.30568e-05  training_efficiency: 2.28724e-06  stability: -0.596332
[2021-06-17 05:30:10,028 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 600000  wall_t: 3063  opt_step: 443208  frame: 600000  fps: 195.886  total_reward: 800  total_reward_ma: 702.292  loss: 0.00390622  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:30:10,110 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 702.292  strength: 59.2917  max_strength: 932  final_strength: 157  sample_efficiency: -1.33159e-05  training_efficiency: -2.55002e-06  stability: -0.459344
[2021-06-17 05:30:43,736 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 610000  wall_t: 3097  opt_step: 450720  frame: 610000  fps: 196.965  total_reward: 587.5  total_reward_ma: 784.617  loss: 0.00243586  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:30:43,817 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 784.617  strength: 141.617  max_strength: 619.5  final_strength: -55.5  sample_efficiency: 8.11398e-07  training_efficiency: 7.71027e-06  stability: 0.399022
[2021-06-17 05:30:45,524 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 610000  wall_t: 3099  opt_step: 450720  frame: 610000  fps: 196.838  total_reward: 1700  total_reward_ma: 727.391  loss: 0.000126851  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:30:45,652 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 610000  wall_t: 3099  opt_step: 450720  frame: 610000  fps: 196.838  total_reward: 750  total_reward_ma: 718.199  loss: 0.00758528  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:30:45,727 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 727.391  strength: 84.3907  max_strength: 1057  final_strength: 1057  sample_efficiency: -1.00392e-05  training_efficiency: 2.27316e-06  stability: -0.237523
[2021-06-17 05:30:45,746 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 718.199  strength: 75.1987  max_strength: 1069.5  final_strength: 107  sample_efficiency: -4.84079e-06  training_efficiency: 5.4607e-06  stability: -0.0760503
[2021-06-17 05:30:56,907 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 610000  wall_t: 3110  opt_step: 450720  frame: 610000  fps: 196.141  total_reward: 837.5  total_reward_ma: 704.508  loss: 0.00392882  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:30:56,948 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 704.508  strength: 61.5082  max_strength: 932  final_strength: 194.5  sample_efficiency: -1.25406e-05  training_efficiency: -2.30281e-06  stability: -0.39494
[2021-06-17 05:31:31,187 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 620000  wall_t: 3145  opt_step: 458232  frame: 620000  fps: 197.138  total_reward: 725  total_reward_ma: 783.656  loss: 0.000125411  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:31:31,273 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 783.656  strength: 140.656  max_strength: 619.5  final_strength: 82  sample_efficiency: 8.18934e-07  training_efficiency: 7.6583e-06  stability: 0.395161
[2021-06-17 05:31:32,673 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 620000  wall_t: 3146  opt_step: 458232  frame: 620000  fps: 197.076  total_reward: 687.5  total_reward_ma: 717.703  loss: 0.00427212  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:31:32,766 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 717.703  strength: 74.7035  max_strength: 1069.5  final_strength: 44.5  sample_efficiency: -4.77879e-06  training_efficiency: 5.4292e-06  stability: -0.0645752
[2021-06-17 05:31:32,827 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 620000  wall_t: 3146  opt_step: 458232  frame: 620000  fps: 197.076  total_reward: 850  total_reward_ma: 729.368  loss: 5.3653e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:31:32,986 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 729.368  strength: 86.3683  max_strength: 1057  final_strength: 207  sample_efficiency: -9.5888e-06  training_efficiency: 2.26965e-06  stability: -0.148542
[2021-06-17 05:31:44,315 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 620000  wall_t: 3158  opt_step: 458232  frame: 620000  fps: 196.327  total_reward: 650  total_reward_ma: 703.629  loss: 0.000426209  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:31:44,403 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 703.629  strength: 60.629  max_strength: 932  final_strength: 7  sample_efficiency: -1.25143e-05  training_efficiency: -2.29446e-06  stability: -0.372601
[2021-06-17 05:32:18,708 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 630000  wall_t: 3192  opt_step: 465744  frame: 630000  fps: 197.368  total_reward: 525  total_reward_ma: 779.55  loss: 0.000157872  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:32:18,804 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 779.55  strength: 136.55  max_strength: 619.5  final_strength: -118  sample_efficiency: 8.08395e-07  training_efficiency: 7.73389e-06  stability: 0.377914
[2021-06-17 05:32:19,714 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 630000  wall_t: 3193  opt_step: 465744  frame: 630000  fps: 197.307  total_reward: 562.5  total_reward_ma: 715.24  loss: 0.000121329  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:32:19,791 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 715.24  strength: 72.24  max_strength: 1069.5  final_strength: -80.5  sample_efficiency: -4.89139e-06  training_efficiency: 5.48726e-06  stability: -0.0813353
[2021-06-17 05:32:19,912 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 630000  wall_t: 3193  opt_step: 465744  frame: 630000  fps: 197.307  total_reward: 800  total_reward_ma: 730.489  loss: 9.32812e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:32:20,070 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 730.489  strength: 87.4894  max_strength: 1057  final_strength: 157  sample_efficiency: -9.27046e-06  training_efficiency: 2.26616e-06  stability: -0.11348
[2021-06-17 05:32:31,344 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 630000  wall_t: 3205  opt_step: 465744  frame: 630000  fps: 196.568  total_reward: 787.5  total_reward_ma: 704.96  loss: 1.75585e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:32:31,391 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 704.96  strength: 61.9603  max_strength: 932  final_strength: 144.5  sample_efficiency: -1.19923e-05  training_efficiency: -2.13004e-06  stability: -0.370045
[2021-06-17 05:33:05,610 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 640000  wall_t: 3239  opt_step: 473256  frame: 640000  fps: 197.592  total_reward: 787.5  total_reward_ma: 779.674  loss: 0.00395031  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:33:05,685 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 779.674  strength: 136.674  max_strength: 619.5  final_strength: 144.5  sample_efficiency: 8.20852e-07  training_efficiency: 7.64104e-06  stability: 0.369382
[2021-06-17 05:33:06,588 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 640000  wall_t: 3240  opt_step: 473256  frame: 640000  fps: 197.531  total_reward: 562.5  total_reward_ma: 727.865  loss: 0.00386389  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:33:06,706 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 727.865  strength: 84.8646  max_strength: 1057  final_strength: -80.5  sample_efficiency: -9.43102e-06  training_efficiency: 2.26843e-06  stability: -0.124853
[2021-06-17 05:33:06,767 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 640000  wall_t: 3240  opt_step: 473256  frame: 640000  fps: 197.531  total_reward: 775  total_reward_ma: 716.174  loss: 0.000101691  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:33:06,867 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 716.174  strength: 73.1737  max_strength: 1069.5  final_strength: 132  sample_efficiency: -4.70948e-06  training_efficiency: 5.39215e-06  stability: -0.100462
[2021-06-17 05:33:18,171 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 640000  wall_t: 3252  opt_step: 473256  frame: 640000  fps: 196.802  total_reward: 687.5  total_reward_ma: 704.688  loss: 1.49697e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:33:18,215 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 704.688  strength: 61.6875  max_strength: 932  final_strength: 44.5  sample_efficiency: -1.18395e-05  training_efficiency: -2.08222e-06  stability: -0.344947
[2021-06-17 05:33:53,196 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 650000  wall_t: 3287  opt_step: 480768  frame: 650000  fps: 197.749  total_reward: 825  total_reward_ma: 780.372  loss: 0.000212156  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:33:53,274 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 780.372  strength: 137.372  max_strength: 619.5  final_strength: 182  sample_efficiency: 8.35479e-07  training_efficiency: 7.52769e-06  stability: 0.379799
[2021-06-17 05:33:53,744 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 650000  wall_t: 3287  opt_step: 480768  frame: 650000  fps: 197.749  total_reward: 525  total_reward_ma: 724.744  loss: 9.78801e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:33:53,855 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 724.744  strength: 81.7436  max_strength: 1057  final_strength: -118  sample_efficiency: -9.67463e-06  training_efficiency: 2.27261e-06  stability: -0.148429
[2021-06-17 05:33:54,089 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 650000  wall_t: 3287  opt_step: 480768  frame: 650000  fps: 197.749  total_reward: 1075  total_reward_ma: 721.694  loss: 0.000101508  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:33:54,181 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 721.694  strength: 78.6941  max_strength: 1069.5  final_strength: 432  sample_efficiency: -4.1818e-06  training_efficiency: 5.11242e-06  stability: -0.0694439
[2021-06-17 05:34:05,301 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 650000  wall_t: 3299  opt_step: 480768  frame: 650000  fps: 197.029  total_reward: 437.5  total_reward_ma: 700.577  loss: 0.0038945  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:34:05,393 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 700.577  strength: 57.5769  max_strength: 932  final_strength: -205.5  sample_efficiency: -1.25741e-05  training_efficiency: -2.31076e-06  stability: -0.39311
[2021-06-17 05:34:40,511 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 660000  wall_t: 3334  opt_step: 488280  frame: 660000  fps: 197.96  total_reward: 687.5  total_reward_ma: 778.965  loss: 8.13121e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:34:40,606 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 778.965  strength: 135.965  max_strength: 619.5  final_strength: 44.5  sample_efficiency: 8.3885e-07  training_efficiency: 7.50051e-06  stability: 0.377041
[2021-06-17 05:34:40,776 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 660000  wall_t: 3334  opt_step: 488280  frame: 660000  fps: 197.96  total_reward: 475  total_reward_ma: 720.96  loss: 0.00386637  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:34:40,896 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 720.96  strength: 77.9596  max_strength: 1057  final_strength: -168  sample_efficiency: -1.004e-05  training_efficiency: 2.27995e-06  stability: -0.183344
[2021-06-17 05:34:41,295 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 660000  wall_t: 3335  opt_step: 488280  frame: 660000  fps: 197.901  total_reward: 1075  total_reward_ma: 727.047  loss: 0.00022962  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:34:41,373 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 727.047  strength: 84.0473  max_strength: 1069.5  final_strength: 432  sample_efficiency: -3.73814e-06  training_efficiency: 4.87377e-06  stability: 0.0208765
[2021-06-17 05:34:52,644 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 660000  wall_t: 3346  opt_step: 488280  frame: 660000  fps: 197.25  total_reward: 637.5  total_reward_ma: 699.621  loss: 6.84042e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:34:52,731 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 699.621  strength: 56.6212  max_strength: 932  final_strength: -5.5  sample_efficiency: -1.25948e-05  training_efficiency: -2.31718e-06  stability: -0.469606
[2021-06-17 05:35:27,407 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 670000  wall_t: 3381  opt_step: 495792  frame: 670000  fps: 198.166  total_reward: 775  total_reward_ma: 778.905  loss: 0.000269658  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:35:27,520 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 778.905  strength: 135.905  max_strength: 619.5  final_strength: 132  sample_efficiency: 8.48326e-07  training_efficiency: 7.42102e-06  stability: 0.380131
[2021-06-17 05:35:27,722 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 670000  wall_t: 3381  opt_step: 495792  frame: 670000  fps: 198.166  total_reward: 437.5  total_reward_ma: 716.729  loss: 0.000589025  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:35:27,757 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 716.729  strength: 73.7289  max_strength: 1057  final_strength: -205.5  sample_efficiency: -1.05198e-05  training_efficiency: 2.29088e-06  stability: -0.229269
[2021-06-17 05:35:28,870 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 670000  wall_t: 3382  opt_step: 495792  frame: 670000  fps: 198.108  total_reward: 787.5  total_reward_ma: 727.95  loss: 0.000196259  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:35:28,972 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 727.95  strength: 84.9495  max_strength: 1069.5  final_strength: 144.5  sample_efficiency: -3.60534e-06  training_efficiency: 4.80124e-06  stability: 0.0453002
[2021-06-17 05:35:40,073 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 670000  wall_t: 3393  opt_step: 495792  frame: 670000  fps: 197.465  total_reward: 625  total_reward_ma: 698.507  loss: 1.24341e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:35:40,119 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 698.507  strength: 55.5075  max_strength: 932  final_strength: -18  sample_efficiency: -1.2663e-05  training_efficiency: -2.33816e-06  stability: -0.475114
[2021-06-17 05:36:14,706 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 680000  wall_t: 3428  opt_step: 503304  frame: 680000  fps: 198.366  total_reward: 562.5  total_reward_ma: 714.461  loss: 0.000103612  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:36:14,850 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 714.461  strength: 71.4608  max_strength: 1057  final_strength: -80.5  sample_efficiency: -1.07184e-05  training_efficiency: 2.29592e-06  stability: -0.280408
[2021-06-17 05:36:15,014 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 680000  wall_t: 3428  opt_step: 503304  frame: 680000  fps: 198.366  total_reward: 675  total_reward_ma: 777.377  loss: 0.000189022  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:36:15,118 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 777.377  strength: 134.377  max_strength: 619.5  final_strength: 32  sample_efficiency: 8.50505e-07  training_efficiency: 7.40199e-06  stability: 0.378134
[2021-06-17 05:36:16,382 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 680000  wall_t: 3430  opt_step: 503304  frame: 680000  fps: 198.251  total_reward: 637.5  total_reward_ma: 726.619  loss: 0.000304167  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:36:16,475 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 726.619  strength: 83.6194  max_strength: 1069.5  final_strength: -5.5  sample_efficiency: -3.61025e-06  training_efficiency: 4.80396e-06  stability: 0.0431838
[2021-06-17 05:36:27,286 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 680000  wall_t: 3441  opt_step: 503304  frame: 680000  fps: 197.617  total_reward: 662.5  total_reward_ma: 697.978  loss: 0.0038922  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:36:27,348 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 697.978  strength: 54.9779  max_strength: 932  final_strength: 19.5  sample_efficiency: -1.25893e-05  training_efficiency: -2.3156e-06  stability: -0.482253
[2021-06-17 05:37:02,319 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 690000  wall_t: 3476  opt_step: 510816  frame: 690000  fps: 198.504  total_reward: 375  total_reward_ma: 709.541  loss: 0.000219911  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:37:02,375 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 709.541  strength: 66.5411  max_strength: 1057  final_strength: -268  sample_efficiency: -1.14286e-05  training_efficiency: 2.31567e-06  stability: -0.340204
[2021-06-17 05:37:02,564 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 690000  wall_t: 3476  opt_step: 510816  frame: 690000  fps: 198.504  total_reward: 600  total_reward_ma: 774.807  loss: 0.000152274  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:37:02,664 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 774.807  strength: 131.807  max_strength: 619.5  final_strength: -43  sample_efficiency: 8.47674e-07  training_efficiency: 7.42773e-06  stability: 0.372104
[2021-06-17 05:37:04,241 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 690000  wall_t: 3478  opt_step: 510816  frame: 690000  fps: 198.39  total_reward: 662.5  total_reward_ma: 725.69  loss: 0.00414373  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:37:04,342 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 725.69  strength: 82.6901  max_strength: 1069.5  final_strength: 19.5  sample_efficiency: -3.59296e-06  training_efficiency: 4.79423e-06  stability: 0.0422583
[2021-06-17 05:37:14,638 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 690000  wall_t: 3488  opt_step: 510816  frame: 690000  fps: 197.821  total_reward: 812.5  total_reward_ma: 699.638  loss: 1.00132e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:37:14,694 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 699.638  strength: 56.6377  max_strength: 932  final_strength: 169.5  sample_efficiency: -1.19804e-05  training_efficiency: -2.13026e-06  stability: -0.474522
[2021-06-17 05:37:49,449 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 700000  wall_t: 3523  opt_step: 518328  frame: 700000  fps: 198.694  total_reward: 762.5  total_reward_ma: 774.631  loss: 0.000359986  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:37:49,522 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 774.631  strength: 131.631  max_strength: 619.5  final_strength: 119.5  sample_efficiency: 8.55208e-07  training_efficiency: 7.35642e-06  stability: 0.369136
[2021-06-17 05:37:49,638 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 700000  wall_t: 3523  opt_step: 518328  frame: 700000  fps: 198.694  total_reward: 687.5  total_reward_ma: 709.226  loss: 5.96116e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:37:49,693 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 709.226  strength: 66.2262  max_strength: 1057  final_strength: 44.5  sample_efficiency: -1.13052e-05  training_efficiency: 2.31196e-06  stability: -0.418433
[2021-06-17 05:37:51,246 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 700000  wall_t: 3525  opt_step: 518328  frame: 700000  fps: 198.582  total_reward: 675  total_reward_ma: 724.966  loss: 0.00786526  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:37:51,304 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 724.966  strength: 81.966  max_strength: 1069.5  final_strength: 32  sample_efficiency: -3.56495e-06  training_efficiency: 4.77826e-06  stability: 0.0455316
[2021-06-17 05:38:02,261 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 700000  wall_t: 3536  opt_step: 518328  frame: 700000  fps: 197.964  total_reward: 925  total_reward_ma: 702.857  loss: 0.00779627  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:38:02,326 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 702.857  strength: 59.8571  max_strength: 932  final_strength: 282  sample_efficiency: -1.10779e-05  training_efficiency: -1.85704e-06  stability: -0.410568
[2021-06-17 05:38:36,904 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 710000  wall_t: 3570  opt_step: 525840  frame: 710000  fps: 198.88  total_reward: 675  total_reward_ma: 773.228  loss: 0.000206212  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:38:36,920 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 710000  wall_t: 3570  opt_step: 525840  frame: 710000  fps: 198.88  total_reward: 625  total_reward_ma: 708.04  loss: 0.000140451  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:38:37,003 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 773.228  strength: 130.228  max_strength: 619.5  final_strength: 32  sample_efficiency: 8.57122e-07  training_efficiency: 7.33754e-06  stability: 0.367821
[2021-06-17 05:38:37,025 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 708.04  strength: 65.0399  max_strength: 1057  final_strength: -18  sample_efficiency: -1.13548e-05  training_efficiency: 2.31356e-06  stability: -0.418299
[2021-06-17 05:38:38,727 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 710000  wall_t: 3572  opt_step: 525840  frame: 710000  fps: 198.768  total_reward: 925  total_reward_ma: 727.783  loss: 0.00391604  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:38:38,812 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 727.783  strength: 84.7834  max_strength: 1069.5  final_strength: 282  sample_efficiency: -3.33196e-06  training_efficiency: 4.6435e-06  stability: 0.0508548
[2021-06-17 05:38:50,251 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 710000  wall_t: 3584  opt_step: 525840  frame: 710000  fps: 198.103  total_reward: 1012.5  total_reward_ma: 707.218  loss: 4.03869e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:38:50,307 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 707.218  strength: 64.2183  max_strength: 932  final_strength: 369.5  sample_efficiency: -1.0066e-05  training_efficiency: -1.55243e-06  stability: -0.315632
[2021-06-17 05:39:24,172 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 720000  wall_t: 3617  opt_step: 533352  frame: 720000  fps: 199.06  total_reward: 525  total_reward_ma: 705.498  loss: 9.01767e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:39:24,212 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 705.498  strength: 62.4977  max_strength: 1057  final_strength: -118  sample_efficiency: -1.16889e-05  training_efficiency: 2.32506e-06  stability: -0.445483
[2021-06-17 05:39:24,226 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 720000  wall_t: 3618  opt_step: 533352  frame: 720000  fps: 199.005  total_reward: 887.5  total_reward_ma: 774.815  loss: 0.00349881  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:39:24,278 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 774.815  strength: 131.815  max_strength: 619.5  final_strength: 244.5  sample_efficiency: 8.70822e-07  training_efficiency: 7.19682e-06  stability: 0.370009
[2021-06-17 05:39:26,063 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 720000  wall_t: 3619  opt_step: 533352  frame: 720000  fps: 198.95  total_reward: 1162.5  total_reward_ma: 733.821  loss: 0.000109255  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:39:26,210 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 733.821  strength: 90.8211  max_strength: 1069.5  final_strength: 519.5  sample_efficiency: -2.95692e-06  training_efficiency: 4.42355e-06  stability: 0.0953193
[2021-06-17 05:39:37,402 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 720000  wall_t: 3631  opt_step: 533352  frame: 720000  fps: 198.292  total_reward: 1025  total_reward_ma: 711.632  loss: 1.73882e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:39:37,444 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 711.632  strength: 68.6319  max_strength: 932  final_strength: 382  sample_efficiency: -9.18051e-06  training_efficiency: -1.28748e-06  stability: -0.209014
[2021-06-17 05:40:10,814 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 730000  wall_t: 3664  opt_step: 540864  frame: 730000  fps: 199.236  total_reward: 550  total_reward_ma: 771.735  loss: 0.00376367  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:40:10,875 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 771.735  strength: 128.735  max_strength: 619.5  final_strength: -93  sample_efficiency: 8.65883e-07  training_efficiency: 7.24974e-06  stability: 0.350678
[2021-06-17 05:40:11,556 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 730000  wall_t: 3665  opt_step: 540864  frame: 730000  fps: 199.181  total_reward: 700  total_reward_ma: 705.422  loss: 0.000113838  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:40:11,598 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 705.422  strength: 62.4224  max_strength: 1057  final_strength: 57  sample_efficiency: -1.15256e-05  training_efficiency: 2.3191e-06  stability: -0.483388
[2021-06-17 05:40:13,000 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 730000  wall_t: 3666  opt_step: 540864  frame: 730000  fps: 199.127  total_reward: 612.5  total_reward_ma: 732.159  loss: 0.000216405  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:40:13,076 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 732.159  strength: 89.1592  max_strength: 1069.5  final_strength: -30.5  sample_efficiency: -2.97719e-06  training_efficiency: 4.43562e-06  stability: 0.0830824
[2021-06-17 05:40:24,487 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 730000  wall_t: 3678  opt_step: 540864  frame: 730000  fps: 198.477  total_reward: 1287.5  total_reward_ma: 719.521  loss: 9.17012e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:40:24,689 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 719.521  strength: 76.5205  max_strength: 932  final_strength: 644.5  sample_efficiency: -7.96323e-06  training_efficiency: -9.2561e-07  stability: -0.115552
[2021-06-17 05:40:58,074 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 740000  wall_t: 3711  opt_step: 548376  frame: 740000  fps: 199.407  total_reward: 612.5  total_reward_ma: 769.583  loss: 8.38612e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:40:58,138 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 769.583  strength: 126.583  max_strength: 619.5  final_strength: -30.5  sample_efficiency: 8.64302e-07  training_efficiency: 7.26741e-06  stability: 0.344252
[2021-06-17 05:40:58,563 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 740000  wall_t: 3712  opt_step: 548376  frame: 740000  fps: 199.353  total_reward: 1025  total_reward_ma: 709.741  loss: 0.000320703  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:40:58,668 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 709.741  strength: 66.741  max_strength: 1057  final_strength: 382  sample_efficiency: -1.05296e-05  training_efficiency: 2.28077e-06  stability: -0.464833
[2021-06-17 05:40:59,676 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 740000  wall_t: 3713  opt_step: 548376  frame: 740000  fps: 199.3  total_reward: 587.5  total_reward_ma: 730.204  loss: 0.000117745  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:40:59,764 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 730.204  strength: 87.2043  max_strength: 1069.5  final_strength: -55.5  sample_efficiency: -3.01442e-06  training_efficiency: 4.45808e-06  stability: 0.0749446
[2021-06-17 05:41:12,029 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 740000  wall_t: 3725  opt_step: 548376  frame: 740000  fps: 198.658  total_reward: 1150  total_reward_ma: 725.338  loss: 1.25218e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:41:12,206 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 725.338  strength: 82.3378  max_strength: 932  final_strength: 507  sample_efficiency: -7.18816e-06  training_efficiency: -6.96851e-07  stability: -0.0114572
[2021-06-17 05:41:45,244 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 750000  wall_t: 3759  opt_step: 555888  frame: 750000  fps: 199.521  total_reward: 562.5  total_reward_ma: 766.822  loss: 0.000855622  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:41:45,317 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 766.822  strength: 123.822  max_strength: 619.5  final_strength: -80.5  sample_efficiency: 8.60237e-07  training_efficiency: 7.31481e-06  stability: 0.336779
[2021-06-17 05:41:45,932 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 750000  wall_t: 3759  opt_step: 555888  frame: 750000  fps: 199.521  total_reward: 912.5  total_reward_ma: 712.444  loss: 0.000295133  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:41:45,975 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 712.444  strength: 69.4445  max_strength: 1057  final_strength: 269.5  sample_efficiency: -9.91577e-06  training_efficiency: 2.25584e-06  stability: -0.374312
[2021-06-17 05:41:47,202 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 750000  wall_t: 3761  opt_step: 555888  frame: 750000  fps: 199.415  total_reward: 612.5  total_reward_ma: 728.635  loss: 7.80237e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:41:47,278 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 728.635  strength: 85.6349  max_strength: 1069.5  final_strength: -30.5  sample_efficiency: -3.03507e-06  training_efficiency: 4.47071e-06  stability: 0.0669886
[2021-06-17 05:41:59,252 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 750000  wall_t: 3773  opt_step: 555888  frame: 750000  fps: 198.781  total_reward: 1500  total_reward_ma: 735.667  loss: 4.45358e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:41:59,435 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 735.667  strength: 92.6667  max_strength: 932  final_strength: 857  sample_efficiency: -6.13738e-06  training_efficiency: -3.89098e-07  stability: 0.0727064
[2021-06-17 05:42:32,216 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 760000  wall_t: 3806  opt_step: 563400  frame: 760000  fps: 199.685  total_reward: 875  total_reward_ma: 768.246  loss: 0.000352028  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:42:32,337 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 768.246  strength: 125.246  max_strength: 619.5  final_strength: 232  sample_efficiency: 8.7134e-07  training_efficiency: 7.17979e-06  stability: 0.33103
[2021-06-17 05:42:33,393 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 760000  wall_t: 3807  opt_step: 563400  frame: 760000  fps: 199.632  total_reward: 1025  total_reward_ma: 716.557  loss: 0.000224222  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:42:33,449 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 716.557  strength: 73.557  max_strength: 1057  final_strength: 382  sample_efficiency: -9.14829e-06  training_efficiency: 2.22298e-06  stability: -0.3032
[2021-06-17 05:42:34,131 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 760000  wall_t: 3807  opt_step: 563400  frame: 760000  fps: 199.632  total_reward: 537.5  total_reward_ma: 726.12  loss: 4.02914e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:42:34,205 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 726.12  strength: 83.12  max_strength: 1069.5  final_strength: -105.5  sample_efficiency: -3.10773e-06  training_efficiency: 4.51573e-06  stability: 0.0508804
[2021-06-17 05:42:46,534 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 760000  wall_t: 3820  opt_step: 563400  frame: 760000  fps: 198.953  total_reward: 1375  total_reward_ma: 744.079  loss: 2.62835e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:42:46,661 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 744.079  strength: 101.079  max_strength: 932  final_strength: 732  sample_efficiency: -5.42719e-06  training_efficiency: -1.82892e-07  stability: 0.169065
[2021-06-17 05:43:19,403 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 770000  wall_t: 3853  opt_step: 570912  frame: 770000  fps: 199.844  total_reward: 975  total_reward_ma: 770.931  loss: 0.000136635  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:43:19,456 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 770.931  strength: 127.931  max_strength: 619.5  final_strength: 332  sample_efficiency: 8.85743e-07  training_efficiency: 6.99684e-06  stability: 0.347335
[2021-06-17 05:43:20,498 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 770000  wall_t: 3854  opt_step: 570912  frame: 770000  fps: 199.792  total_reward: 987.5  total_reward_ma: 720.076  loss: 0.000291531  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:43:20,560 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 720.076  strength: 77.0758  max_strength: 1057  final_strength: 344.5  sample_efficiency: -8.54187e-06  training_efficiency: 2.19562e-06  stability: -0.220857
[2021-06-17 05:43:21,054 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 770000  wall_t: 3854  opt_step: 570912  frame: 770000  fps: 199.792  total_reward: 987.5  total_reward_ma: 729.515  loss: 9.19181e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:43:21,109 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 729.515  strength: 86.5145  max_strength: 1069.5  final_strength: 344.5  sample_efficiency: -2.87985e-06  training_efficiency: 4.37278e-06  stability: 0.0350295
[2021-06-17 05:43:33,594 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 770000  wall_t: 3867  opt_step: 570912  frame: 770000  fps: 199.121  total_reward: 1425  total_reward_ma: 752.922  loss: 1.79891e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:43:33,715 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 752.922  strength: 109.922  max_strength: 932  final_strength: 782  sample_efficiency: -4.80577e-06  training_efficiency: -4.1637e-09  stability: 0.248243
[2021-06-17 05:44:07,004 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 780000  wall_t: 3900  opt_step: 578424  frame: 780000  fps: 200  total_reward: 662.5  total_reward_ma: 769.541  loss: 0.000381877  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:44:07,105 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 769.541  strength: 126.541  max_strength: 619.5  final_strength: 19.5  sample_efficiency: 8.86526e-07  training_efficiency: 6.98643e-06  stability: 0.337608
[2021-06-17 05:44:08,335 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 780000  wall_t: 3902  opt_step: 578424  frame: 780000  fps: 199.897  total_reward: 1125  total_reward_ma: 725.267  loss: 0.000388555  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:44:08,390 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 725.267  strength: 82.2671  max_strength: 1057  final_strength: 482  sample_efficiency: -7.80395e-06  training_efficiency: 2.16055e-06  stability: -0.14999
[2021-06-17 05:44:08,536 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 780000  wall_t: 3902  opt_step: 578424  frame: 780000  fps: 199.897  total_reward: 975  total_reward_ma: 732.662  loss: 0.000237609  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:44:08,628 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 732.662  strength: 89.6618  max_strength: 1069.5  final_strength: 332  sample_efficiency: -2.68228e-06  training_efficiency: 4.24727e-06  stability: 0.0830557
[2021-06-17 05:44:21,575 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 780000  wall_t: 3915  opt_step: 578424  frame: 780000  fps: 199.234  total_reward: 1162.5  total_reward_ma: 758.173  loss: 2.33486e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:44:21,739 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 758.173  strength: 115.173  max_strength: 932  final_strength: 519.5  sample_efficiency: -4.45372e-06  training_efficiency: 9.60526e-08  stability: 0.286685
[2021-06-17 05:44:54,177 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 790000  wall_t: 3948  opt_step: 585936  frame: 790000  fps: 200.101  total_reward: 837.5  total_reward_ma: 770.401  loss: 0.000168471  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:44:54,217 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 770.401  strength: 127.401  max_strength: 619.5  final_strength: 194.5  sample_efficiency: 8.93856e-07  training_efficiency: 6.8844e-06  stability: 0.338917
[2021-06-17 05:44:55,403 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 790000  wall_t: 3949  opt_step: 585936  frame: 790000  fps: 200.051  total_reward: 1312.5  total_reward_ma: 732.7  loss: 0.000126985  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:44:55,577 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 732.7  strength: 89.7004  max_strength: 1057  final_strength: 669.5  sample_efficiency: -6.94706e-06  training_efficiency: 2.11767e-06  stability: -0.0636088
[2021-06-17 05:44:55,699 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 790000  wall_t: 3949  opt_step: 585936  frame: 790000  fps: 200.051  total_reward: 487.5  total_reward_ma: 729.558  loss: 0.000234977  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:44:55,758 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 729.558  strength: 86.5585  max_strength: 1069.5  final_strength: -155.5  sample_efficiency: -2.77206e-06  training_efficiency: 4.30504e-06  stability: 0.0568784
[2021-06-17 05:45:09,180 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 790000  wall_t: 3963  opt_step: 585936  frame: 790000  fps: 199.344  total_reward: 1062.5  total_reward_ma: 762.025  loss: 3.85421e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:45:09,316 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 762.025  strength: 119.025  max_strength: 932  final_strength: 419.5  sample_efficiency: -4.19856e-06  training_efficiency: 1.67908e-07  stability: 0.316803
[2021-06-17 05:45:40,867 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 800000  wall_t: 3994  opt_step: 593448  frame: 800000  fps: 200.3  total_reward: 1400  total_reward_ma: 778.271  loss: 0.000157998  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:45:40,939 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 778.271  strength: 135.271  max_strength: 757  final_strength: 757  sample_efficiency: 9.18769e-07  training_efficiency: 6.52069e-06  stability: 0.351692
[2021-06-17 05:45:42,640 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 800000  wall_t: 3996  opt_step: 593448  frame: 800000  fps: 200.2  total_reward: 825  total_reward_ma: 730.751  loss: 0.000659387  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:45:42,686 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 800000  wall_t: 3996  opt_step: 593448  frame: 800000  fps: 200.2  total_reward: 862.5  total_reward_ma: 734.323  loss: 0.00016956  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:45:42,736 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 730.751  strength: 87.7515  max_strength: 1069.5  final_strength: 182  sample_efficiency: -2.66779e-06  training_efficiency: 4.23712e-06  stability: 0.0354316
[2021-06-17 05:45:42,858 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 734.323  strength: 91.3229  max_strength: 1057  final_strength: 219.5  sample_efficiency: -6.70078e-06  training_efficiency: 2.10468e-06  stability: -0.026624
[2021-06-17 05:45:56,470 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 800000  wall_t: 4010  opt_step: 593448  frame: 800000  fps: 199.501  total_reward: 737.5  total_reward_ma: 761.719  loss: 0.00392559  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:45:56,524 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 761.719  strength: 118.719  max_strength: 932  final_strength: 94.5  sample_efficiency: -4.14434e-06  training_efficiency: 1.83004e-07  stability: 0.312719
[2021-06-17 05:46:28,323 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 810000  wall_t: 4042  opt_step: 600960  frame: 810000  fps: 200.396  total_reward: 1412.5  total_reward_ma: 786.101  loss: 8.43804e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:46:28,390 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 786.101  strength: 143.101  max_strength: 769.5  final_strength: 769.5  sample_efficiency: 9.39734e-07  training_efficiency: 6.19827e-06  stability: 0.397043
[2021-06-17 05:46:29,928 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 810000  wall_t: 4043  opt_step: 600960  frame: 810000  fps: 200.346  total_reward: 1075  total_reward_ma: 738.529  loss: 9.70389e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:46:30,060 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 738.529  strength: 95.5288  max_strength: 1057  final_strength: 432  sample_efficiency: -6.25776e-06  training_efficiency: 2.08007e-06  stability: 0.00422037
[2021-06-17 05:46:30,393 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 810000  wall_t: 4044  opt_step: 600960  frame: 810000  fps: 200.297  total_reward: 787.5  total_reward_ma: 731.452  loss: 0.00755606  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:46:30,441 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 731.452  strength: 88.4521  max_strength: 1069.5  final_strength: 144.5  sample_efficiency: -2.58908e-06  training_efficiency: 4.18522e-06  stability: 0.0550967
[2021-06-17 05:46:43,708 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 810000  wall_t: 4057  opt_step: 600960  frame: 810000  fps: 199.655  total_reward: 625  total_reward_ma: 760.031  loss: 0.00402766  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:46:43,753 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 760.031  strength: 117.031  max_strength: 932  final_strength: -18  sample_efficiency: -4.15456e-06  training_efficiency: 1.80191e-07  stability: 0.307713
[2021-06-17 05:47:15,128 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 820000  wall_t: 4088  opt_step: 608472  frame: 820000  fps: 200.587  total_reward: 1437.5  total_reward_ma: 794.045  loss: 0.00803811  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:47:15,222 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 794.045  strength: 151.045  max_strength: 794.5  final_strength: 794.5  sample_efficiency: 9.57681e-07  training_efficiency: 5.9061e-06  stability: 0.437071
[2021-06-17 05:47:17,032 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 820000  wall_t: 4090  opt_step: 608472  frame: 820000  fps: 200.489  total_reward: 725  total_reward_ma: 738.364  loss: 0.000207895  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:47:17,084 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 738.364  strength: 95.3638  max_strength: 1057  final_strength: 82  sample_efficiency: -6.17935e-06  training_efficiency: 2.07549e-06  stability: 0.014582
[2021-06-17 05:47:17,518 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 820000  wall_t: 4091  opt_step: 608472  frame: 820000  fps: 200.44  total_reward: 1350  total_reward_ma: 738.995  loss: 0.000164825  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:47:17,574 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 738.995  strength: 95.9954  max_strength: 1069.5  final_strength: 707  sample_efficiency: -2.24701e-06  training_efficiency: 3.95693e-06  stability: 0.0741541
[2021-06-17 05:47:31,373 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 820000  wall_t: 4105  opt_step: 608472  frame: 820000  fps: 199.756  total_reward: 625  total_reward_ma: 758.384  loss: 0.00393175  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:47:31,420 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 758.384  strength: 115.384  max_strength: 932  final_strength: -18  sample_efficiency: -4.16478e-06  training_efficiency: 1.77407e-07  stability: 0.306398
[2021-06-17 05:48:01,984 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 830000  wall_t: 4135  opt_step: 615984  frame: 830000  fps: 200.726  total_reward: 725  total_reward_ma: 793.213  loss: 0.000153241  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:48:02,047 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 793.213  strength: 150.213  max_strength: 794.5  final_strength: 82  sample_efficiency: 9.59306e-07  training_efficiency: 5.87793e-06  stability: 0.415655
[2021-06-17 05:48:03,750 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 830000  wall_t: 4137  opt_step: 615984  frame: 830000  fps: 200.628  total_reward: 562.5  total_reward_ma: 736.245  loss: 0.000146884  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:48:03,793 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 736.245  strength: 93.245  max_strength: 1057  final_strength: -80.5  sample_efficiency: -6.25615e-06  training_efficiency: 2.0802e-06  stability: 0.00413483
[2021-06-17 05:48:04,394 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 830000  wall_t: 4138  opt_step: 615984  frame: 830000  fps: 200.58  total_reward: 1200  total_reward_ma: 744.55  loss: 0.000532819  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:48:04,451 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 744.55  strength: 101.55  max_strength: 1069.5  final_strength: 557  sample_efficiency: -2.01889e-06  training_efficiency: 3.80272e-06  stability: 0.138254
[2021-06-17 05:48:18,550 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 830000  wall_t: 4152  opt_step: 615984  frame: 830000  fps: 199.904  total_reward: 600  total_reward_ma: 756.476  loss: 2.73891e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:48:18,598 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 756.476  strength: 113.476  max_strength: 932  final_strength: -43  sample_efficiency: -4.18929e-06  training_efficiency: 1.70806e-07  stability: 0.302436
[2021-06-17 05:48:49,315 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 840000  wall_t: 4183  opt_step: 623496  frame: 840000  fps: 200.813  total_reward: 500  total_reward_ma: 789.722  loss: 0.000115343  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:48:49,386 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 789.722  strength: 146.722  max_strength: 794.5  final_strength: -143  sample_efficiency: 9.56624e-07  training_efficiency: 5.92752e-06  stability: 0.401452
[2021-06-17 05:48:50,575 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 840000  wall_t: 4184  opt_step: 623496  frame: 840000  fps: 200.765  total_reward: 587.5  total_reward_ma: 734.474  loss: 0.000190767  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:48:50,647 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 734.474  strength: 91.4742  max_strength: 1057  final_strength: -55.5  sample_efficiency: -6.30994e-06  training_efficiency: 2.08364e-06  stability: -0.00622356
[2021-06-17 05:48:51,580 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 840000  wall_t: 4185  opt_step: 623496  frame: 840000  fps: 200.717  total_reward: 1112.5  total_reward_ma: 748.93  loss: 0.0048386  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:48:51,651 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 748.93  strength: 105.93  max_strength: 1069.5  final_strength: 469.5  sample_efficiency: -1.84956e-06  training_efficiency: 3.6867e-06  stability: 0.184821
[2021-06-17 05:49:05,785 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 840000  wall_t: 4199  opt_step: 623496  frame: 840000  fps: 200.048  total_reward: 612.5  total_reward_ma: 754.762  loss: 7.01952e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:49:05,853 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 754.762  strength: 111.762  max_strength: 932  final_strength: -30.5  sample_efficiency: -4.20677e-06  training_efficiency: 1.6615e-07  stability: 0.299251
[2021-06-17 05:49:36,230 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 850000  wall_t: 4230  opt_step: 631008  frame: 850000  fps: 200.946  total_reward: 1087.5  total_reward_ma: 793.225  loss: 0.00199836  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:49:36,313 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 793.225  strength: 150.225  max_strength: 794.5  final_strength: 444.5  sample_efficiency: 9.64277e-07  training_efficiency: 5.77635e-06  stability: 0.394507
[2021-06-17 05:49:37,492 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 850000  wall_t: 4231  opt_step: 631008  frame: 850000  fps: 200.898  total_reward: 662.5  total_reward_ma: 733.627  loss: 0.000221671  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:49:37,550 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 733.627  strength: 90.6275  max_strength: 1057  final_strength: 19.5  sample_efficiency: -6.29099e-06  training_efficiency: 2.08237e-06  stability: -0.0134915
[2021-06-17 05:49:38,518 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 850000  wall_t: 4232  opt_step: 631008  frame: 850000  fps: 200.851  total_reward: 575  total_reward_ma: 746.884  loss: 0.00110804  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:49:38,576 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 746.884  strength: 103.884  max_strength: 1069.5  final_strength: -68  sample_efficiency: -1.87286e-06  training_efficiency: 3.70289e-06  stability: 0.167427
[2021-06-17 05:49:52,865 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 850000  wall_t: 4246  opt_step: 631008  frame: 850000  fps: 200.188  total_reward: 837.5  total_reward_ma: 755.735  loss: 7.32469e-07  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:49:52,914 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 755.735  strength: 112.735  max_strength: 932  final_strength: 194.5  sample_efficiency: -4.09751e-06  training_efficiency: 1.94944e-07  stability: 0.296975
[2021-06-17 05:50:23,487 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 860000  wall_t: 4277  opt_step: 638520  frame: 860000  fps: 201.076  total_reward: 912.5  total_reward_ma: 794.612  loss: 0.00425272  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:50:23,564 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 794.612  strength: 151.612  max_strength: 794.5  final_strength: 269.5  sample_efficiency: 9.6838e-07  training_efficiency: 5.68933e-06  stability: 0.401879
[2021-06-17 05:50:24,837 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 860000  wall_t: 4278  opt_step: 638520  frame: 860000  fps: 201.029  total_reward: 337.5  total_reward_ma: 729.021  loss: 0.000107659  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:50:24,891 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 729.021  strength: 86.0213  max_strength: 1057  final_strength: -305.5  sample_efficiency: -6.5988e-06  training_efficiency: 2.10369e-06  stability: -0.0531155
[2021-06-17 05:50:25,558 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 860000  wall_t: 4279  opt_step: 638520  frame: 860000  fps: 200.982  total_reward: 450  total_reward_ma: 743.432  loss: 0.000216305  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:50:25,645 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 743.432  strength: 100.432  max_strength: 1069.5  final_strength: -193  sample_efficiency: -1.94069e-06  training_efficiency: 3.75064e-06  stability: 0.146859
[2021-06-17 05:50:40,357 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 860000  wall_t: 4294  opt_step: 638520  frame: 860000  fps: 200.279  total_reward: 962.5  total_reward_ma: 758.14  loss: 1.82292e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:50:40,413 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 758.14  strength: 115.14  max_strength: 932  final_strength: 319.5  sample_efficiency: -3.92778e-06  training_efficiency: 2.39187e-07  stability: 0.311244
[2021-06-17 05:51:10,527 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 870000  wall_t: 4324  opt_step: 646032  frame: 870000  fps: 201.203  total_reward: 687.5  total_reward_ma: 793.381  loss: 0.000114526  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:51:10,574 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 793.381  strength: 150.381  max_strength: 794.5  final_strength: 44.5  sample_efficiency: 9.68996e-07  training_efficiency: 5.67524e-06  stability: 0.396986
[2021-06-17 05:51:11,666 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 870000  wall_t: 4325  opt_step: 646032  frame: 870000  fps: 201.156  total_reward: 337.5  total_reward_ma: 724.521  loss: 0.000147395  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:51:11,707 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 724.521  strength: 81.5211  max_strength: 1057  final_strength: -305.5  sample_efficiency: -6.93255e-06  training_efficiency: 2.12763e-06  stability: -0.0966048
[2021-06-17 05:51:12,770 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 870000  wall_t: 4326  opt_step: 646032  frame: 870000  fps: 201.11  total_reward: 262.5  total_reward_ma: 737.904  loss: 0.00292067  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:51:12,810 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 737.904  strength: 94.9037  max_strength: 1069.5  final_strength: -380.5  sample_efficiency: -2.0831e-06  training_efficiency: 3.85215e-06  stability: 0.106087
[2021-06-17 05:51:27,448 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 870000  wall_t: 4341  opt_step: 646032  frame: 870000  fps: 200.415  total_reward: 937.5  total_reward_ma: 760.201  loss: 3.24091e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:51:27,490 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 760.201  strength: 117.201  max_strength: 932  final_strength: 294.5  sample_efficiency: -3.78114e-06  training_efficiency: 2.76986e-07  stability: 0.330943
[2021-06-17 05:51:57,398 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 880000  wall_t: 4371  opt_step: 653544  frame: 880000  fps: 201.327  total_reward: 875  total_reward_ma: 794.309  loss: 0.000259886  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:51:57,446 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 794.309  strength: 151.309  max_strength: 794.5  final_strength: 232  sample_efficiency: 9.71912e-07  training_efficiency: 5.60302e-06  stability: 0.399037
[2021-06-17 05:51:58,510 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 880000  wall_t: 4372  opt_step: 653544  frame: 880000  fps: 201.281  total_reward: 400  total_reward_ma: 720.833  loss: 0.00389113  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:51:58,650 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 720.833  strength: 77.8333  max_strength: 1057  final_strength: -243  sample_efficiency: -7.21882e-06  training_efficiency: 2.14883e-06  stability: -0.143841
[2021-06-17 05:51:59,779 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 880000  wall_t: 4373  opt_step: 653544  frame: 880000  fps: 201.235  total_reward: 750  total_reward_ma: 738.041  loss: 0.000231323  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:51:59,949 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 738.041  strength: 95.0411  max_strength: 1069.5  final_strength: 107  sample_efficiency: -2.04191e-06  training_efficiency: 3.82244e-06  stability: 0.0648916
[2021-06-17 05:52:14,896 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 880000  wall_t: 4388  opt_step: 653544  frame: 880000  fps: 200.547  total_reward: 350  total_reward_ma: 755.54  loss: 3.82384e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:52:15,007 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 755.54  strength: 112.54  max_strength: 932  final_strength: -293  sample_efficiency: -3.92662e-06  training_efficiency: 2.39911e-07  stability: 0.292649
[2021-06-17 05:52:44,945 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 890000  wall_t: 4418  opt_step: 661056  frame: 890000  fps: 201.449  total_reward: 1000  total_reward_ma: 796.62  loss: 0.00038548  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:52:45,003 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 796.62  strength: 153.62  max_strength: 794.5  final_strength: 357  sample_efficiency: 9.75872e-07  training_efficiency: 5.49621e-06  stability: 0.409508
[2021-06-17 05:52:45,658 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 890000  wall_t: 4419  opt_step: 661056  frame: 890000  fps: 201.403  total_reward: 837.5  total_reward_ma: 722.144  loss: 0.000226977  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:52:45,719 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 722.144  strength: 79.1442  max_strength: 1057  final_strength: 194.5  sample_efficiency: -6.98846e-06  training_efficiency: 2.13127e-06  stability: -0.184422
[2021-06-17 05:52:47,147 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 890000  wall_t: 4420  opt_step: 661056  frame: 890000  fps: 201.357  total_reward: 950  total_reward_ma: 740.423  loss: 0.00444535  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:52:47,190 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 740.423  strength: 97.4227  max_strength: 1069.5  final_strength: 307  sample_efficiency: -1.92983e-06  training_efficiency: 3.74066e-06  stability: 0.0768549
[2021-06-17 05:53:02,467 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 890000  wall_t: 4436  opt_step: 661056  frame: 890000  fps: 200.631  total_reward: 687.5  total_reward_ma: 754.775  loss: 9.22922e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:53:02,509 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 754.775  strength: 111.775  max_strength: 932  final_strength: 44.5  sample_efficiency: -3.90403e-06  training_efficiency: 2.45605e-07  stability: 0.271722
[2021-06-17 05:53:32,103 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 900000  wall_t: 4465  opt_step: 668568  frame: 900000  fps: 201.568  total_reward: 862.5  total_reward_ma: 797.352  loss: 0.000356489  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:53:32,206 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 797.352  strength: 154.352  max_strength: 794.5  final_strength: 219.5  sample_efficiency: 9.78009e-07  training_efficiency: 5.433e-06  stability: 0.41487
[2021-06-17 05:53:32,686 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 900000  wall_t: 4466  opt_step: 668568  frame: 900000  fps: 201.523  total_reward: 1237.5  total_reward_ma: 727.87  loss: 0.000797484  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:53:32,773 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 727.87  strength: 84.8704  max_strength: 1057  final_strength: 594.5  sample_efficiency: -6.35807e-06  training_efficiency: 2.0818e-06  stability: -0.151717
[2021-06-17 05:53:34,574 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 900000  wall_t: 4468  opt_step: 668568  frame: 900000  fps: 201.432  total_reward: 350  total_reward_ma: 736.085  loss: 0.000241228  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:53:34,701 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 736.085  strength: 93.0847  max_strength: 1069.5  final_strength: -293  sample_efficiency: -2.03618e-06  training_efficiency: 3.81918e-06  stability: 0.0403414
[2021-06-17 05:53:49,861 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 900000  wall_t: 4483  opt_step: 668568  frame: 900000  fps: 200.758  total_reward: 325  total_reward_ma: 750  loss: 0.00391643  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:53:49,906 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 750  strength: 107  max_strength: 932  final_strength: -318  sample_efficiency: -4.06964e-06  training_efficiency: 2.04324e-07  stability: 0.23854
[2021-06-17 05:54:19,232 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 910000  wall_t: 4513  opt_step: 676080  frame: 910000  fps: 201.64  total_reward: 750  total_reward_ma: 796.831  loss: 0.00062854  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:54:19,278 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 796.831  strength: 153.831  max_strength: 794.5  final_strength: 107  sample_efficiency: 9.78933e-07  training_efficiency: 5.40278e-06  stability: 0.416017
[2021-06-17 05:54:20,101 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 910000  wall_t: 4513  opt_step: 676080  frame: 910000  fps: 201.64  total_reward: 900  total_reward_ma: 729.762  loss: 0.000400273  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:54:20,262 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 729.762  strength: 86.7619  max_strength: 1057  final_strength: 257  sample_efficiency: -6.11533e-06  training_efficiency: 2.06219e-06  stability: -0.106262
[2021-06-17 05:54:21,782 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 910000  wall_t: 4515  opt_step: 676080  frame: 910000  fps: 201.55  total_reward: 650  total_reward_ma: 735.139  loss: 0.0038163  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:54:21,841 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 735.139  strength: 92.1387  max_strength: 1069.5  final_strength: 7  sample_efficiency: -2.03357e-06  training_efficiency: 3.81722e-06  stability: 0.00677818
[2021-06-17 05:54:37,639 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 910000  wall_t: 4531  opt_step: 676080  frame: 910000  fps: 200.839  total_reward: 637.5  total_reward_ma: 748.764  loss: 8.27327e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:54:37,681 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 748.764  strength: 105.764  max_strength: 932  final_strength: -5.5  sample_efficiency: -4.07259e-06  training_efficiency: 2.03595e-07  stability: 0.213396
[2021-06-17 05:55:06,459 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 920000  wall_t: 4560  opt_step: 683592  frame: 920000  fps: 201.754  total_reward: 775  total_reward_ma: 796.594  loss: 0.0028171  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:55:06,511 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 796.594  strength: 153.594  max_strength: 794.5  final_strength: 132  sample_efficiency: 9.79942e-07  training_efficiency: 5.36598e-06  stability: 0.42048
[2021-06-17 05:55:07,278 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 920000  wall_t: 4561  opt_step: 683592  frame: 920000  fps: 201.71  total_reward: 662.5  total_reward_ma: 729.031  loss: 0.00401098  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:55:07,332 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 729.031  strength: 86.0308  max_strength: 1057  final_strength: 19.5  sample_efficiency: -6.09759e-06  training_efficiency: 2.06071e-06  stability: -0.100334
[2021-06-17 05:55:09,029 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 920000  wall_t: 4562  opt_step: 683592  frame: 920000  fps: 201.666  total_reward: 825  total_reward_ma: 736.115  loss: 9.95479e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:55:09,139 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 736.115  strength: 93.1154  max_strength: 1069.5  final_strength: 182  sample_efficiency: -1.96727e-06  training_efficiency: 3.7672e-06  stability: 0.0076074
[2021-06-17 05:55:25,193 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 920000  wall_t: 4579  opt_step: 683592  frame: 920000  fps: 200.917  total_reward: 837.5  total_reward_ma: 749.728  loss: 9.27514e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:55:25,237 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 749.728  strength: 106.728  max_strength: 932  final_strength: 194.5  sample_efficiency: -3.97039e-06  training_efficiency: 2.28539e-07  stability: 0.212946
[2021-06-17 05:55:54,042 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 930000  wall_t: 4607  opt_step: 691104  frame: 930000  fps: 201.867  total_reward: 962.5  total_reward_ma: 798.378  loss: 0.00015921  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:55:54,144 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 798.378  strength: 155.378  max_strength: 794.5  final_strength: 319.5  sample_efficiency: 9.8205e-07  training_efficiency: 5.27932e-06  stability: 0.425894
[2021-06-17 05:55:54,528 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 930000  wall_t: 4608  opt_step: 691104  frame: 930000  fps: 201.823  total_reward: 737.5  total_reward_ma: 729.122  loss: 0.000156258  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:55:54,592 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 729.122  strength: 86.1219  max_strength: 1057  final_strength: 94.5  sample_efficiency: -6.01296e-06  training_efficiency: 2.05347e-06  stability: -0.0976226
[2021-06-17 05:55:56,624 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 930000  wall_t: 4610  opt_step: 691104  frame: 930000  fps: 201.735  total_reward: 587.5  total_reward_ma: 734.517  loss: 8.14707e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:55:56,698 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 734.517  strength: 91.5174  max_strength: 1069.5  final_strength: -55.5  sample_efficiency: -1.98711e-06  training_efficiency: 3.78233e-06  stability: 0.000967145
[2021-06-17 05:56:12,558 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 930000  wall_t: 4626  opt_step: 691104  frame: 930000  fps: 201.038  total_reward: 787.5  total_reward_ma: 750.134  loss: 4.46754e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:56:12,602 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 750.134  strength: 107.134  max_strength: 932  final_strength: 144.5  sample_efficiency: -3.89721e-06  training_efficiency: 2.4621e-07  stability: 0.223444
[2021-06-17 05:56:41,488 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 940000  wall_t: 4655  opt_step: 698616  frame: 940000  fps: 201.933  total_reward: 712.5  total_reward_ma: 728.945  loss: 0.000125204  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:56:41,564 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 728.945  strength: 85.945  max_strength: 1057  final_strength: 69.5  sample_efficiency: -5.95208e-06  training_efficiency: 2.04812e-06  stability: -0.0877935
[2021-06-17 05:56:42,507 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 940000  wall_t: 4656  opt_step: 698616  frame: 940000  fps: 201.89  total_reward: 1150  total_reward_ma: 802.119  loss: 9.90768e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:56:42,616 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 802.119  strength: 159.119  max_strength: 794.5  final_strength: 507  sample_efficiency: 9.84822e-07  training_efficiency: 5.14889e-06  stability: 0.438588
[2021-06-17 05:56:43,685 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 940000  wall_t: 4657  opt_step: 698616  frame: 940000  fps: 201.847  total_reward: 812.5  total_reward_ma: 735.347  loss: 0.00419996  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:56:43,744 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 735.347  strength: 92.347  max_strength: 1069.5  final_strength: 169.5  sample_efficiency: -1.92754e-06  training_efficiency: 3.73643e-06  stability: -0.0055474
[2021-06-17 05:57:00,906 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 940000  wall_t: 4674  opt_step: 698616  frame: 940000  fps: 201.113  total_reward: 525  total_reward_ma: 747.739  loss: 0.00390825  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:57:00,963 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 747.739  strength: 104.739  max_strength: 932  final_strength: -118  sample_efficiency: -3.95667e-06  training_efficiency: 2.32005e-07  stability: 0.20836
[2021-06-17 05:57:28,619 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 950000  wall_t: 4702  opt_step: 706128  frame: 950000  fps: 202.042  total_reward: 587.5  total_reward_ma: 727.456  loss: 0.000143992  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:57:28,711 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 727.456  strength: 84.4561  max_strength: 1057  final_strength: -55.5  sample_efficiency: -6.00053e-06  training_efficiency: 2.05249e-06  stability: -0.093908
[2021-06-17 05:57:29,636 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 950000  wall_t: 4703  opt_step: 706128  frame: 950000  fps: 201.999  total_reward: 912.5  total_reward_ma: 803.281  loss: 0.00134813  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:57:29,680 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 803.281  strength: 160.281  max_strength: 794.5  final_strength: 269.5  sample_efficiency: 9.86022e-07  training_efficiency: 5.08283e-06  stability: 0.441739
[2021-06-17 05:57:30,744 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 950000  wall_t: 4704  opt_step: 706128  frame: 950000  fps: 201.956  total_reward: 787.5  total_reward_ma: 735.896  loss: 0.00325072  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:57:30,833 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 735.896  strength: 92.896  max_strength: 1069.5  final_strength: 144.5  sample_efficiency: -1.87874e-06  training_efficiency: 3.69844e-06  stability: 0.0112072
[2021-06-17 05:57:48,398 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 950000  wall_t: 4722  opt_step: 706128  frame: 950000  fps: 201.186  total_reward: 462.5  total_reward_ma: 744.737  loss: 0.00428371  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:57:48,447 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 744.737  strength: 101.737  max_strength: 932  final_strength: -180.5  sample_efficiency: -4.05022e-06  training_efficiency: 2.0989e-07  stability: 0.192524
[2021-06-17 05:58:15,878 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 960000  wall_t: 4749  opt_step: 713640  frame: 960000  fps: 202.148  total_reward: 450  total_reward_ma: 724.566  loss: 0.000189962  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:58:15,965 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 724.566  strength: 81.566  max_strength: 1057  final_strength: -193  sample_efficiency: -6.17411e-06  training_efficiency: 2.06854e-06  stability: -0.118612
[2021-06-17 05:58:17,022 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 960000  wall_t: 4750  opt_step: 713640  frame: 960000  fps: 202.105  total_reward: 637.5  total_reward_ma: 801.554  loss: 9.89386e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:58:17,111 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 801.554  strength: 158.554  max_strength: 794.5  final_strength: -5.5  sample_efficiency: 9.86003e-07  training_efficiency: 5.08416e-06  stability: 0.43356
[2021-06-17 05:58:17,591 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 960000  wall_t: 4751  opt_step: 713640  frame: 960000  fps: 202.063  total_reward: 700  total_reward_ma: 735.522  loss: 0.000116078  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:58:17,663 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 735.522  strength: 92.5221  max_strength: 1069.5  final_strength: 57  sample_efficiency: -1.86e-06  training_efficiency: 3.68369e-06  stability: 0.0174825
[2021-06-17 05:58:36,289 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 960000  wall_t: 4770  opt_step: 713640  frame: 960000  fps: 201.258  total_reward: 200  total_reward_ma: 739.062  loss: 3.26845e-06  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:58:36,409 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 739.062  strength: 96.0625  max_strength: 932  final_strength: -443  sample_efficiency: -4.29482e-06  training_efficiency: 1.5266e-07  stability: 0.150285
[2021-06-17 05:59:03,137 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 970000  wall_t: 4796  opt_step: 721152  frame: 970000  fps: 202.252  total_reward: 625  total_reward_ma: 723.539  loss: 0.00372731  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:59:03,219 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 723.539  strength: 80.5395  max_strength: 1057  final_strength: -18  sample_efficiency: -6.19071e-06  training_efficiency: 2.07011e-06  stability: -0.146184
[2021-06-17 05:59:04,504 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 970000  wall_t: 4798  opt_step: 721152  frame: 970000  fps: 202.168  total_reward: 687.5  total_reward_ma: 800.378  loss: 0.00407266  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:59:04,581 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 800.378  strength: 157.378  max_strength: 794.5  final_strength: 44.5  sample_efficiency: 9.86134e-07  training_efficiency: 5.07338e-06  stability: 0.433355
[2021-06-17 05:59:04,882 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 970000  wall_t: 4798  opt_step: 721152  frame: 970000  fps: 202.168  total_reward: 575  total_reward_ma: 733.867  loss: 0.000111044  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:59:04,929 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 733.867  strength: 90.8672  max_strength: 1069.5  final_strength: -68  sample_efficiency: -1.8823e-06  training_efficiency: 3.70142e-06  stability: 0.00971448
[2021-06-17 05:59:23,622 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 970000  wall_t: 4817  opt_step: 721152  frame: 970000  fps: 201.37  total_reward: 537.5  total_reward_ma: 736.985  loss: 1.28381e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:59:23,677 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 736.985  strength: 93.9845  max_strength: 932  final_strength: -105.5  sample_efficiency: -4.35646e-06  training_efficiency: 1.38379e-07  stability: 0.109466
[2021-06-17 05:59:50,839 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 980000  wall_t: 4844  opt_step: 728664  frame: 980000  fps: 202.312  total_reward: 950  total_reward_ma: 725.85  loss: 0.000309721  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:59:50,922 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 725.85  strength: 82.8503  max_strength: 1057  final_strength: 307  sample_efficiency: -5.91805e-06  training_efficiency: 2.04373e-06  stability: -0.148825
[2021-06-17 05:59:51,706 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 980000  wall_t: 4845  opt_step: 728664  frame: 980000  fps: 202.27  total_reward: 500  total_reward_ma: 797.313  loss: 0.000111408  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:59:51,761 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 797.313  strength: 154.313  max_strength: 794.5  final_strength: -143  sample_efficiency: 9.85809e-07  training_efficiency: 5.10837e-06  stability: 0.422724
[2021-06-17 05:59:52,425 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 980000  wall_t: 4846  opt_step: 728664  frame: 980000  fps: 202.229  total_reward: 475  total_reward_ma: 731.226  loss: 0.000199261  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 05:59:52,537 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 731.226  strength: 88.2257  max_strength: 1069.5  final_strength: -168  sample_efficiency: -1.9387e-06  training_efficiency: 3.74667e-06  stability: -0.00927091
[2021-06-17 06:00:11,001 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 980000  wall_t: 4864  opt_step: 728664  frame: 980000  fps: 201.48  total_reward: 1112.5  total_reward_ma: 740.816  loss: 0.00391415  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 06:00:11,046 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 740.816  strength: 97.8163  max_strength: 932  final_strength: 469.5  sample_efficiency: -4.09311e-06  training_efficiency: 1.98817e-07  stability: 0.0991609
[2021-06-17 06:00:38,247 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 990000  wall_t: 4892  opt_step: 736176  frame: 990000  fps: 202.371  total_reward: 1025  total_reward_ma: 728.872  loss: 0.00042519  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 06:00:38,294 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 728.872  strength: 85.872  max_strength: 1057  final_strength: 382  sample_efficiency: -5.60674e-06  training_efficiency: 2.01293e-06  stability: -0.105386
[2021-06-17 06:00:39,296 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 990000  wall_t: 4893  opt_step: 736176  frame: 990000  fps: 202.33  total_reward: 650  total_reward_ma: 795.825  loss: 0.000138825  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 06:00:39,349 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 795.825  strength: 152.825  max_strength: 794.5  final_strength: 7  sample_efficiency: 9.85821e-07  training_efficiency: 5.10664e-06  stability: 0.417265
[2021-06-17 06:00:40,313 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 990000  wall_t: 4894  opt_step: 736176  frame: 990000  fps: 202.289  total_reward: 662.5  total_reward_ma: 730.532  loss: 0.000175986  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 06:00:40,353 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 730.532  strength: 87.5315  max_strength: 1069.5  final_strength: 19.5  sample_efficiency: -1.93207e-06  training_efficiency: 3.7413e-06  stability: -0.0288817
[2021-06-17 06:00:58,338 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 990000  wall_t: 4912  opt_step: 736176  frame: 990000  fps: 201.547  total_reward: 1112.5  total_reward_ma: 744.571  loss: 0.00388769  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 06:00:58,390 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 744.571  strength: 101.571  max_strength: 932  final_strength: 469.5  sample_efficiency: -3.85484e-06  training_efficiency: 2.52958e-07  stability: 0.143282
[2021-06-17 06:01:24,940 PID:38690 INFO __init__.py log_summary] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df] epi: 0  t: 1e+06  wall_t: 4938  opt_step: 743688  frame: 1e+06  fps: 202.511  total_reward: 787.5  total_reward_ma: 729.458  loss: 0.000112744  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 06:01:24,986 PID:38690 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [train_df metrics] final_return_ma: 729.458  strength: 86.4583  max_strength: 1057  final_strength: 144.5  sample_efficiency: -5.49632e-06  training_efficiency: 2.00176e-06  stability: -0.0836536
[2021-06-17 06:01:26,188 PID:38688 INFO __init__.py log_summary] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df] epi: 0  t: 1e+06  wall_t: 4940  opt_step: 743688  frame: 1e+06  fps: 202.429  total_reward: 812.5  total_reward_ma: 795.992  loss: 0.000196675  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 06:01:26,522 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [train_df metrics] final_return_ma: 795.992  strength: 152.992  max_strength: 794.5  final_strength: 169.5  sample_efficiency: 9.85978e-07  training_efficiency: 5.06496e-06  stability: 0.417535
[2021-06-17 06:01:26,920 PID:38689 INFO __init__.py log_summary] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df] epi: 0  t: 1e+06  wall_t: 4940  opt_step: 743688  frame: 1e+06  fps: 202.429  total_reward: 575  total_reward_ma: 728.976  loss: 0.000152209  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 06:01:26,977 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [train_df metrics] final_return_ma: 728.976  strength: 85.9762  max_strength: 1069.5  final_strength: -68  sample_efficiency: -1.95526e-06  training_efficiency: 3.76025e-06  stability: -0.0366639
[2021-06-17 06:01:37,084 PID:38687 INFO __init__.py log_summary] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df] epi: 0  t: 1e+06  wall_t: 4950  opt_step: 743688  frame: 1e+06  fps: 202.02  total_reward: 1275  total_reward_ma: 749.875  loss: 1.03785e-05  lr: 0.0001  explore_var: nan  entropy_coef: nan  entropy: nan  grad_norm: nan
[2021-06-17 06:01:37,140 PID:38687 INFO __init__.py log_metrics] Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [train_df metrics] final_return_ma: 749.875  strength: 106.875  max_strength: 932  final_strength: 632  sample_efficiency: -3.56775e-06  training_efficiency: 3.17514e-07  stability: 0.183283
[2021-06-17 06:02:56,905 PID:38687 WARNING viz.py <lambda>] Failed to generate graph. Run retro-analysis to generate graphs later. 
For some reason plotly.py was unable to communicate with the
local orca server process, even though the server process seems to be running.

Please review the process and connection information below:

orca status
-----------
    state: running
    executable: /home/cvladu/anaconda3/envs/lab/bin/orca
    version: 1.3.0
    port: 40611
    pid: 40655
    command: ['/home/cvladu/anaconda3/envs/lab/bin/orca', 'serve', '-p', '40611', '--plotly', '/home/cvladu/anaconda3/envs/lab/lib/python3.7/site-packages/plotly/package_data/plotly.min.js', '--graph-only', '--mathjax', 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js']



If running on a headless server, prepend your Python command with `xvfb-run -a `, for example `xvfb-run -a python run_lab.py`
[2021-06-17 06:02:57,001 PID:38689 WARNING viz.py <lambda>][2021-06-17 06:02:57,017 PID:38690 WARNING viz.py <lambda>] Failed to generate graph. Run retro-analysis to generate graphs later. 
For some reason plotly.py was unable to communicate with the
local orca server process, even though the server process seems to be running.

Please review the process and connection information below:

orca status
-----------
    state: running
    executable: /home/cvladu/anaconda3/envs/lab/bin/orca
    version: 1.3.0
    port: 35495
    pid: 40656
    command: ['/home/cvladu/anaconda3/envs/lab/bin/orca', 'serve', '-p', '35495', '--plotly', '/home/cvladu/anaconda3/envs/lab/lib/python3.7/site-packages/plotly/package_data/plotly.min.js', '--graph-only', '--mathjax', 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js']



If running on a headless server, prepend your Python command with `xvfb-run -a `, for example `xvfb-run -a python run_lab.py`
 Failed to generate graph. Run retro-analysis to generate graphs later. 
For some reason plotly.py was unable to communicate with the
local orca server process, even though the server process seems to be running.

Please review the process and connection information below:

orca status
-----------
    state: running
    executable: /home/cvladu/anaconda3/envs/lab/bin/orca
    version: 1.3.0
    port: 39705
    pid: 40654
    command: ['/home/cvladu/anaconda3/envs/lab/bin/orca', 'serve', '-p', '39705', '--plotly', '/home/cvladu/anaconda3/envs/lab/lib/python3.7/site-packages/plotly/package_data/plotly.min.js', '--graph-only', '--mathjax', 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js']



If running on a headless server, prepend your Python command with `xvfb-run -a `, for example `xvfb-run -a python run_lab.py`
[2021-06-17 06:02:57,009 PID:38688 WARNING viz.py <lambda>] Failed to generate graph. Run retro-analysis to generate graphs later. 
For some reason plotly.py was unable to communicate with the
local orca server process, even though the server process seems to be running.

Please review the process and connection information below:

orca status
-----------
    state: running
    executable: /home/cvladu/anaconda3/envs/lab/bin/orca
    version: 1.3.0
    port: 36893
    pid: 40657
    command: ['/home/cvladu/anaconda3/envs/lab/bin/orca', 'serve', '-p', '36893', '--plotly', '/home/cvladu/anaconda3/envs/lab/lib/python3.7/site-packages/plotly/package_data/plotly.min.js', '--graph-only', '--mathjax', 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js']



If running on a headless server, prepend your Python command with `xvfb-run -a `, for example `xvfb-run -a python run_lab.py`
[2021-06-17 06:03:35,433 PID:38688 INFO __init__.py log_metrics] Trial 0 session 1 Dueling_DDQN_PrioritizedReplay_t0_s1 [eval_df metrics] final_return_ma: 795.992  strength: 152.992  max_strength: 794.5  final_strength: 169.5  sample_efficiency: 9.85978e-07  training_efficiency: 5.06496e-06  stability: 0.417535
[2021-06-17 06:03:35,433 PID:38689 INFO __init__.py log_metrics] Trial 0 session 2 Dueling_DDQN_PrioritizedReplay_t0_s2 [eval_df metrics] final_return_ma: 728.976  strength: 85.9762  max_strength: 1069.5  final_strength: -68  sample_efficiency: -1.95526e-06  training_efficiency: 3.76025e-06  stability: -0.0366639
[2021-06-17 06:03:35,445 PID:38690 INFO __init__.py log_metrics][2021-06-17 06:03:35,445 PID:38687 INFO __init__.py log_metrics] Trial 0 session 3 Dueling_DDQN_PrioritizedReplay_t0_s3 [eval_df metrics] final_return_ma: 729.458  strength: 86.4583  max_strength: 1057  final_strength: 144.5  sample_efficiency: -5.49632e-06  training_efficiency: 2.00176e-06  stability: -0.0836536 Trial 0 session 0 Dueling_DDQN_PrioritizedReplay_t0_s0 [eval_df metrics] final_return_ma: 749.875  strength: 106.875  max_strength: 932  final_strength: 632  sample_efficiency: -3.56775e-06  training_efficiency: 3.17514e-07  stability: 0.183283

[2021-06-17 06:03:50,767 PID:38690 INFO logger.py info] Session 3 done
[2021-06-17 06:03:50,767 PID:38689 INFO logger.py info] Session 2 done
[2021-06-17 06:03:50,780 PID:38687 INFO logger.py info] Session 0 done
[2021-06-17 06:03:51,093 PID:38688 INFO logger.py info] Session 1 done
[2021-06-17 06:06:01,766 PID:38460 INFO analysis.py analyze_trial] All trial data zipped to data/Dueling_DDQN_PrioritizedReplay_2021_06_17_043903.zip
[2021-06-17 06:06:01,781 PID:38460 INFO logger.py info] Trial 0 done
[2021-06-17 06:06:51,052 PID:41245 INFO run_lab.py get_spec_and_run] Running lab spec_file:../../../config_files/RL_project_wizardofwor.json spec_name:A2C_nstep in mode:train
[2021-06-17 06:06:51,253 PID:41245 INFO logger.py info] Running sessions
[2021-06-17 06:06:55,134 PID:41482 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7fe5c6ad7c18>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7fe5bbeaef98>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 06:06:55,132 PID:41481 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7ff545445be0>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7ff53a828f28>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 06:06:55,132 PID:41483 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7f173f3fc320>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7f16b4c035f8>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 06:06:55,169 PID:41484 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'frame_op': 'concat',
 'frame_op_len': 4,
 'max_frame': 1000000,
 'max_t': None,
 'name': 'WizardOfWor-v0',
 'num_envs': 8,
 'reward_scale': 'sign'}
- eval_frequency = 5000
- log_frequency = 10000
- frame_op = concat
- frame_op_len = 4
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = sign
- num_envs = 8
- name = WizardOfWor-v0
- max_t = 10000
- max_frame = 1000000
- to_render = False
- is_venv = True
- clock_speed = 8
- clock = <slm_lab.env.base.Clock object at 0x7f048b7e3be0>
- done = False
- total_reward = nan
- u_env = <slm_lab.env.vec_env.VecFrameStack object at 0x7f0480baff60>
- observation_space = Box(4, 84, 84)
- action_space = Discrete(10)
- observable_dim = {'state': (4, 84, 84)}
- action_dim = 10
- is_discrete = True
[2021-06-17 06:07:10,904 PID:41483 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train
[2021-06-17 06:07:11,664 PID:41482 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train
[2021-06-17 06:07:11,768 PID:41484 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train
[2021-06-17 06:07:11,808 PID:41481 INFO base.py end_init_nets] Initialized algorithm models for lab_mode: train
[2021-06-17 06:07:19,466 PID:41484 INFO base.py __init__][2021-06-17 06:07:19,466 PID:41481 INFO base.py __init__][2021-06-17 06:07:19,466 PID:41482 INFO base.py __init__] ActorCritic:
- agent = <slm_lab.agent.Agent object at 0x7f0478ec0630>
- action_pdtype = Categorical
- action_policy = <function default at 0x7f0488caa1e0>
- explore_var_spec = None
- entropy_coef_spec = {'end_step': 0,
 'end_val': 0.01,
 'name': 'no_decay',
 'start_step': 0,
 'start_val': 0.01}
- policy_loss_coef = 1.0
- val_loss_coef = 0.5
- gamma = 0.99
- lam = None
- num_step_returns = 0.99
- training_frequency = 0.99
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f0478ec05f8>
- entropy_coef_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f0478ec08d0>
- calc_advs_v_targets = <bound method ActorCritic.calc_nstep_advs_v_targets of <slm_lab.agent.algorithm.actor_critic.ActorCritic object at 0x7f0478ec0588>>
- shared = True
- net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tails): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=10, bias=True)
      (1): Softmax(dim=None)
    )
    (1): Sequential(
      (0): Linear(in_features=512, out_features=1, bias=True)
      (1): Softmax(dim=None)
    )
  )
  (loss_fn): MSELoss()
)
- net_names = ['net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7f0478e7f0f0>
- global_net = None
[2021-06-17 06:07:19,466 PID:41483 INFO base.py __init__] ActorCritic:
- agent = <slm_lab.agent.Agent object at 0x7ff532b385c0>
- action_pdtype = Categorical
- action_policy = <function default at 0x7ff5429211e0>
- explore_var_spec = None
- entropy_coef_spec = {'end_step': 0,
 'end_val': 0.01,
 'name': 'no_decay',
 'start_step': 0,
 'start_val': 0.01}
- policy_loss_coef = 1.0
- val_loss_coef = 0.5
- gamma = 0.99
- lam = None
- num_step_returns = 0.99
- training_frequency = 0.99
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7ff53a828390>
- entropy_coef_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7ff532b38588>
- calc_advs_v_targets = <bound method ActorCritic.calc_nstep_advs_v_targets of <slm_lab.agent.algorithm.actor_critic.ActorCritic object at 0x7ff532b38518>>
- shared = True
- net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tails): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=10, bias=True)
      (1): Softmax(dim=None)
    )
    (1): Sequential(
      (0): Linear(in_features=512, out_features=1, bias=True)
      (1): Softmax(dim=None)
    )
  )
  (loss_fn): MSELoss()
)
- net_names = ['net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7ff532afe080>
- global_net = None
 ActorCritic:
- agent = <slm_lab.agent.Agent object at 0x7fe5b41c06d8>
- action_pdtype = Categorical
- action_policy = <function default at 0x7fe5c3faa1e0>
- explore_var_spec = None
- entropy_coef_spec = {'end_step': 0,
 'end_val': 0.01,
 'name': 'no_decay',
 'start_step': 0,
 'start_val': 0.01}
- policy_loss_coef = 1.0
- val_loss_coef = 0.5
- gamma = 0.99
- lam = None
- num_step_returns = 0.99
- training_frequency = 0.99
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7fe5b41c0b70>
- entropy_coef_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7fe5b41c0898>
- calc_advs_v_targets = <bound method ActorCritic.calc_nstep_advs_v_targets of <slm_lab.agent.algorithm.actor_critic.ActorCritic object at 0x7fe5b41c05f8>>
- shared = True
- net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tails): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=10, bias=True)
      (1): Softmax(dim=None)
    )
    (1): Sequential(
      (0): Linear(in_features=512, out_features=1, bias=True)
      (1): Softmax(dim=None)
    )
  )
  (loss_fn): MSELoss()
)
- net_names = ['net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7fe5b417f0b8>
- global_net = None
 ActorCritic:
- agent = <slm_lab.agent.Agent object at 0x7f16a6b2a630>
- action_pdtype = Categorical
- action_policy = <function default at 0x7f16b69131e0>
- explore_var_spec = None
- entropy_coef_spec = {'end_step': 0,
 'end_val': 0.01,
 'name': 'no_decay',
 'start_step': 0,
 'start_val': 0.01}
- policy_loss_coef = 1.0
- val_loss_coef = 0.5
- gamma = 0.99
- lam = None
- num_step_returns = 0.99
- training_frequency = 0.99
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f16a6b2a5f8>
- entropy_coef_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f16a6b2a8d0>
- calc_advs_v_targets = <bound method ActorCritic.calc_nstep_advs_v_targets of <slm_lab.agent.algorithm.actor_critic.ActorCritic object at 0x7f16a6b2a588>>
- shared = True
- net = ConvNet(
  (conv_model): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (fc_model): Sequential(
    (0): Linear(in_features=1568, out_features=512, bias=True)
    (1): ReLU()
  )
  (model_tails): ModuleList(
    (0): Sequential(
      (0): Linear(in_features=512, out_features=10, bias=True)
      (1): Softmax(dim=None)
    )
    (1): Sequential(
      (0): Linear(in_features=512, out_features=1, bias=True)
      (1): Softmax(dim=None)
    )
  )
  (loss_fn): MSELoss()
)
- net_names = ['net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)
- lr_scheduler = <slm_lab.agent.net.net_util.NoOpLRScheduler object at 0x7f16a6ae90b8>
- global_net = None
[2021-06-17 06:07:19,468 PID:41481 INFO __init__.py __init__][2021-06-17 06:07:19,468 PID:41484 INFO __init__.py __init__] Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_060651',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/A2C_nstep_2021_06_17_060651/graph/A2C_nstep_t0_s0',
 'info_prepath': 'data/A2C_nstep_2021_06_17_060651/info/A2C_nstep_t0_s0',
 'log_prepath': 'data/A2C_nstep_2021_06_17_060651/log/A2C_nstep_t0_s0',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/A2C_nstep_2021_06_17_060651/model/A2C_nstep_t0_s0',
 'prepath': 'data/A2C_nstep_2021_06_17_060651/A2C_nstep_t0_s0',
 'random_seed': 1623899213,
 'resume': False,
 'rigorous_eval': 0,
 'session': 0,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'default',
               'center_return': True,
               'entropy_coef_spec': {'end_step': 0,
                                     'end_val': 0.01,
                                     'name': 'no_decay',
                                     'start_step': 0,
                                     'start_val': 0.01},
               'gamma': 0.99,
               'lam': None,
               'name': 'ActorCritic',
               'num_step_returns': 0.99,
               'training_frequency': 5,
               'val_loss_coef': 0.5},
 'memory': {'name': 'OnPolicyBatchReplay'},
 'name': 'A2C_nstep',
 'net': {'actor_optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'batch_norm': False,
         'clip_grad_val': 0.5,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'critic_optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': 'orthogonal_',
         'loss_spec': {'name': 'MSELoss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'ConvNet',
         'use_same_optim': False}}
- name = A2C_nstep
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7ff532b385c0>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7ff5cb6f6048>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": 0.01,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.onpolicy.OnPolicyBatchReplay object at 0x7ff532b387b8>"
}
- algorithm = <slm_lab.agent.algorithm.actor_critic.ActorCritic object at 0x7ff532b38518>
 Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_060651',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/A2C_nstep_2021_06_17_060651/graph/A2C_nstep_t0_s3',
 'info_prepath': 'data/A2C_nstep_2021_06_17_060651/info/A2C_nstep_t0_s3',
 'log_prepath': 'data/A2C_nstep_2021_06_17_060651/log/A2C_nstep_t0_s3',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/A2C_nstep_2021_06_17_060651/model/A2C_nstep_t0_s3',
 'prepath': 'data/A2C_nstep_2021_06_17_060651/A2C_nstep_t0_s3',
 'random_seed': 1623902213,
 'resume': False,
 'rigorous_eval': 0,
 'session': 3,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'default',
               'center_return': True,
               'entropy_coef_spec': {'end_step': 0,
                                     'end_val': 0.01,
                                     'name': 'no_decay',
                                     'start_step': 0,
                                     'start_val': 0.01},
               'gamma': 0.99,
               'lam': None,
               'name': 'ActorCritic',
               'num_step_returns': 0.99,
               'training_frequency': 5,
               'val_loss_coef': 0.5},
 'memory': {'name': 'OnPolicyBatchReplay'},
 'name': 'A2C_nstep',
 'net': {'actor_optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'batch_norm': False,
         'clip_grad_val': 0.5,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'critic_optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': 'orthogonal_',
         'loss_spec': {'name': 'MSELoss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'ConvNet',
         'use_same_optim': False}}
- name = A2C_nstep
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7f0478ec0630>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7f04dce85e80>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": 0.01,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.onpolicy.OnPolicyBatchReplay object at 0x7f0478ec0860>"
}
- algorithm = <slm_lab.agent.algorithm.actor_critic.ActorCritic object at 0x7f0478ec0588>[2021-06-17 06:07:19,468 PID:41482 INFO __init__.py __init__]
[2021-06-17 06:07:19,468 PID:41483 INFO __init__.py __init__] Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_060651',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/A2C_nstep_2021_06_17_060651/graph/A2C_nstep_t0_s1',
 'info_prepath': 'data/A2C_nstep_2021_06_17_060651/info/A2C_nstep_t0_s1',
 'log_prepath': 'data/A2C_nstep_2021_06_17_060651/log/A2C_nstep_t0_s1',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/A2C_nstep_2021_06_17_060651/model/A2C_nstep_t0_s1',
 'prepath': 'data/A2C_nstep_2021_06_17_060651/A2C_nstep_t0_s1',
 'random_seed': 1623900213,
 'resume': False,
 'rigorous_eval': 0,
 'session': 1,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'default',
               'center_return': True,
               'entropy_coef_spec': {'end_step': 0,
                                     'end_val': 0.01,
                                     'name': 'no_decay',
                                     'start_step': 0,
                                     'start_val': 0.01},
               'gamma': 0.99,
               'lam': None,
               'name': 'ActorCritic',
               'num_step_returns': 0.99,
               'training_frequency': 5,
               'val_loss_coef': 0.5},
 'memory': {'name': 'OnPolicyBatchReplay'},
 'name': 'A2C_nstep',
 'net': {'actor_optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'batch_norm': False,
         'clip_grad_val': 0.5,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'critic_optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': 'orthogonal_',
         'loss_spec': {'name': 'MSELoss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'ConvNet',
         'use_same_optim': False}}
- name = A2C_nstep
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7fe5b41c06d8>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7fe618123898>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": 0.01,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.onpolicy.OnPolicyBatchReplay object at 0x7fe5b41c0748>"
}
- algorithm = <slm_lab.agent.algorithm.actor_critic.ActorCritic object at 0x7fe5b41c05f8>
 Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_060651',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/A2C_nstep_2021_06_17_060651/graph/A2C_nstep_t0_s2',
 'info_prepath': 'data/A2C_nstep_2021_06_17_060651/info/A2C_nstep_t0_s2',
 'log_prepath': 'data/A2C_nstep_2021_06_17_060651/log/A2C_nstep_t0_s2',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/A2C_nstep_2021_06_17_060651/model/A2C_nstep_t0_s2',
 'prepath': 'data/A2C_nstep_2021_06_17_060651/A2C_nstep_t0_s2',
 'random_seed': 1623901213,
 'resume': False,
 'rigorous_eval': 0,
 'session': 2,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Categorical',
               'action_policy': 'default',
               'center_return': True,
               'entropy_coef_spec': {'end_step': 0,
                                     'end_val': 0.01,
                                     'name': 'no_decay',
                                     'start_step': 0,
                                     'start_val': 0.01},
               'gamma': 0.99,
               'lam': None,
               'name': 'ActorCritic',
               'num_step_returns': 0.99,
               'training_frequency': 5,
               'val_loss_coef': 0.5},
 'memory': {'name': 'OnPolicyBatchReplay'},
 'name': 'A2C_nstep',
 'net': {'actor_optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'batch_norm': False,
         'clip_grad_val': 0.5,
         'conv_hid_layers': [[32, 8, 4, 0, 1],
                             [64, 4, 2, 0, 1],
                             [32, 3, 1, 0, 1]],
         'critic_optim_spec': {'lr': 0.0001, 'name': 'Adam'},
         'cuda_id': 0,
         'fc_hid_layers': [512],
         'gpu': True,
         'hid_layers_activation': 'relu',
         'init_fn': 'orthogonal_',
         'loss_spec': {'name': 'MSELoss'},
         'lr_scheduler_spec': None,
         'normalize': True,
         'out_layer_activation': 'softmax',
         'shared': True,
         'type': 'ConvNet',
         'use_same_optim': False}}
- name = A2C_nstep
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7f16a6b2a630>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7f170aad6e48>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": NaN,
  "entropy_coef": 0.01,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(4, 84, 84)",
  "action_space": "Discrete(10)",
  "observable_dim": {
    "state": [
      4,
      84,
      84
    ]
  },
  "state_dim": "(4, 84, 84)",
  "action_dim": 10,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Categorical",
  "ActionPD": "<class 'torch.distributions.categorical.Categorical'>",
  "memory": "<slm_lab.agent.memory.onpolicy.OnPolicyBatchReplay object at 0x7f16a6b2a860>"
}
- algorithm = <slm_lab.agent.algorithm.actor_critic.ActorCritic object at 0x7f16a6b2a588>
[2021-06-17 06:07:19,479 PID:41481 INFO logger.py info][2021-06-17 06:07:19,479 PID:41484 INFO logger.py info][2021-06-17 06:07:19,479 PID:41482 INFO logger.py info] Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_060651',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/A2C_nstep_2021_06_17_060651/graph/A2C_nstep_t0_s3',
 'info_prepath': 'data/A2C_nstep_2021_06_17_060651/info/A2C_nstep_t0_s3',
 'log_prepath': 'data/A2C_nstep_2021_06_17_060651/log/A2C_nstep_t0_s3',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/A2C_nstep_2021_06_17_060651/model/A2C_nstep_t0_s3',
 'prepath': 'data/A2C_nstep_2021_06_17_060651/A2C_nstep_t0_s3',
 'random_seed': 1623902213,
 'resume': False,
 'rigorous_eval': 0,
 'session': 3,
 'trial': 0}
- index = 3
- agent = <slm_lab.agent.Agent object at 0x7f0478ec0630>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7f04dce85e80>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7f04dce85e80> Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_060651',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/A2C_nstep_2021_06_17_060651/graph/A2C_nstep_t0_s0',
 'info_prepath': 'data/A2C_nstep_2021_06_17_060651/info/A2C_nstep_t0_s0',
 'log_prepath': 'data/A2C_nstep_2021_06_17_060651/log/A2C_nstep_t0_s0',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/A2C_nstep_2021_06_17_060651/model/A2C_nstep_t0_s0',
 'prepath': 'data/A2C_nstep_2021_06_17_060651/A2C_nstep_t0_s0',
 'random_seed': 1623899213,
 'resume': False,
 'rigorous_eval': 0,
 'session': 0,
 'trial': 0}
- index = 0
- agent = <slm_lab.agent.Agent object at 0x7ff532b385c0>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7ff5cb6f6048>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7ff5cb6f6048>
 Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_060651',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/A2C_nstep_2021_06_17_060651/graph/A2C_nstep_t0_s1',
 'info_prepath': 'data/A2C_nstep_2021_06_17_060651/info/A2C_nstep_t0_s1',
 'log_prepath': 'data/A2C_nstep_2021_06_17_060651/log/A2C_nstep_t0_s1',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/A2C_nstep_2021_06_17_060651/model/A2C_nstep_t0_s1',
 'prepath': 'data/A2C_nstep_2021_06_17_060651/A2C_nstep_t0_s1',
 'random_seed': 1623900213,
 'resume': False,
 'rigorous_eval': 0,
 'session': 1,
 'trial': 0}
- index = 1
- agent = <slm_lab.agent.Agent object at 0x7fe5b41c06d8>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7fe618123898>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7fe618123898>

[2021-06-17 06:07:19,479 PID:41483 INFO logger.py info] Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 5000,
 'experiment': 0,
 'experiment_ts': '2021_06_17_060651',
 'git_sha': 'a31aefc43fd1ab5e531676d86e5dd2e25c4f7101',
 'graph_prepath': 'data/A2C_nstep_2021_06_17_060651/graph/A2C_nstep_t0_s2',
 'info_prepath': 'data/A2C_nstep_2021_06_17_060651/info/A2C_nstep_t0_s2',
 'log_prepath': 'data/A2C_nstep_2021_06_17_060651/log/A2C_nstep_t0_s2',
 'max_session': 4,
 'max_trial': 1,
 'model_prepath': 'data/A2C_nstep_2021_06_17_060651/model/A2C_nstep_t0_s2',
 'prepath': 'data/A2C_nstep_2021_06_17_060651/A2C_nstep_t0_s2',
 'random_seed': 1623901213,
 'resume': False,
 'rigorous_eval': 0,
 'session': 2,
 'trial': 0}
- index = 2
- agent = <slm_lab.agent.Agent object at 0x7f16a6b2a630>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7f170aad6e48>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7f170aad6e48>
[2021-06-17 06:07:19,479 PID:41484 INFO logger.py info][2021-06-17 06:07:19,479 PID:41481 INFO logger.py info] Running RL loop for trial 0 session 3
 Running RL loop for trial 0 session 0[2021-06-17 06:07:19,479 PID:41482 INFO logger.py info]
 Running RL loop for trial 0 session 1
[2021-06-17 06:07:19,479 PID:41483 INFO logger.py info] Running RL loop for trial 0 session 2
[2021-06-17 06:07:21,791 PID:41482 INFO __init__.py log_summary][2021-06-17 06:07:21,791 PID:41484 INFO __init__.py log_summary][2021-06-17 06:07:21,791 PID:41483 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:07:21,791 PID:41481 INFO __init__.py log_summary]
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:09:24,391 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 10000  wall_t: 132  opt_step: 0  frame: 10000  fps: 75.7576  total_reward: 400  total_reward_ma: 400  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:09:24,374 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 10000  wall_t: 132  opt_step: 0  frame: 10000  fps: 75.7576  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:09:24,452 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 10000  wall_t: 132  opt_step: 0  frame: 10000  fps: 75.7576  total_reward: 133.333  total_reward_ma: 133.333  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:09:24,350 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 10000  wall_t: 132  opt_step: 0  frame: 10000  fps: 75.7576  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:10:23,424 PID:41482 INFO __init__.py log_summary][2021-06-17 06:10:23,424 PID:41484 INFO __init__.py log_summary][2021-06-17 06:10:23,424 PID:41483 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 20000  wall_t: 197  opt_step: 0  frame: 20000  fps: 101.523  total_reward: 260  total_reward_ma: 196.667  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 20000  wall_t: 202  opt_step: 0  frame: 20000  fps: 99.0099  total_reward: 350  total_reward_ma: 375  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 20000  wall_t: 200  opt_step: 0  frame: 20000  fps: 100  total_reward: 520  total_reward_ma: 520  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan

[2021-06-17 06:10:23,424 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 20000  wall_t: 202  opt_step: 0  frame: 20000  fps: 99.0099  total_reward: 360  total_reward_ma: 360  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:11:20,007 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 196.667  strength: -446.333  max_strength: -383  final_strength: -383  sample_efficiency: 7.85474e-05  training_efficiency: -0  stability: 1
[2021-06-17 06:11:20,186 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 360  strength: -283  max_strength: -283  final_strength: -283  sample_efficiency: 5e-05  training_efficiency: -0  stability: nan
[2021-06-17 06:11:20,204 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 520  strength: -123  max_strength: -123  final_strength: -123  sample_efficiency: 5e-05  training_efficiency: -0  stability: nan
[2021-06-17 06:11:20,268 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 375  strength: -268  max_strength: -243  final_strength: -293  sample_efficiency: 7.26679e-05  training_efficiency: -0  stability: 0.794239
[2021-06-17 06:12:38,296 PID:41482 INFO __init__.py log_summary][2021-06-17 06:12:38,296 PID:41484 INFO __init__.py log_summary][2021-06-17 06:12:38,296 PID:41481 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 30000  wall_t: 341  opt_step: 0  frame: 30000  fps: 87.9765  total_reward: 616.667  total_reward_ma: 336.667  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 30000  wall_t: 337  opt_step: 0  frame: 30000  fps: 89.0208  total_reward: 266.667  total_reward_ma: 338.889  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:12:38,296 PID:41483 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 30000  wall_t: 338  opt_step: 0  frame: 30000  fps: 88.7574  total_reward: 266.667  total_reward_ma: 313.333  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan

 Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 30000  wall_t: 339  opt_step: 0  frame: 30000  fps: 88.4956  total_reward: 657.143  total_reward_ma: 588.571  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:12:49,752 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 336.667  strength: -306.333  max_strength: -26.3333  final_strength: -26.3333  sample_efficiency: 7.72518e-05  training_efficiency: -0  stability: 1[2021-06-17 06:12:49,752 PID:41484 INFO __init__.py log_metrics][2021-06-17 06:12:49,752 PID:41483 INFO __init__.py log_metrics]
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 338.889  strength: -304.111  max_strength: -243  final_strength: -376.333  sample_efficiency: 5.64426e-05  training_efficiency: -0  stability: 0.751244 Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 588.571  strength: -54.4286  max_strength: 14.1429  final_strength: 14.1429  sample_efficiency: 5.21654e-05  training_efficiency: -0  stability: 1

[2021-06-17 06:12:49,752 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 313.333  strength: -329.667  max_strength: -283  final_strength: -376.333  sample_efficiency: 4.0487e-05  training_efficiency: -0  stability: 0.6702
[2021-06-17 06:13:59,245 PID:41483 INFO __init__.py log_summary][2021-06-17 06:13:59,245 PID:41481 INFO __init__.py log_summary][2021-06-17 06:13:59,245 PID:41482 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 40000  wall_t: 407  opt_step: 0  frame: 40000  fps: 98.2801  total_reward: 700  total_reward_ma: 442.222  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 40000  wall_t: 408  opt_step: 0  frame: 40000  fps: 98.0392  total_reward: 337.5  total_reward_ma: 504.881  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:13:59,245 PID:41484 INFO __init__.py log_summary]

 Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 40000  wall_t: 407  opt_step: 0  frame: 40000  fps: 98.2801  total_reward: 757.143  total_reward_ma: 441.786  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 40000  wall_t: 406  opt_step: 0  frame: 40000  fps: 98.5222  total_reward: 750  total_reward_ma: 441.667  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:14:18,038 PID:41481 INFO __init__.py log_metrics][2021-06-17 06:14:18,038 PID:41483 INFO __init__.py log_metrics][2021-06-17 06:14:18,038 PID:41484 INFO __init__.py log_metrics][2021-06-17 06:14:18,038 PID:41482 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 504.881  strength: -138.119  max_strength: 14.1429  final_strength: -305.5  sample_efficiency: 3.21367e-05  training_efficiency: -0  stability: -1.93635 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 441.667  strength: -201.333  max_strength: 107  final_strength: 107  sample_efficiency: 6.06202e-05  training_efficiency: -0  stability: 0.853855 Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 442.222  strength: -200.778  max_strength: 57  final_strength: 57  sample_efficiency: 4.19526e-05  training_efficiency: -0  stability: 0.858443


 Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 441.786  strength: -201.214  max_strength: 114.143  final_strength: 114.143  sample_efficiency: 8.46621e-05  training_efficiency: -0  stability: 1
[2021-06-17 06:14:51,197 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 50000  wall_t: 467  opt_step: 0  frame: 50000  fps: 107.066  total_reward: 512.5  total_reward_ma: 455.929  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:14:51,158 PID:41484 INFO __init__.py log_summary][2021-06-17 06:14:51,158 PID:41483 INFO __init__.py log_summary]
[2021-06-17 06:14:51,158 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 50000  wall_t: 467  opt_step: 0  frame: 50000  fps: 107.066  total_reward: 437.5  total_reward_ma: 441.042  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 50000  wall_t: 467  opt_step: 0  frame: 50000  fps: 107.066  total_reward: 600  total_reward_ma: 473.333  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 50000  wall_t: 467  opt_step: 0  frame: 50000  fps: 107.066  total_reward: 400  total_reward_ma: 478.661  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:15:01,931 PID:41483 INFO __init__.py log_metrics][2021-06-17 06:15:01,931 PID:41481 INFO __init__.py log_metrics][2021-06-17 06:15:01,931 PID:41482 INFO __init__.py log_metrics][2021-06-17 06:15:01,931 PID:41484 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 441.042  strength: -201.958  max_strength: 57  final_strength: -205.5  sample_efficiency: 3.63682e-05  training_efficiency: -0  stability: 0.409242 Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 478.661  strength: -164.339  max_strength: 14.1429  final_strength: -243  sample_efficiency: 2.76502e-05  training_efficiency: -0  stability: 0.228581

 Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 455.929  strength: -187.071  max_strength: 114.143  final_strength: -130.5  sample_efficiency: 7.56405e-05  training_efficiency: -0  stability: 0.696042 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 473.333  strength: -169.667  max_strength: 107  final_strength: -43  sample_efficiency: 5.85612e-05  training_efficiency: -0  stability: 0.648179

[2021-06-17 06:15:30,508 PID:41484 INFO __init__.py log_summary][2021-06-17 06:15:30,508 PID:41482 INFO __init__.py log_summary][2021-06-17 06:15:30,508 PID:41483 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 60000  wall_t: 509  opt_step: 0  frame: 60000  fps: 117.878  total_reward: 537.5  total_reward_ma: 484.028  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 60000  wall_t: 509  opt_step: 0  frame: 60000  fps: 117.878  total_reward: 325  total_reward_ma: 434.107  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 60000  wall_t: 509  opt_step: 0  frame: 60000  fps: 117.878  total_reward: 525  total_reward_ma: 487.929  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:15:30,508 PID:41481 INFO __init__.py log_summary]

 Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 60000  wall_t: 509  opt_step: 0  frame: 60000  fps: 117.878  total_reward: 700  total_reward_ma: 492.833  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:15:48,575 PID:41482 INFO __init__.py log_metrics][2021-06-17 06:15:48,575 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 484.028  strength: -158.972  max_strength: 107  final_strength: -105.5  sample_efficiency: 5.39274e-05  training_efficiency: -0  stability: 0.592338 Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 434.107  strength: -208.893  max_strength: 114.143  final_strength: -318  sample_efficiency: 6.06777e-05  training_efficiency: -0  stability: 0.537992
[2021-06-17 06:15:48,575 PID:41483 INFO __init__.py log_metrics]
 Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 487.929  strength: -155.071  max_strength: 14.1429  final_strength: -118  sample_efficiency: 2.59787e-05  training_efficiency: -0  stability: 0.513745
[2021-06-17 06:15:48,575 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 492.833  strength: -150.167  max_strength: 57  final_strength: 57  sample_efficiency: 3.78639e-05  training_efficiency: -0  stability: 0.559521
[2021-06-17 06:17:26,742 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 70000  wall_t: 619  opt_step: 0  frame: 70000  fps: 113.086  total_reward: 912.5  total_reward_ma: 562.778  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:17:26,753 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 70000  wall_t: 616  opt_step: 0  frame: 70000  fps: 113.636  total_reward: 862.5  total_reward_ma: 550.357  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:17:26,893 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 70000  wall_t: 617  opt_step: 0  frame: 70000  fps: 113.452  total_reward: 587.5  total_reward_ma: 498.81  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:17:27,105 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 70000  wall_t: 619  opt_step: 0  frame: 70000  fps: 113.086  total_reward: 750  total_reward_ma: 479.235  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:17:40,263 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 479.235  strength: -163.765  max_strength: 114.143  final_strength: 107  sample_efficiency: 6.50079e-05  training_efficiency: -0  stability: 0.655212[2021-06-17 06:17:40,263 PID:41484 INFO __init__.py log_metrics]
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 498.81  strength: -144.19  max_strength: 107  final_strength: -55.5  sample_efficiency: 5.17477e-05  training_efficiency: -0  stability: 0.637428
[2021-06-17 06:17:40,263 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 550.357  strength: -92.6429  max_strength: 219.5  final_strength: 219.5  sample_efficiency: 3.0596e-05  training_efficiency: -0  stability: 0.587748
[2021-06-17 06:17:40,365 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 562.778  strength: -80.2222  max_strength: 269.5  final_strength: 269.5  sample_efficiency: 5.10653e-05  training_efficiency: -0  stability: 0.526082
[2021-06-17 06:18:24,003 PID:41481 INFO __init__.py log_summary][2021-06-17 06:18:24,003 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 80000  wall_t: 686  opt_step: 0  frame: 80000  fps: 116.618  total_reward: 575  total_reward_ma: 508.333  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 80000  wall_t: 685  opt_step: 0  frame: 80000  fps: 116.788  total_reward: 1112.5  total_reward_ma: 641.31  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:18:24,003 PID:41482 INFO __init__.py log_summary]

[2021-06-17 06:18:24,003 PID:41483 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 80000  wall_t: 680  opt_step: 0  frame: 80000  fps: 117.647  total_reward: 462.5  total_reward_ma: 477.143  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 80000  wall_t: 684  opt_step: 0  frame: 80000  fps: 116.959  total_reward: 850  total_reward_ma: 593.163  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:18:45,777 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 593.163  strength: -49.8367  max_strength: 219.5  final_strength: 207  sample_efficiency: 4.13336e-05  training_efficiency: -0  stability: 0.402467
[2021-06-17 06:18:45,841 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 508.333  strength: -134.667  max_strength: 107  final_strength: -68  sample_efficiency: 4.92704e-05  training_efficiency: -0  stability: 0.64498
[2021-06-17 06:18:45,852 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 477.143  strength: -165.857  max_strength: 114.143  final_strength: -180.5  sample_efficiency: 5.7865e-05  training_efficiency: -0  stability: 0.372235
[2021-06-17 06:18:45,958 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 641.31  strength: -1.69048  max_strength: 469.5  final_strength: 469.5  sample_efficiency: 0.00158118  training_efficiency: -0  stability: 0.260734
[2021-06-17 06:19:46,923 PID:41481 INFO __init__.py log_summary][2021-06-17 06:19:46,895 PID:41482 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 90000  wall_t: 763  opt_step: 0  frame: 90000  fps: 117.955  total_reward: 725  total_reward_ma: 651.771  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:19:46,966 PID:41484 INFO __init__.py log_summary]
 Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 90000  wall_t: 765  opt_step: 0  frame: 90000  fps: 117.647  total_reward: 525  total_reward_ma: 482.46  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:19:46,953 PID:41483 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 90000  wall_t: 765  opt_step: 0  frame: 90000  fps: 117.647  total_reward: 712.5  total_reward_ma: 531.018  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan

 Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 90000  wall_t: 761  opt_step: 0  frame: 90000  fps: 118.265  total_reward: 725  total_reward_ma: 609.643  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:20:05,638 PID:41483 INFO __init__.py log_metrics][2021-06-17 06:20:05,638 PID:41484 INFO __init__.py log_metrics][2021-06-17 06:20:05,638 PID:41482 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 609.643  strength: -33.3571  max_strength: 219.5  final_strength: 82  sample_efficiency: 5.06204e-05  training_efficiency: -0  stability: -0.310401 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 531.018  strength: -111.981  max_strength: 107  final_strength: 69.5  sample_efficiency: 5.19018e-05  training_efficiency: -0  stability: 0.667389
 Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 482.46  strength: -160.54  max_strength: 114.143  final_strength: -118  sample_efficiency: 5.40466e-05  training_efficiency: -0  stability: 0.457633

[2021-06-17 06:20:05,638 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 651.771  strength: 8.77083  max_strength: 469.5  final_strength: 82  sample_efficiency: -0.000253676  training_efficiency: 0  stability: -61.8167
[2021-06-17 06:21:42,744 PID:41484 INFO __init__.py log_summary][2021-06-17 06:21:42,744 PID:41481 INFO __init__.py log_summary][2021-06-17 06:21:42,751 PID:41483 INFO __init__.py log_summary][2021-06-17 06:21:42,754 PID:41482 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 100000  wall_t: 881  opt_step: 0  frame: 100000  fps: 113.507  total_reward: 587.5  total_reward_ma: 536.667  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 100000  wall_t: 880  opt_step: 0  frame: 100000  fps: 113.636  total_reward: 687.5  total_reward_ma: 618.294  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 100000  wall_t: 880  opt_step: 0  frame: 100000  fps: 113.636  total_reward: 387.5  total_reward_ma: 472.964  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 100000  wall_t: 880  opt_step: 0  frame: 100000  fps: 113.636  total_reward: 662.5  total_reward_ma: 652.963  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:22:07,862 PID:41481 INFO __init__.py log_metrics][2021-06-17 06:22:07,862 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 472.964  strength: -170.036  max_strength: 114.143  final_strength: -255.5  sample_efficiency: 4.74281e-05  training_efficiency: -0  stability: 0.406763
[2021-06-17 06:22:07,862 PID:41483 INFO __init__.py log_metrics][2021-06-17 06:22:07,863 PID:41484 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 618.294  strength: -24.7063  max_strength: 219.5  final_strength: 44.5  sample_efficiency: 5.87497e-05  training_efficiency: -0  stability: -0.853587
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 536.667  strength: -106.333  max_strength: 107  final_strength: -55.5  sample_efficiency: 4.97148e-05  training_efficiency: -0  stability: 0.520423
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 652.963  strength: 9.96296  max_strength: 469.5  final_strength: 19.5  sample_efficiency: -0.000196334  training_efficiency: 0  stability: -10.4846
[2021-06-17 06:22:52,795 PID:41484 INFO __init__.py log_summary][2021-06-17 06:22:52,795 PID:41483 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 110000  wall_t: 952  opt_step: 0  frame: 110000  fps: 115.546  total_reward: 712.5  total_reward_ma: 552.651  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:22:52,795 PID:41481 INFO __init__.py log_summary]
[2021-06-17 06:22:52,819 PID:41482 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 110000  wall_t: 955  opt_step: 0  frame: 110000  fps: 115.183  total_reward: 687.5  total_reward_ma: 656.417  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 110000  wall_t: 953  opt_step: 0  frame: 110000  fps: 115.425  total_reward: 437.5  total_reward_ma: 469.74  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 110000  wall_t: 954  opt_step: 0  frame: 110000  fps: 115.304  total_reward: 700  total_reward_ma: 626.464  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:23:08,979 PID:41483 INFO __init__.py log_metrics][2021-06-17 06:23:08,979 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 552.651  strength: -90.3485  max_strength: 107  final_strength: 69.5  sample_efficiency: 5.25557e-05  training_efficiency: -0  stability: 0.545455
[2021-06-17 06:23:08,979 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 469.74  strength: -173.26  max_strength: 114.143  final_strength: -205.5  sample_efficiency: 4.32943e-05  training_efficiency: -0  stability: 0.495904
 Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 626.464  strength: -16.5357  max_strength: 219.5  final_strength: 57  sample_efficiency: 7.58675e-05  training_efficiency: -0  stability: -1.22454
[2021-06-17 06:23:08,979 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 656.417  strength: 13.4167  max_strength: 469.5  final_strength: 44.5  sample_efficiency: -0.000128199  training_efficiency: 0  stability: -7.98699
[2021-06-17 06:23:56,378 PID:41484 INFO __init__.py log_summary][2021-06-17 06:23:56,321 PID:41482 INFO __init__.py log_summary][2021-06-17 06:23:56,321 PID:41481 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 120000  wall_t: 1019  opt_step: 0  frame: 120000  fps: 117.763  total_reward: 1137.5  total_reward_ma: 525.387  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:23:56,375 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 120000  wall_t: 1019  opt_step: 0  frame: 120000  fps: 117.763  total_reward: 500  total_reward_ma: 614.968  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 120000  wall_t: 1016  opt_step: 0  frame: 120000  fps: 118.11  total_reward: 662.5  total_reward_ma: 656.97  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 120000  wall_t: 1015  opt_step: 0  frame: 120000  fps: 118.227  total_reward: 725  total_reward_ma: 567.014  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:24:20,841 PID:41481 INFO __init__.py log_metrics][2021-06-17 06:24:20,841 PID:41482 INFO __init__.py log_metrics][2021-06-17 06:24:20,841 PID:41484 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 525.387  strength: -117.613  max_strength: 494.5  final_strength: 494.5  sample_efficiency: 5.55437e-05  training_efficiency: -0  stability: 0.550259[2021-06-17 06:24:20,841 PID:41483 INFO __init__.py log_metrics]
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 656.97  strength: 13.9697  max_strength: 469.5  final_strength: 19.5  sample_efficiency: -0.000110873  training_efficiency: 0  stability: -5.19255 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 567.014  strength: -75.9861  max_strength: 107  final_strength: 82  sample_efficiency: 5.65325e-05  training_efficiency: -0  stability: 0.513668
 Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 614.968  strength: -28.0325  max_strength: 219.5  final_strength: -143  sample_efficiency: 4.45487e-05  training_efficiency: -0  stability: -3.20086

[2021-06-17 06:25:07,728 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 130000  wall_t: 1083  opt_step: 0  frame: 130000  fps: 120.037  total_reward: 375  total_reward_ma: 633.472  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:25:07,857 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 130000  wall_t: 1083  opt_step: 0  frame: 130000  fps: 120.037  total_reward: 737.5  total_reward_ma: 625.179  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:25:07,811 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 130000  wall_t: 1083  opt_step: 0  frame: 130000  fps: 120.037  total_reward: 562.5  total_reward_ma: 528.242  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:25:07,958 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 130000  wall_t: 1082  opt_step: 0  frame: 130000  fps: 120.148  total_reward: 912.5  total_reward_ma: 593.59  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:25:14,930 PID:41481 INFO __init__.py log_metrics][2021-06-17 06:25:14,930 PID:41482 INFO __init__.py log_metrics][2021-06-17 06:25:14,930 PID:41484 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 528.242  strength: -114.758  max_strength: 494.5  final_strength: -80.5  sample_efficiency: 5.29617e-05  training_efficiency: -0  stability: -0.0147275 Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 633.472  strength: -9.52778  max_strength: 469.5  final_strength: -268  sample_efficiency: 0.000167047  training_efficiency: -0  stability: -6.27766 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 593.59  strength: -49.4103  max_strength: 269.5  final_strength: 269.5  sample_efficiency: 7.70242e-05  training_efficiency: -0  stability: 0.469932


[2021-06-17 06:25:14,930 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 625.179  strength: -17.8214  max_strength: 219.5  final_strength: 94.5  sample_efficiency: 6.08349e-05  training_efficiency: -0  stability: -1.25272
[2021-06-17 06:25:46,816 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 140000  wall_t: 1131  opt_step: 0  frame: 140000  fps: 123.784  total_reward: 550  total_reward_ma: 529.796  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:25:46,816 PID:41483 INFO __init__.py log_summary][2021-06-17 06:25:46,838 PID:41481 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 140000  wall_t: 1130  opt_step: 0  frame: 140000  fps: 123.894  total_reward: 900  total_reward_ma: 646.319  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 140000  wall_t: 1130  opt_step: 0  frame: 140000  fps: 123.894  total_reward: 525  total_reward_ma: 625.128  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan

[2021-06-17 06:25:46,816 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 140000  wall_t: 1127  opt_step: 0  frame: 140000  fps: 124.224  total_reward: 662.5  total_reward_ma: 598.512  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:26:01,587 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 529.796  strength: -113.204  max_strength: 494.5  final_strength: -93  sample_efficiency: 5.0273e-05  training_efficiency: -0  stability: 0.031648
[2021-06-17 06:26:01,615 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 625.128  strength: -17.8718  max_strength: 469.5  final_strength: -118  sample_efficiency: 8.58333e-05  training_efficiency: -0  stability: -8.78134
[2021-06-17 06:26:01,659 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 598.512  strength: -44.4881  max_strength: 269.5  final_strength: 19.5  sample_efficiency: 7.9212e-05  training_efficiency: -0  stability: -0.141671
[2021-06-17 06:26:01,646 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 646.319  strength: 3.31868  max_strength: 257  final_strength: 257  sample_efficiency: -0.000259006  training_efficiency: 0  stability: -2.24816
[2021-06-17 06:26:36,594 PID:41484 INFO __init__.py log_summary][2021-06-17 06:26:36,594 PID:41483 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 150000  wall_t: 1168  opt_step: 0  frame: 150000  fps: 128.425  total_reward: 725  total_reward_ma: 606.944  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 150000  wall_t: 1171  opt_step: 0  frame: 150000  fps: 128.096  total_reward: 562.5  total_reward_ma: 640.332  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:26:36,594 PID:41482 INFO __init__.py log_summary]

[2021-06-17 06:26:36,594 PID:41481 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 150000  wall_t: 1168  opt_step: 0  frame: 150000  fps: 128.425  total_reward: 400  total_reward_ma: 521.143  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 150000  wall_t: 1173  opt_step: 0  frame: 150000  fps: 127.877  total_reward: 600  total_reward_ma: 623.333  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:26:52,178 PID:41482 INFO __init__.py log_metrics][2021-06-17 06:26:52,178 PID:41481 INFO __init__.py log_metrics][2021-06-17 06:26:52,178 PID:41484 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 623.333  strength: -19.6667  max_strength: 469.5  final_strength: -43  sample_efficiency: 7.34695e-05  training_efficiency: -0  stability: -3.81349 Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 521.143  strength: -121.857  max_strength: 494.5  final_strength: -243  sample_efficiency: 4.44759e-05  training_efficiency: -0  stability: -0.00617445
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 606.944  strength: -36.0556  max_strength: 269.5  final_strength: 82  sample_efficiency: 9.02112e-05  training_efficiency: -0  stability: -0.177415

[2021-06-17 06:26:52,354 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 640.332  strength: -2.66837  max_strength: 257  final_strength: -80.5  sample_efficiency: 0.000313486  training_efficiency: -0  stability: -22.9238
[2021-06-17 06:27:54,884 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 160000  wall_t: 1253  opt_step: 0  frame: 160000  fps: 127.694  total_reward: 587.5  total_reward_ma: 636.81  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:27:54,908 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 160000  wall_t: 1257  opt_step: 0  frame: 160000  fps: 127.287  total_reward: 575  total_reward_ma: 524.509  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:27:55,292 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 160000  wall_t: 1252  opt_step: 0  frame: 160000  fps: 127.796  total_reward: 500  total_reward_ma: 615.111  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:27:57,029 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 160000  wall_t: 1261  opt_step: 0  frame: 160000  fps: 126.883  total_reward: 575  total_reward_ma: 604.948  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:28:12,501 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 615.111  strength: -27.8889  max_strength: 469.5  final_strength: -143  sample_efficiency: 5.04917e-05  training_efficiency: -0  stability: -3.42494
[2021-06-17 06:28:12,502 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 524.509  strength: -118.491  max_strength: 494.5  final_strength: -68  sample_efficiency: 4.31048e-05  training_efficiency: -0  stability: 0.127589
[2021-06-17 06:28:12,503 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 604.948  strength: -38.0521  max_strength: 269.5  final_strength: -68  sample_efficiency: 8.08337e-05  training_efficiency: -0  stability: -0.633282
[2021-06-17 06:28:12,503 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 636.81  strength: -6.19047  max_strength: 257  final_strength: -55.5  sample_efficiency: 0.000129853  training_efficiency: -0  stability: -26.6291
[2021-06-17 06:28:48,921 PID:41481 INFO __init__.py log_summary][2021-06-17 06:28:48,984 PID:41482 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 170000  wall_t: 1307  opt_step: 0  frame: 170000  fps: 130.069  total_reward: 750  total_reward_ma: 623.542  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 170000  wall_t: 1305  opt_step: 0  frame: 170000  fps: 130.268  total_reward: 725  total_reward_ma: 536.302  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:28:48,942 PID:41483 INFO __init__.py log_summary][2021-06-17 06:28:49,111 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 170000  wall_t: 1303  opt_step: 0  frame: 170000  fps: 130.468  total_reward: 475  total_reward_ma: 597.304  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 170000  wall_t: 1298  opt_step: 0  frame: 170000  fps: 130.971  total_reward: 400  total_reward_ma: 622.009  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:29:02,947 PID:41483 INFO __init__.py log_metrics][2021-06-17 06:29:02,947 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 536.302  strength: -106.697  max_strength: 494.5  final_strength: 82  sample_efficiency: 4.47875e-05  training_efficiency: -0  stability: 0.15888
 Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 622.009  strength: -20.9911  max_strength: 257  final_strength: -243  sample_efficiency: 4.01576e-05  training_efficiency: -0  stability: -12.1346[2021-06-17 06:29:02,947 PID:41481 INFO __init__.py log_metrics]
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 623.542  strength: -19.4583  max_strength: 469.5  final_strength: 107  sample_efficiency: 6.58231e-05  training_efficiency: -0  stability: -1.91235
[2021-06-17 06:29:02,947 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 597.304  strength: -45.6961  max_strength: 269.5  final_strength: -168  sample_efficiency: 6.46245e-05  training_efficiency: -0  stability: -0.615111
[2021-06-17 06:29:43,795 PID:41483 INFO __init__.py log_summary][2021-06-17 06:29:43,816 PID:41484 INFO __init__.py log_summary][2021-06-17 06:29:43,805 PID:41481 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 180000  wall_t: 1360  opt_step: 0  frame: 180000  fps: 132.353  total_reward: 450  total_reward_ma: 611.891  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:29:43,859 PID:41482 INFO __init__.py log_summary]
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 180000  wall_t: 1364  opt_step: 0  frame: 180000  fps: 131.965  total_reward: 637.5  total_reward_ma: 624.363  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 180000  wall_t: 1367  opt_step: 0  frame: 180000  fps: 131.675  total_reward: 1487.5  total_reward_ma: 589.147  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 180000  wall_t: 1363  opt_step: 0  frame: 180000  fps: 132.062  total_reward: 475  total_reward_ma: 590.509  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:29:53,053 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 590.509  strength: -52.4907  max_strength: 269.5  final_strength: -168  sample_efficiency: 5.41215e-05  training_efficiency: -0  stability: -0.265823
[2021-06-17 06:29:53,126 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 589.147  strength: -53.8532  max_strength: 844.5  final_strength: 844.5  sample_efficiency: 7.89663e-05  training_efficiency: -0  stability: 0.120855
[2021-06-17 06:29:53,153 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 624.363  strength: -18.6373  max_strength: 469.5  final_strength: -5.5  sample_efficiency: 6.47769e-05  training_efficiency: -0  stability: -3.27462
[2021-06-17 06:29:53,267 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 611.891  strength: -31.1092  max_strength: 257  final_strength: -193  sample_efficiency: 2.753e-05  training_efficiency: -0  stability: -2.63143
[2021-06-17 06:30:58,879 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 190000  wall_t: 1438  opt_step: 0  frame: 190000  fps: 132.128  total_reward: 837.5  total_reward_ma: 636.204  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:30:59,037 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 190000  wall_t: 1433  opt_step: 0  frame: 190000  fps: 132.589  total_reward: 775  total_reward_ma: 600.219  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:30:58,928 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 190000  wall_t: 1430  opt_step: 0  frame: 190000  fps: 132.867  total_reward: 787.5  total_reward_ma: 621.647  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:30:59,004 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 190000  wall_t: 1428  opt_step: 0  frame: 190000  fps: 133.053  total_reward: 1087.5  total_reward_ma: 615.376  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:31:07,539 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 600.219  strength: -42.7807  max_strength: 269.5  final_strength: 132  sample_efficiency: 6.20558e-05  training_efficiency: -0  stability: -0.0407479
[2021-06-17 06:31:07,540 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 621.647  strength: -21.3532  max_strength: 257  final_strength: 144.5  sample_efficiency: 3.59013e-05  training_efficiency: -0  stability: -1.30619
[2021-06-17 06:31:07,540 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 636.204  strength: -6.7963  max_strength: 469.5  final_strength: 194.5  sample_efficiency: 0.000159399  training_efficiency: -0  stability: -3.20042
[2021-06-17 06:31:07,540 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 615.376  strength: -27.6241  max_strength: 844.5  final_strength: 444.5  sample_efficiency: 0.000141385  training_efficiency: -0  stability: -1.0577
[2021-06-17 06:31:57,112 PID:41481 INFO __init__.py log_summary][2021-06-17 06:31:57,138 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 200000  wall_t: 1496  opt_step: 0  frame: 200000  fps: 133.69  total_reward: 800  total_reward_ma: 610.208  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 200000  wall_t: 1493  opt_step: 0  frame: 200000  fps: 133.958  total_reward: 525  total_reward_ma: 630.351  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:31:57,849 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 200000  wall_t: 1503  opt_step: 0  frame: 200000  fps: 133.067  total_reward: 825  total_reward_ma: 632.35  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:31:58,315 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 200000  wall_t: 1503  opt_step: 0  frame: 200000  fps: 133.067  total_reward: 1125  total_reward_ma: 640.857  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:32:09,290 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 630.351  strength: -12.6491  max_strength: 469.5  final_strength: -118  sample_efficiency: 8.35914e-05  training_efficiency: -0  stability: -12.4332
[2021-06-17 06:32:09,283 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 632.35  strength: -10.6504  max_strength: 257  final_strength: 182  sample_efficiency: 6.36939e-05  training_efficiency: -0  stability: -2.1732
[2021-06-17 06:32:09,369 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 640.857  strength: -2.14286  max_strength: 844.5  final_strength: 482  sample_efficiency: 0.00167526  training_efficiency: -0  stability: -2.80035
[2021-06-17 06:32:10,674 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 610.208  strength: -32.7917  max_strength: 269.5  final_strength: 157  sample_efficiency: 7.57144e-05  training_efficiency: -0  stability: -0.20976
[2021-06-17 06:33:16,641 PID:41482 INFO __init__.py log_summary][2021-06-17 06:33:16,581 PID:41483 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 210000  wall_t: 1573  opt_step: 0  frame: 210000  fps: 133.503  total_reward: 712.5  total_reward_ma: 644.269  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:33:16,580 PID:41481 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 210000  wall_t: 1561  opt_step: 0  frame: 210000  fps: 134.529  total_reward: 937.5  total_reward_ma: 647.607  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 210000  wall_t: 1573  opt_step: 0  frame: 210000  fps: 133.503  total_reward: 500  total_reward_ma: 623.833  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:33:16,659 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 210000  wall_t: 1565  opt_step: 0  frame: 210000  fps: 134.185  total_reward: 987.5  total_reward_ma: 628.175  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:33:39,334 PID:41484 INFO __init__.py log_metrics][2021-06-17 06:33:39,334 PID:41483 INFO __init__.py log_metrics][2021-06-17 06:33:39,334 PID:41481 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 647.607  strength: 4.60714  max_strength: 294.5  final_strength: 294.5  sample_efficiency: -0.00012466  training_efficiency: 0  stability: -5.02718 Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 623.833  strength: -19.1667  max_strength: 469.5  final_strength: -143  sample_efficiency: 5.41846e-05  training_efficiency: -0  stability: -5.94175
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 628.175  strength: -14.8254  max_strength: 344.5  final_strength: 344.5  sample_efficiency: 0.000154226  training_efficiency: -0  stability: -0.499365
[2021-06-17 06:33:39,334 PID:41482 INFO __init__.py log_metrics]
 Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 644.269  strength: 1.26871  max_strength: 844.5  final_strength: 69.5  sample_efficiency: -0.00268238  training_efficiency: 0  stability: -55.1666
[2021-06-17 06:35:59,035 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 220000  wall_t: 1727  opt_step: 0  frame: 220000  fps: 127.389  total_reward: 1325  total_reward_ma: 659.848  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:35:58,888 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 220000  wall_t: 1727  opt_step: 0  frame: 220000  fps: 127.389  total_reward: 712.5  total_reward_ma: 647.37  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:35:59,213 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 220000  wall_t: 1734  opt_step: 0  frame: 220000  fps: 126.874  total_reward: 1212.5  total_reward_ma: 674.507  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:35:59,335 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 220000  wall_t: 1733  opt_step: 0  frame: 220000  fps: 126.947  total_reward: 850  total_reward_ma: 634.603  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:36:30,378 PID:41481 INFO __init__.py log_metrics][2021-06-17 06:36:30,378 PID:41482 INFO __init__.py log_metrics][2021-06-17 06:36:30,378 PID:41483 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 647.37  strength: 4.37013  max_strength: 844.5  final_strength: 69.5  sample_efficiency: -0.000740047  training_efficiency: 0  stability: -89.3486 Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 674.507  strength: 31.5068  max_strength: 569.5  final_strength: 569.5  sample_efficiency: -1.34482e-05  training_efficiency: 0  stability: -12.2364
[2021-06-17 06:36:30,378 PID:41484 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 634.603  strength: -8.39683  max_strength: 469.5  final_strength: 207  sample_efficiency: 0.000112457  training_efficiency: -0  stability: -3.35217

 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 659.848  strength: 16.8485  max_strength: 682  final_strength: 682  sample_efficiency: -0.000121175  training_efficiency: 0  stability: -2.15846
[2021-06-17 06:38:04,697 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 230000  wall_t: 1862  opt_step: 0  frame: 230000  fps: 123.523  total_reward: 987.5  total_reward_ma: 688.734  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan[2021-06-17 06:38:04,697 PID:41484 INFO __init__.py log_summary][2021-06-17 06:38:04,697 PID:41481 INFO __init__.py log_summary]
 Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 230000  wall_t: 1860  opt_step: 0  frame: 230000  fps: 123.656  total_reward: 1375  total_reward_ma: 690.942  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 230000  wall_t: 1858  opt_step: 0  frame: 230000  fps: 123.789  total_reward: 587.5  total_reward_ma: 632.462  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan

[2021-06-17 06:38:04,827 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 230000  wall_t: 1863  opt_step: 0  frame: 230000  fps: 123.457  total_reward: 900  total_reward_ma: 658.354  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:38:23,957 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 688.734  strength: 45.7338  max_strength: 569.5  final_strength: 344.5  sample_efficiency: -7.35488e-06  training_efficiency: 0  stability: -1.18342
[2021-06-17 06:38:23,993 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 690.942  strength: 47.942  max_strength: 732  final_strength: 732  sample_efficiency: -3.78473e-05  training_efficiency: 0  stability: -1.65288
[2021-06-17 06:38:24,046 PID:41481 INFO __init__.py log_metrics] Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 632.462  strength: -10.5379  max_strength: 469.5  final_strength: -55.5  sample_efficiency: 8.65758e-05  training_efficiency: -0  stability: -9.9499
[2021-06-17 06:38:24,083 PID:41482 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 658.354  strength: 15.354  max_strength: 844.5  final_strength: 257  sample_efficiency: -0.000198313  training_efficiency: 0  stability: -24.0372
[2021-06-17 06:44:59,429 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 240000  wall_t: 1944  opt_step: 0  frame: 240000  fps: 123.457  total_reward: 912.5  total_reward_ma: 668.943  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:44:59,429 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 240000  wall_t: 1957  opt_step: 0  frame: 240000  fps: 122.637  total_reward: 550  total_reward_ma: 685.069  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:45:00,607 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 240000  wall_t: 1943  opt_step: 0  frame: 240000  fps: 123.52  total_reward: 487.5  total_reward_ma: 626.159  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:45:13,213 PID:41482 INFO __init__.py log_metrics][2021-06-17 06:45:13,213 PID:41484 INFO __init__.py log_metrics][2021-06-17 06:45:13,213 PID:41481 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 668.943  strength: 25.9435  max_strength: 844.5  final_strength: 269.5  sample_efficiency: -0.000110673  training_efficiency: 0  stability: -5.81634 Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 685.069  strength: 42.0694  max_strength: 732  final_strength: -93  sample_efficiency: -4.17172e-05  training_efficiency: 0  stability: -0.639964
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 626.159  strength: -16.8406  max_strength: 469.5  final_strength: -155.5  sample_efficiency: 5.34916e-05  training_efficiency: -0  stability: -7.75988

[2021-06-17 06:46:18,028 PID:41484 INFO __init__.py log_summary] Trial 0 session 3 A2C_nstep_t0_s3 [train_df] epi: 0  t: 250000  wall_t: 2354  opt_step: 0  frame: 250000  fps: 106.202  total_reward: 637.5  total_reward_ma: 683.167  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:46:18,028 PID:41482 INFO __init__.py log_summary] Trial 0 session 1 A2C_nstep_t0_s1 [train_df] epi: 0  t: 250000  wall_t: 2360  opt_step: 0  frame: 250000  fps: 105.932  total_reward: 1100  total_reward_ma: 686.186  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:46:21,988 PID:41481 INFO __init__.py log_summary] Trial 0 session 0 A2C_nstep_t0_s0 [train_df] epi: 0  t: 250000  wall_t: 2368  opt_step: 0  frame: 250000  fps: 105.574  total_reward: 912.5  total_reward_ma: 638.09  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:46:44,934 PID:41482 INFO __init__.py log_metrics][2021-06-17 06:46:44,934 PID:41481 INFO __init__.py log_metrics] Trial 0 session 1 A2C_nstep_t0_s1 [train_df metrics] final_return_ma: 686.186  strength: 43.1857  max_strength: 844.5  final_strength: 457  sample_efficiency: -6.21334e-05  training_efficiency: 0  stability: -2.86601
 Trial 0 session 0 A2C_nstep_t0_s0 [train_df metrics] final_return_ma: 638.09  strength: -4.90972  max_strength: 469.5  final_strength: 269.5  sample_efficiency: 0.000166685  training_efficiency: -0  stability: -4.24312
[2021-06-17 06:46:45,387 PID:41484 INFO __init__.py log_metrics] Trial 0 session 3 A2C_nstep_t0_s3 [train_df metrics] final_return_ma: 683.167  strength: 40.1667  max_strength: 732  final_strength: -5.5  sample_efficiency: -4.19676e-05  training_efficiency: 0  stability: -0.79102
[2021-06-17 06:51:02,252 PID:41483 INFO __init__.py log_summary] Trial 0 session 2 A2C_nstep_t0_s2 [train_df] epi: 0  t: 240000  wall_t: 1943  opt_step: 0  frame: 240000  fps: 123.52  total_reward: 437.5  total_reward_ma: 677.811  loss: nan  lr: 0.0001  explore_var: nan  entropy_coef: 0.01  entropy: nan  grad_norm: nan
[2021-06-17 06:52:46,494 PID:41483 INFO __init__.py log_metrics] Trial 0 session 2 A2C_nstep_t0_s2 [train_df metrics] final_return_ma: 677.811  strength: 34.8106  max_strength: 569.5  final_strength: -205.5  sample_efficiency: -1.03121e-05  training_efficiency: 0  stability: -0.982465
